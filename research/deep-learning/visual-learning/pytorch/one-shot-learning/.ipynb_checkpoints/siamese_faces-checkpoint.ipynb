{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Siamese One-Shot-Learning Network, AT&T Faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import codecs\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageOps\n",
    "import random\n",
    "import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.utils import save_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DIR = './datasets/faces/training/'\n",
    "TEST_DIR = './datasets/faces/testing/'\n",
    "BATCH_SIZE = 64\n",
    "N_EPOCHS = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_img(img, text=None, should_save=False):\n",
    "    \n",
    "    img = img.numpy()\n",
    "    plt.axis('off')\n",
    "    if text:\n",
    "        plt.text(75, 8, text, style='italic', fontweight='bold',\n",
    "            bbox={'facecolor': 'white', 'aplha': 0.8, 'pad': 10 })\n",
    "    plt.imshow(np.transpose(img, (1, 2, 0)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_plot(iteration, loss):\n",
    "    plt.plot(iteration, loss)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Custom Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseNetworkDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, dataset, transform=None, should_invert=True):\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "        self.should_invert = should_invert\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        imageA_tupple = random.choice(self.dataset.imgs)\n",
    "        should_get_same_class = random.randint(0,1) # make sure approx 50% of images are in the same class\n",
    "        if should_get_same_class:\n",
    "            while True:\n",
    "                # keep looping untul the same class image is found\n",
    "                imageB_tupple = random.choice(self.dataset.imgs)\n",
    "                if imageA_tupple[1] == imageB_tupple[1]:\n",
    "                    break\n",
    "        else:\n",
    "            while True:\n",
    "                # keep looping untill the different class image is found\n",
    "                imageB_tupple = random.choice(self.dataset.imgs)\n",
    "                if imageA_tupple[1] == imageB_tupple[1]:\n",
    "                    break\n",
    "                    \n",
    "        imageA = Image.open(imageA_tupple[0])\n",
    "        imageB = Image.open(imageB_tupple[0])\n",
    "        \n",
    "        imageA = imageA.convert('L')\n",
    "        imageB = imageB.convert('L')\n",
    "        \n",
    "        if self.should_invert:\n",
    "            imageA = ImageOps.invert(imageA)\n",
    "            imageB = ImageOps.invert(imageB)\n",
    "            \n",
    "        if self.transform is not None:\n",
    "            imageA = self.transform(imageA)\n",
    "            imageB = self.transform(imageB)\n",
    "            \n",
    "        return imageA, imageB, torch.from_numpy(np.array([int(imageA_tupple[1] != imageB_tupple[1])], dtype=np.float32))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset.imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
