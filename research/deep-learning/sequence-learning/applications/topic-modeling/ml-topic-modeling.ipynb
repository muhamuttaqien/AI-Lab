{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling, LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "import nltk\n",
    "import spacy\n",
    "\n",
    "import gensim\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>target</th>\n",
       "      <th>target_names</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>From: lerxst@wam.umd.edu (where's my thing)\\nS...</td>\n",
       "      <td>7</td>\n",
       "      <td>rec.autos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>From: guykuo@carson.u.washington.edu (Guy Kuo)...</td>\n",
       "      <td>4</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>From: irwin@cmptrc.lonestar.org (Irwin Arnstei...</td>\n",
       "      <td>8</td>\n",
       "      <td>rec.motorcycles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>From: tchen@magnus.acs.ohio-state.edu (Tsung-K...</td>\n",
       "      <td>6</td>\n",
       "      <td>misc.forsale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>From: dabl2@nlm.nih.gov (Don A.B. Lindbergh)\\n...</td>\n",
       "      <td>2</td>\n",
       "      <td>comp.os.ms-windows.misc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                content  target  \\\n",
       "0     From: lerxst@wam.umd.edu (where's my thing)\\nS...       7   \n",
       "1     From: guykuo@carson.u.washington.edu (Guy Kuo)...       4   \n",
       "10    From: irwin@cmptrc.lonestar.org (Irwin Arnstei...       8   \n",
       "100   From: tchen@magnus.acs.ohio-state.edu (Tsung-K...       6   \n",
       "1000  From: dabl2@nlm.nih.gov (Don A.B. Lindbergh)\\n...       2   \n",
       "\n",
       "                 target_names  \n",
       "0                   rec.autos  \n",
       "1       comp.sys.mac.hardware  \n",
       "10            rec.motorcycles  \n",
       "100              misc.forsale  \n",
       "1000  comp.os.ms-windows.misc  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_news = pd.read_json('./datasets/news_groups.json') # grasp from https://raw.githubusercontent.com/selva86/datasets/master/newsgroups.json\n",
    "df_news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOPIC EXAMPLES (20 Topics):\n",
      "\n",
      "rec.autos, comp.sys.mac.hardware, rec.motorcycles, misc.forsale, comp.os.ms-windows.misc, alt.atheism, comp.graphics, rec.sport.baseball, rec.sport.hockey, sci.electronics, sci.space, talk.politics.misc, sci.med, talk.politics.mideast, soc.religion.christian, comp.windows.x, comp.sys.ibm.pc.hardware, talk.politics.guns, talk.religion.misc, sci.crypt\n"
     ]
    }
   ],
   "source": [
    "print(f'TOPIC EXAMPLES ({len(df_news.target_names.unique())} Topics):\\n')\n",
    "print(', '.join(df_news.target_names.unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Removing e-mails, new line characters and single quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = df_news['content'].values.tolist()\n",
    "\n",
    "# removing e-mails\n",
    "cleaned_content = [re.sub('\\S*@\\S*\\s?', '', sentence) for sentence in content]\n",
    "\n",
    "# removing new line characters\n",
    "cleaned_content = [re.sub('\\s+', ' ', sentence) for sentence in cleaned_content]\n",
    "\n",
    "# removing single quotes\n",
    "cleaned_content = [re.sub(\"\\'\", \"\", sentence) for sentence in cleaned_content]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>cleaned_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>From: lerxst@wam.umd.edu (where's my thing)\\nS...</td>\n",
       "      <td>From: (wheres my thing) Subject: WHAT car is t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>From: guykuo@carson.u.washington.edu (Guy Kuo)...</td>\n",
       "      <td>From: (Guy Kuo) Subject: SI Clock Poll - Final...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>From: irwin@cmptrc.lonestar.org (Irwin Arnstei...</td>\n",
       "      <td>From: (Irwin Arnstein) Subject: Re: Recommenda...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>From: tchen@magnus.acs.ohio-state.edu (Tsung-K...</td>\n",
       "      <td>From: (Tsung-Kun Chen) Subject: ** Software fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>From: dabl2@nlm.nih.gov (Don A.B. Lindbergh)\\n...</td>\n",
       "      <td>From: (Don A.B. Lindbergh) Subject: Diamond SS...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                content  \\\n",
       "0     From: lerxst@wam.umd.edu (where's my thing)\\nS...   \n",
       "1     From: guykuo@carson.u.washington.edu (Guy Kuo)...   \n",
       "10    From: irwin@cmptrc.lonestar.org (Irwin Arnstei...   \n",
       "100   From: tchen@magnus.acs.ohio-state.edu (Tsung-K...   \n",
       "1000  From: dabl2@nlm.nih.gov (Don A.B. Lindbergh)\\n...   \n",
       "\n",
       "                                        cleaned_content  \n",
       "0     From: (wheres my thing) Subject: WHAT car is t...  \n",
       "1     From: (Guy Kuo) Subject: SI Clock Poll - Final...  \n",
       "10    From: (Irwin Arnstein) Subject: Re: Recommenda...  \n",
       "100   From: (Tsung-Kun Chen) Subject: ** Software fo...  \n",
       "1000  From: (Don A.B. Lindbergh) Subject: Diamond SS...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_news['cleaned_content'] = cleaned_content\n",
    "df_news[['content', 'cleaned_content']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Tokenizing each sentence and removing punctuations, unnecessary characters also stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.utils import simple_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sentences(sentences):\n",
    "    for sentence in tqdm_notebook(sentences):\n",
    "        tokenized_sentence = simple_preprocess(str(sentence), deacc=True)\n",
    "        tokenized_sentence = [word for word in tokenized_sentence if word not in stop_words]\n",
    "        yield(tokenized_sentence) # true means removing punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4628033a20ec453b93454806bc923587",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=11314), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenized_sentences = list(tokenize_sentences(cleaned_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>tokenized_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>From: lerxst@wam.umd.edu (where's my thing)\\nS...</td>\n",
       "      <td>[wheres, thing, car, nntp, posting, host, rac,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>From: guykuo@carson.u.washington.edu (Guy Kuo)...</td>\n",
       "      <td>[guy, kuo, si, clock, poll, final, call, summa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>From: irwin@cmptrc.lonestar.org (Irwin Arnstei...</td>\n",
       "      <td>[irwin, arnstein, recommendation, duc, summary...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>From: tchen@magnus.acs.ohio-state.edu (Tsung-K...</td>\n",
       "      <td>[tsung, kun, chen, software, forsale, lots, nn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>From: dabl2@nlm.nih.gov (Don A.B. Lindbergh)\\n...</td>\n",
       "      <td>[lindbergh, diamond, ss, win, mouse, cursor, o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                content  \\\n",
       "0     From: lerxst@wam.umd.edu (where's my thing)\\nS...   \n",
       "1     From: guykuo@carson.u.washington.edu (Guy Kuo)...   \n",
       "10    From: irwin@cmptrc.lonestar.org (Irwin Arnstei...   \n",
       "100   From: tchen@magnus.acs.ohio-state.edu (Tsung-K...   \n",
       "1000  From: dabl2@nlm.nih.gov (Don A.B. Lindbergh)\\n...   \n",
       "\n",
       "                                      tokenized_content  \n",
       "0     [wheres, thing, car, nntp, posting, host, rac,...  \n",
       "1     [guy, kuo, si, clock, poll, final, call, summa...  \n",
       "10    [irwin, arnstein, recommendation, duc, summary...  \n",
       "100   [tsung, kun, chen, software, forsale, lots, nn...  \n",
       "1000  [lindbergh, diamond, ss, win, mouse, cursor, o...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_news['tokenized_content'] = tokenized_sentences\n",
    "df_news[['content', 'tokenized_content']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Making Bigram & Tigram Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model first\n",
    "bigram = gensim.models.Phrases(tokenized_sentences, min_count=5, threshold=100) # higher threshold fewer phrases\n",
    "trigram = gensim.models.Phrases(bigram[tokenized_sentences], threshold=100)\n",
    "\n",
    "bigram_model = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_model = gensim.models.phrases.Phraser(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bigram_words(text): return [bigram_model[word] for word in tqdm_notebook(text)]\n",
    "def make_trigram_words(text): return [trigram_model[bigram_model[word]] for word in tqdm_notebook(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22c742d6ca454769a1b74a4df5b25562",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=11314), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BIGRAM EXAMPLES:\n",
      "\n",
      "wheres, thing, car, nntp_posting, host, rac_wam, umd, organization, university, maryland_college, park, lines, wondering, anyone, could, enlighten, car, saw, day, door, sports, car, looked, late, early, called, bricklin, doors, really, small, addition, front_bumper, separate, rest, body, know, anyone, tellme, model, name, engine, specs, years, production, car, made, history, whatever, info, funky, looking, car, please, mail, thanks, il, brought, neighborhood, lerxst\n"
     ]
    }
   ],
   "source": [
    "bigram_words = make_bigram_words(tokenized_sentences)\n",
    "\n",
    "print('BIGRAM EXAMPLES:\\n')\n",
    "print(', '.join(bigram_words[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da998e2ff15748deafab80ed3e5f9a00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=11314), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TRIGRAM EXAMPLES:\n",
      "\n",
      "wheres, thing, car, nntp_posting_host, rac_wam_umd, organization, university_maryland_college, park, lines, wondering, anyone, could, enlighten, car, saw, day, door, sports, car, looked, late, early, called, bricklin, doors, really, small, addition, front_bumper, separate, rest, body, know, anyone, tellme, model, name, engine, specs, years, production, car, made, history, whatever, info, funky, looking, car, please, mail, thanks, il, brought, neighborhood, lerxst\n"
     ]
    }
   ],
   "source": [
    "trigram_words = make_trigram_words(tokenized_sentences)\n",
    "\n",
    "print('TRIGRAM EXAMPLES:\\n')\n",
    "print(', '.join(trigram_words[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) Lemmatizing Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_words(text, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \n",
    "    nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "        \n",
    "    lemmatized_words = []\n",
    "    \n",
    "    for sentence in tqdm_notebook(text):\n",
    "        doc = nlp(' '.join(sentence))\n",
    "        lemmatized_words.append([word.lemma_ for word in doc if word.pos_ in allowed_postags])\n",
    "        \n",
    "    return lemmatized_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18cb55d4e0ef4d7ab6de89b83b7a47c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=11314), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "lemmatized_words = lemmatize_words(bigram_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LEMMATIZATION EXAMPLES:\n",
      "\n",
      "where, s, thing, car, nntp_posting, host, line, wonder, enlighten, car, see, day, door, sport, car, look, late, early, call, door, really, small, addition, front_bumper, separate, rest, body, know, year, production, car, make, history, info, funky, look, car, mail, thank, bring, neighborhood, lerxst\n"
     ]
    }
   ],
   "source": [
    "print('LEMMATIZATION EXAMPLES:\\n')\n",
    "print(', '.join(lemmatized_words[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5) Creating the dictionary and corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.corpora as corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "index2word = corpora.Dictionary(lemmatized_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6) Preparing text features using bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a02d3e5649141c28389fbcb70492713",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=11314), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "features = [index2word.doc2bow(text) for text in tqdm_notebook(lemmatized_words)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "addition 1, body 1, bring 1, call 1, car 5, day 1, door 2, early 1, enlighten 1, front_bumper 1\n"
     ]
    }
   ],
   "source": [
    "features_look = []\n",
    "for index, frequency in features[:1][0][:10]: features_look.append(f'{index2word[index]} {frequency}')\n",
    "print(', '.join(features_look))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build LDA Topic Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
