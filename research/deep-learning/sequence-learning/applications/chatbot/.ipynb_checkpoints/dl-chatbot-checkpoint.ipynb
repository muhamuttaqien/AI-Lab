{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative-Based Chatbot Using Sequence to Sequence Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import time\n",
    "import csv\n",
    "import math\n",
    "import random\n",
    "import codecs\n",
    "import itertools\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import optim\n",
    "from torch.jit import script, trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import normalize_string, filter_pairs, indexes_from_sentence, time_since, moving_average, show_plot_evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_token = 0\n",
    "SOS_token = 1\n",
    "EOS_token = 2\n",
    "MAX_LENGTH = 10\n",
    "MIN_COUNT = 3\n",
    "\n",
    "# model configs\n",
    "LR = 0.0001\n",
    "N_EPOCHS = 4000\n",
    "TEACHER_FORCING_RATIO = 1.0\n",
    "HIDDEN_SIZE = 500\n",
    "DROPOUT = 0.1\n",
    "CLIP = 50.0\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "ENCODER_N_LAYERS = 2\n",
    "DECODER_N_LAYERS = 2\n",
    "DECODER_LEARNING_RATIO = 5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "if is_cuda: device = torch.device('cuda')\n",
    "else: device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    \n",
    "    def __init__(self, name):\n",
    "        super(Vocabulary, self).__init__()\n",
    "        \n",
    "        self.name = name\n",
    "        self.trimmed = False\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = { PAD_token: 'PAD', SOS_token: 'SOS', EOS_token: 'EOS' }\n",
    "        self.num_words = 3 # count constant tokens PAD, SOS and EOS\n",
    "        \n",
    "    def add_sentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.add_word(word)\n",
    "            \n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.num_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.num_words] = word\n",
    "            self.num_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "            \n",
    "    # remove words below a certain count threshold\n",
    "    def trim(self, min_count):\n",
    "\n",
    "        if self.trimmed: return\n",
    "\n",
    "        self.trimmed = True\n",
    "\n",
    "        keep_words = []\n",
    "        for key, value in self.word2count.items():\n",
    "            if value >= min_count:\n",
    "                keep_words.append(key)\n",
    "\n",
    "        print('Keep Words: {}/{} = {:.4f}'.format(\n",
    "               len(keep_words), len(self.word2index), len(keep_words) / len(self.word2index)))\n",
    "\n",
    "        # reinitialize dictionaries\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = { PAD_token: 'PAD', SOS_token: 'SOS', EOS_token: 'EOS' }\n",
    "        self.num_words = 3  # count constant tokens PAD, SOS and EOS\n",
    "\n",
    "        for word in keep_words:\n",
    "            self.add_word(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_vocabulary(data_file, corpus_name):\n",
    "    print('Reading lines...')\n",
    "    \n",
    "    # read the file and split into lines\n",
    "    lines = open(data_file, encoding='utf-8').read().strip().split('\\n')\n",
    "    # split every line into pairs and normalize\n",
    "    pairs = [[normalize_string(string) for string in l.split('\\t')] for l in lines]\n",
    "    vocabulary = Vocabulary(corpus_name)\n",
    "    \n",
    "    return vocabulary, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_prepare_data(corpus, corpus_name, data_file, save_dir):\n",
    "    print('Starting training data preparation...')\n",
    "    vocabulary, pairs = read_vocabulary(data_file, corpus_name)\n",
    "    \n",
    "    print('Reading {!s} sentence pairs'.format(len(pairs)))\n",
    "    pairs = filter_pairs(pairs, MAX_LENGTH)\n",
    "    \n",
    "    print('Trimmed to {!s} sentence pairs'.format(len(pairs)))\n",
    "    print('Counting words...')\n",
    "    for pair in pairs:\n",
    "        vocabulary.add_sentence(pair[0])\n",
    "        vocabulary.add_sentence(pair[1])\n",
    "    print(f'Counted words: {vocabulary.num_words}')\n",
    "    \n",
    "    return vocabulary, pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets grasp from here: www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html\n",
    "corpus_name = 'cornell_movie_dialogs'\n",
    "corpus = os.path.join('datasets', corpus_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_lines(file, n=10):\n",
    "    with open(file, 'rb') as data:\n",
    "        lines = data.readlines()\n",
    "    for line in lines[:n]:\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_lines(os.path.join(corpus, 'movie_lines.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_lines(file_name, fields):\n",
    "    \n",
    "    lines = {}\n",
    "    with open(file_name, 'r', encoding='iso-8859-1') as f:\n",
    "        for line in f:\n",
    "            values = line.split(' +++$+++ ')\n",
    "            \n",
    "            line_obj = {}\n",
    "            for i, field in enumerate(fields):\n",
    "                line_obj[field] = values[i]\n",
    "            \n",
    "            lines[line_obj['lineID']] = line_obj\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_conversations(file_name, lines, fields):\n",
    "    \n",
    "    conversations = []\n",
    "    with open(file_name, 'r', encoding='iso-8859-1') as f:\n",
    "        for line in f:\n",
    "            values = line.split(\" +++$+++ \")\n",
    "            \n",
    "            conv_obj = {}\n",
    "            for i, field in enumerate(fields):\n",
    "                conv_obj[field] = values[i]\n",
    "                \n",
    "            utterance_id_pattern = re.compile('L[0-9]+')\n",
    "            line_ids = utterance_id_pattern.findall(conv_obj['utteranceIDs'])\n",
    "            \n",
    "            conv_obj['lines'] = []\n",
    "            for line_id in line_ids:\n",
    "                conv_obj['lines'].append(lines[line_id])\n",
    "                \n",
    "            conversations.append(conv_obj)\n",
    "    return conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sentence_pairs(conversations):\n",
    "    \n",
    "    qa_pairs = []\n",
    "    for conversation in conversations:\n",
    "        for i in range(len(conversation['lines']) - 1):\n",
    "            input_line = conversation['lines'][i]['text'].strip()\n",
    "            target_line = conversation['lines'][i+1]['text'].strip()\n",
    "            \n",
    "            if input_line and target_line:\n",
    "                qa_pairs.append([input_line, target_line])\n",
    "                \n",
    "    return qa_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = os.path.join(corpus, 'formatted_movie_lines.txt')\n",
    "\n",
    "delimiter = '\\t'\n",
    "delimiter = str(codecs.decode(delimiter, 'unicode_escape'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = {}\n",
    "conversations = []\n",
    "MOVIE_LINES_FIELDS = ['lineID', 'characterID', 'movieID', 'character', 'text']\n",
    "MOVIE_CONVERSATIONS_FIELDS = ['character1ID', 'character2ID', 'movieID', 'utteranceIDs']\n",
    "\n",
    "print('\\nProcessing corpus...')\n",
    "lines = load_lines(os.path.join(corpus, 'movie_lines.txt'), MOVIE_LINES_FIELDS)\n",
    "\n",
    "print('\\nLoading conversations...')\n",
    "conversations = load_conversations(os.path.join(corpus, 'movie_conversations.txt'),\n",
    "                                   lines, MOVIE_CONVERSATIONS_FIELDS)\n",
    "\n",
    "print('\\nWriting newly formatted file...')\n",
    "with open(data_file, 'w', encoding='utf-8') as output_file:\n",
    "    writer = csv.writer(output_file, delimiter=delimiter, lineterminator='\\n')\n",
    "    for pair in extract_sentence_pairs(conversations):\n",
    "        writer.writerow(pair)\n",
    "        \n",
    "print('\\nSample lines from file:')\n",
    "print_lines(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = os.path.join('datasets', 'save')\n",
    "vocabulary, pairs = load_prepare_data(corpus, corpus_name, data_file, save_dir)\n",
    "\n",
    "print('\\nPairs:')\n",
    "for pair in pairs[:10]: print(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_rare_words(vocabulary, pairs, min_count):\n",
    "    \n",
    "    vocabulary.trim(min_count)\n",
    "    \n",
    "    keep_pairs = []\n",
    "    for pair in pairs:\n",
    "        input_sentence = pair[0]\n",
    "        output_sentence = pair[1]\n",
    "        \n",
    "        keep_input = True\n",
    "        keep_output = True\n",
    "        \n",
    "        # check input sentence\n",
    "        for word in input_sentence.split(' '):\n",
    "            if word not in vocabulary.word2index:\n",
    "                keep_input = False\n",
    "                break\n",
    "           \n",
    "        # check output sentence\n",
    "        for word in output_sentence.split(' '):\n",
    "            if word not in vocabulary.word2index:\n",
    "                keep_output = False\n",
    "                break\n",
    "                \n",
    "        # only keep pairs that do not contain trimmed word(s) in their input or output sentence\n",
    "        if keep_input and keep_output:\n",
    "            keep_pairs.append(pair)\n",
    "            \n",
    "    print('Trimmed from {} pairs to {}, {:.4f} of total.'.format(len(pairs), len(keep_pairs), len(keep_pairs) / len(pairs)))\n",
    "    return keep_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = trim_rare_words(vocabulary, pairs, MIN_COUNT)\n",
    "\n",
    "print('\\nPairs:')\n",
    "for pair in pairs[:10]: print(pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Datasets for Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_padding(l, fillvalue=PAD_token):\n",
    "    return list(itertools.zip_longest(*l, fillvalue=fillvalue))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_matrix(l, value=PAD_token):\n",
    "    matrix = []\n",
    "    for i, sequence in enumerate(l):\n",
    "        matrix.append([])\n",
    "        for token in sequence:\n",
    "            if token == PAD_token:\n",
    "                matrix[i].append(0)\n",
    "            else:\n",
    "                matrix[i].append(1)\n",
    "    \n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns padded input sequence tensor and lengths\n",
    "def input_var(l, vocabulary):\n",
    "    \n",
    "    indexes_batch = [indexes_from_sentence(vocabulary, sentence, EOS_token) for sentence in l]\n",
    "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
    "    pad_list = zero_padding(indexes_batch)\n",
    "    pad_var = torch.LongTensor(pad_list)\n",
    "    \n",
    "    return pad_var, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns padded target sequence tensor, padding mask and max target length\n",
    "def output_var(l, vocabulary):\n",
    "    \n",
    "    indexes_batch = [indexes_from_sentence(vocabulary, sentence, EOS_token) for sentence in l]\n",
    "    max_target_len = max([len(indexes) for indexes in indexes_batch])\n",
    "    pad_list = zero_padding(indexes_batch)\n",
    "    mask = binary_matrix(pad_list)\n",
    "    mask = torch.BoolTensor(mask)\n",
    "    pad_var = torch.LongTensor(pad_list)\n",
    "    \n",
    "    return pad_var, mask, max_target_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns all items for a given batch of pairs\n",
    "def batch_to_train_data(vocabulary, pair_batch):\n",
    "    \n",
    "    pair_batch.sort(key=lambda x: len(x[0].split(' ')), reverse=True)\n",
    "    input_batch, output_batch = [], []\n",
    "    for pair in pair_batch:\n",
    "        input_batch.append(pair[0])\n",
    "        output_batch.append(pair[1])\n",
    "    \n",
    "    inp, lengths = input_var(input_batch, vocabulary)\n",
    "    output, mask, max_target_len = output_var(output_batch, vocabulary)\n",
    "    \n",
    "    return inp, lengths, output, mask, max_target_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SMALL_BATCH_SIZE = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = batch_to_train_data(vocabulary, [random.choice(pairs) for _ in range(SMALL_BATCH_SIZE)])\n",
    "input_variable, lengths, target_variable, mask, max_target_len = batches\n",
    "\n",
    "print(\"Input Variable:\", input_variable)\n",
    "print(\"Lengths:\", lengths)\n",
    "print(\"Target Variable:\", target_variable)\n",
    "print(\"Mask:\", mask)\n",
    "print(\"Max Target Len:\", max_target_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build [Seq2seq](https://arxiv.org/pdf/1409.3215.pdf) Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_size, embedding, n_layers=1, dropout=0):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding_layer = embedding\n",
    "        \n",
    "        # initialize GRU: the input size and hidden size params are bot set to 'hidden_size'\n",
    "        # because our input size is a word embedding with number of features equals to 'hidden_size'\n",
    "        self.gru_layer = nn.GRU(hidden_size, hidden_size, n_layers,\n",
    "                                dropout=(0 if n_layers == 1 else dropout), bidirectional=True)\n",
    "        \n",
    "    def forward(self, input_seq, input_lengths, hidden=None):\n",
    "        \n",
    "        # convert word indexes to embeddings\n",
    "        embeds = self.embedding_layer(input_seq) # input_seq: (max_length, batch_size)\n",
    "        \n",
    "        # pack padded batch of sequences for RNN module\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(embeds, input_lengths)\n",
    "        \n",
    "        # forward pass throught GRU layer\n",
    "        gru_out, hidden = self.gru_layer(packed, hidden) # hidden shape: (n_layers x num_directions, batch_size, hidden_size)\n",
    "        \n",
    "        # unpack the padding\n",
    "        outputs, _ = nn.utils.rnn.pad_packed_sequence(gru_out) # outputs shape: (max_length, batch_size, hidden_size)\n",
    "        \n",
    "        # sum bidirectional GRU outputs\n",
    "        outputs = outputs[:, :, :self.hidden_size] + outputs[:, :, self.hidden_size:]\n",
    "        \n",
    "        # return output and final hidden state\n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build [Attention](https://arxiv.org/pdf/1706.03762.pdf) Decoder Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlobalAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, method, hidden_size):\n",
    "        \n",
    "        super(GlobalAttention, self).__init__()\n",
    "        \n",
    "        self.method = method\n",
    "        \n",
    "        if self.method not in ['dot', 'general', 'concat']:\n",
    "            raise ValueError(self.method, 'is not an appropriate attention method.')\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        if self.method == 'general':\n",
    "            self.attention_layer = nn.Linear(self.hidden_size * 2, hidden_size)\n",
    "            self.v = nn.Parameter(torch.FloatTensor(hidden_size))\n",
    "            \n",
    "    def dot_score(self, hidden, encoder_output):\n",
    "        return torch.sum(hidden * encoder_output, dim=2)\n",
    "    \n",
    "    def general_score(self, hidden, encoder_output):\n",
    "        energy = self.attention_layer(encoder_output)\n",
    "        return torch.sum(hidden * energy, dim=2)\n",
    "    \n",
    "    def concat_score(self, hidden, encoder_output):\n",
    "        energy = self.attention_layer(torch.cat((hidden.expand(encoder_output.size(0), -1, -1), encoder_output), 2)).tanh()\n",
    "        return torch.sum(self.v * energy, dim=2)\n",
    "    \n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        # calculate the attention weights (energies) based on the given method\n",
    "        if self.method == 'general':\n",
    "            attention_energies = self.general_score(hidden, encoder_outputs)\n",
    "        elif self.method == 'concat':\n",
    "            attention_energies = self.concat_score(hidden, encoder_outputs)\n",
    "        elif self.method == 'dot':\n",
    "            attention_energies = self.dot_score(hidden, encoder_outputs)\n",
    "            \n",
    "        # transpose max_length and batch_size dimensions\n",
    "        attention_energies = attention_energies.t()\n",
    "        \n",
    "        # return the softmax normalized probability scores (with added dimension)\n",
    "        return F.softmax(attention_energies, dim=1).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlobalAttentionDecoder(nn.Module):\n",
    "    \n",
    "    # input_step shape: (1, batch_size)\n",
    "    # last_hidden shape: (n_layers x num_directions, batch_size, hidden_size)\n",
    "    # encoder_outputs shape: (max_length, batch_size, hidden_size)\n",
    "    def __init__(self, embedding, hidden_size, output_size, n_layers=1, dropout=0.1, attention_model='dot'):\n",
    "        \n",
    "        super(GlobalAttentionDecoder, self).__init__()\n",
    "        \n",
    "        self.attention_model = attention_model\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        # define layers\n",
    "        self.embedding_layer = embedding\n",
    "        self.embedding_dropout = nn.Dropout(dropout)\n",
    "        self.gru_layer = nn.GRU(hidden_size, hidden_size, n_layers, dropout=(0 if n_layers == 1 else dropout))\n",
    "        self.concat_layer = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.fc_layer = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "        self.attention_layer = GlobalAttention(attention_model, hidden_size)\n",
    "    \n",
    "    # output shape: (batch_size, vocabulary.num_words)\n",
    "    # hidden shape: (n_layers x num_directions, batch_size, hidden_size)\n",
    "    def forward(self, input_step, last_hidden, encoder_outputs):\n",
    "        \n",
    "        # run this one step (word) at a time\n",
    "        embeds = self.embedding_layer(input_step)\n",
    "        embeds = self.embedding_dropout(embeds)\n",
    "        \n",
    "        # forward through undirectional GRU\n",
    "        gru_out, hidden = self.gru_layer(embeds, last_hidden)\n",
    "        \n",
    "        # calculate attention weights from the current GRU output\n",
    "        attention_weights = self.attention_layer(gru_out, encoder_outputs)\n",
    "        \n",
    "        # multiply attention weights to encoder outputs to get new 'weighted sum' context vector\n",
    "        context_vector = attention_weights.bmm(encoder_outputs.transpose(0, 1))\n",
    "        \n",
    "        # concatenate weighted context vector and GRU output\n",
    "        gru_out = gru_out.squeeze(0)\n",
    "        \n",
    "        context_vector = context_vector.squeeze(1)\n",
    "        concat_input = torch.cat((gru_out, context_vector), 1)\n",
    "        concat_output = torch.tanh(self.concat_layer(concat_input))\n",
    "        \n",
    "        # predict next word\n",
    "        output = self.fc_layer(concat_output)\n",
    "        output = F.softmax(output, dim=1)\n",
    "        \n",
    "        # return output and final hidden state\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize Seq2seq Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING = nn.Embedding(vocabulary.num_words, HIDDEN_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(HIDDEN_SIZE, EMBEDDING, ENCODER_N_LAYERS, DROPOUT)\n",
    "encoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = GlobalAttentionDecoder(EMBEDDING, HIDDEN_SIZE, vocabulary.num_words, DECODER_N_LAYERS, DROPOUT, attention_model='dot')\n",
    "decoder.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_nll_loss(input_seq, target_seq, mask):\n",
    "    \n",
    "    n_total = mask.sum()\n",
    "    cross_entropy = -torch.log(torch.gather(input_seq, 1, target_seq.view(-1, 1)).squeeze(1))\n",
    "    loss = cross_entropy.masked_select(mask).mean()\n",
    "    loss = loss.to(device)\n",
    "    \n",
    "    return loss, n_total.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr=LR)\n",
    "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=LR * DECODER_LEARNING_RATIO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Seq2seq Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequence of Operations:\n",
    "\n",
    "1. Forward pass entire input batch through encoder\n",
    "2. Initialize decoder inputs as SOS_token, and hidden state as the encoderâ€™s final hidden state\n",
    "3. Forward input batch sequence through decoder one time step at a time\n",
    "4. If teacher forcing: set next decoder input as the current target; else: set next decoder input as current decoder output\n",
    "5. Calculate and accumulate loss\n",
    "6. Perform backpropagation\n",
    "7. Clip gradients\n",
    "8. Update encoder and decoder model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tick = time.time()\n",
    "losses_history = []\n",
    "total_loss_print = 0\n",
    "total_loss_plot = 0\n",
    "\n",
    "print_every = 5000\n",
    "plot_every = 100\n",
    "\n",
    "training_batches = [batch_to_train_data(vocabulary, [random.choice(pairs) for _ in range(BATCH_SIZE)])\n",
    "                    for _ in range(N_EPOCHS)]\n",
    "\n",
    "print('Training the network...')\n",
    "for epoch in range(1, N_EPOCHS+1):\n",
    "    \n",
    "    training_batch = training_batches[epoch - 1]\n",
    "    \n",
    "    # extract fields from batch\n",
    "    input_variable, lengths, target_variable, mask, max_target_len = training_batch\n",
    "    \n",
    "    # zero gradients\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    \n",
    "    # set device options\n",
    "    input_variable = input_variable.to(device)\n",
    "    lengths = lengths.to(device)\n",
    "    target_variable = target_variable.to(device)\n",
    "    mask = mask.to(device)\n",
    "    \n",
    "    # initialize variables\n",
    "    loss = 0\n",
    "    print_losses = []\n",
    "    n_totals = 0\n",
    "    \n",
    "    # forward pass through encoder\n",
    "    encoder_outputs, encoder_hidden = encoder(input_variable, lengths)\n",
    "    \n",
    "    # create initial decoder input (start with SOS tokens for each sentence)\n",
    "    decoder_input = torch.LongTensor([[SOS_token for _ in range(BATCH_SIZE)]])\n",
    "    decoder_input = decoder_input.to(device)\n",
    "    \n",
    "    # set initial decoder hidden state to the encoder's final hidden state\n",
    "    decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
    "    \n",
    "    # determine if we are using teacher forching this iteration\n",
    "    USE_TEACHER_FORCING = True if random.random() < TEACHER_FORCING_RATIO else False\n",
    "    \n",
    "    # forward batch of sequences through decoder one time step at a time\n",
    "    if USE_TEACHER_FORCING:\n",
    "        \n",
    "        for t in range(max_target_len):\n",
    "            \n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            \n",
    "            # teacher forcing: next input is current target\n",
    "            decoder_input = target_variable[t].view(1, -1)\n",
    "            \n",
    "            # calculate and accumulate loss\n",
    "            mask_loss, n_total = mask_nll_loss(decoder_output, target_variable[t], mask[t])\n",
    "            loss += mask_loss\n",
    "            print_losses.append(mask_loss.item() * n_total)\n",
    "            n_totals += n_total\n",
    "    else:\n",
    "        \n",
    "        for t in range(max_target_len):\n",
    "            \n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            \n",
    "            # no teacher forcing: next input is decoder's own current output\n",
    "            _, topi = decoder_output.topk(1)\n",
    "            decoder_input = torch.LongTensor([[topi[i][0] for i in range(BATCH_SIZE)]])\n",
    "            decoder_input = decoder_input.to(device)\n",
    "            \n",
    "            # calculate and accumulate loss\n",
    "            mask_loss, n_total = mask_nll_loss(decoder_output, target_variable[t], mask[t])\n",
    "            loss += mask_loss\n",
    "            print_losses.append(mask_loss.item() * n_total)\n",
    "            n_totals += n_total\n",
    "    \n",
    "    # perform backpropagation\n",
    "    loss.backward()\n",
    "    \n",
    "    # clip gradients\n",
    "    nn.utils.clip_grad_norm_(encoder.parameters(), CLIP)\n",
    "    nn.utils.clip_grad_norm_(decoder.parameters(), CLIP)\n",
    "    \n",
    "    # adjust model weights\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    \n",
    "    loss = sum(print_losses) / n_totals\n",
    "    total_loss_print += loss\n",
    "    total_loss_plot += loss\n",
    "    \n",
    "    if epoch % print_every == 0:\n",
    "        avg_loss_print = print_loss_total / print_every\n",
    "        total_loss_print = 0\n",
    "        print('%d | %d epochs, Average Loss: %.4f, Times Taken: %s' % (epoch, N_EPOCHS, avg_loss_print,\n",
    "                                                                        time_since(tick, epoch / N_EPOCHS)))\n",
    "    \n",
    "    if epoch & plot_every == 0:\n",
    "        avg_loss_plot = total_loss_plot / plot_every\n",
    "        losses_history.append(avg_loss_plot)\n",
    "        total_loss_plot = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_plot_evaluation(losses_history, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate The Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequence of Operations:\n",
    "\n",
    "1. Forward input through encoder model\n",
    "2. Prepare encoderâ€™s final hidden layer to be first hidden input to the decoder\n",
    "3. Initialize decoderâ€™s first input as SOS_token\n",
    "4. Initialize tensors to append decoded words to\n",
    "5. Iteratively decode one word token at a time: Forward pass through decoder, Obtain most likely word token and its softmax score, Record token and score, Prepare current token to be next decoder input\n",
    "6. Return collections of word tokens and scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GreedySearchDecoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, encoder, decoder):\n",
    "        \n",
    "        super(GreedySearchDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "    def forward(self, input_seq, input_length, max_length):\n",
    "        \n",
    "        # forward input through encoder model\n",
    "        encoder_outputs, encoder_hidden = self.encoder(input_seq, input_length)\n",
    "        \n",
    "        # prepare encoder's final hidden layer to be first hidden input to the decoder\n",
    "        decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
    "        \n",
    "        # initialize decoder input with SOS_token\n",
    "        decoder_input = torch.ones(1, 1, device=device, dtype=torch.long) * SOS_token\n",
    "        \n",
    "        # initialize tensors to append decoded words\n",
    "        all_tokens = torch.zeros([0], device=device, dtype=torch.long)\n",
    "        all_scores = torch.zeros([0], device=device)\n",
    "        \n",
    "        # iteratively decode one word token at a time\n",
    "        for _ in range(MAX_LENGTH):\n",
    "            \n",
    "            # forward pass through decoder\n",
    "            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            \n",
    "            # obtain most likely word token and its softmax score\n",
    "            decoder_scores, decoder_input = torch.max(decoder_output, dim=1)\n",
    "            \n",
    "            # record token and score\n",
    "            all_tokens = torch.cat((all_tokens, decoder_input), dim=0)\n",
    "            all_scores = torch.cat((all_scores, decoder_scores), dim=0)\n",
    "            \n",
    "            # prepare current token to be next decoder input (add a dimension)\n",
    "            decoder_input = torch.unsqueeze(decoder_input, 0)\n",
    "            \n",
    "        # return collections of word tokens and scores\n",
    "        return all_tokens, all_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize Search Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_decoder = GreedySearchDecoder(encoder, decoder)\n",
    "search_decoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_network(encoder, decoder, search_decoder, vocabulary, input_sequence, max_length=MAX_LENGTH):\n",
    "    \n",
    "    # format input sentence as a batch\n",
    "    indexes_batch = [indexes_from_sentence(vocabulary, input_sequence, EOS_token)]\n",
    "    \n",
    "    # create lengths tensor\n",
    "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
    "    lengths = lengths.to(device)\n",
    "    \n",
    "    # transpose dimensions of batch to match models' expectations\n",
    "    input_batch = torch.LongTensor(indexes_batch).transpose(0, 1)\n",
    "    input_batch = input_batch.to(device)\n",
    "    \n",
    "    # decode sentence with search decoder\n",
    "    tokens, scores = search_decoder(input_batch, lengths, max_length)\n",
    "    decoded_words = [vocabulary.index2word[token.item()] for token in tokens]\n",
    "    \n",
    "    return decoded_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_sentence(encoder, decoder, search_decoder, vocabulary):\n",
    "    \n",
    "    input_sequence = ''\n",
    "    \n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    while(1):\n",
    "        # get input sentence\n",
    "        input_sentence = input('ðŸ’»: ')\n",
    "\n",
    "        # check if it is quit case\n",
    "        if input_sentence == 'q' or input_sequence == 'quit': break\n",
    "\n",
    "        # normalize sentence\n",
    "        input_sentence = normalize_string(input_sentence)\n",
    "\n",
    "        # evaluate sentence\n",
    "        output_words = evaluate_network(encoder, decoder, search_decoder, vocabulary, input_sentence)\n",
    "\n",
    "        # format and print response sentence\n",
    "        output_words[:] = [x for x in output_words if not (x == 'EOS' or x == 'PAD')]\n",
    "        print('ðŸ”¥: ', ' '.join(output_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat with Chatbot!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sentence(encoder, decoder, search_decoder, vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
