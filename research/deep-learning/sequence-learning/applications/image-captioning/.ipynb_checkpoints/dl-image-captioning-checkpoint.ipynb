{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Captioning Using Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from pycocotools.coco import COCO\n",
    "\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils import resize_image_due_to_pytorch_issue, load_image, resize_images, build_vocabulary, get_data_loader, show_plot_evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# image configs\n",
    "IMAGE_SIZE = 256\n",
    "IMAGE_PATH = './datasets/train2014/'\n",
    "RESIZED_IMAGE_PATH = './datasets/resized2014/'\n",
    "CROP_SIZE = 100 # cannot set to 224 (resnet input) due to pytorch issue\n",
    "\n",
    "# caption configs\n",
    "CAPTION_PATH = 'datasets/annotations/captions_train2014.json'\n",
    "VOCABULARY_PATH = './datasets/vocabulary.pkl'\n",
    "\n",
    "# model configs\n",
    "EMBEDDING_SIZE = 256\n",
    "HIDDEN_SIZE = 512\n",
    "N_LAYERS = 1\n",
    "N_EPOCHS = 5\n",
    "BATCH_SIZE = 128\n",
    "LR = 0.001\n",
    "WEIGHT_PATH = './weights/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "if is_cuda: device = torch.device('cuda')\n",
    "else: device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "resize_images(IMAGE_PATH, RESIZED_IMAGE_PATH, IMAGE_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Vocabulary():\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super(Vocabulary, self).__init__()\n",
    "        \n",
    "        self.word2index = {}\n",
    "        self.index2word = {}\n",
    "        self.num_words = 0\n",
    "        \n",
    "    def add_word(self, word):\n",
    "        \n",
    "        if not word in self.word2index:\n",
    "            self.word2index[word] = self.num_words\n",
    "            self.index2word[self.num_words] = word\n",
    "            self.num_words += 1\n",
    "            \n",
    "    def __call__(self, word):\n",
    "        \n",
    "        if not word in self.word2index:\n",
    "            return self.word2index['<unknown>']\n",
    "        return self.word2index[word]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocabulary = build_vocabulary(Vocabulary, min_word_count=4,\n",
    "                              caption_path=CAPTION_PATH, vocabulary_path=VOCABULARY_PATH)\n",
    "\n",
    "print(f'Total Vocabulary Size: {len(vocabulary)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class COCODataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, image_path, coco_path, vocab, transform=None):\n",
    "        \n",
    "        super(COCODataset, self).__init__()\n",
    "        \n",
    "        self.image_path = image_path\n",
    "        self.coco = COCO(coco_path)\n",
    "        self.ids = list(self.coco.anns.keys())\n",
    "        self.vocab = vocab\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        coco = self.coco\n",
    "        vocab = self.vocab\n",
    "        annot_id = self.ids[index]\n",
    "        image_id = coco.anns[annot_id]['image_id']\n",
    "        caption = coco.anns[annot_id]['caption']\n",
    "        path = coco.loadImgs(image_id)[0]['file_name']\n",
    "        \n",
    "        image = Image.open(os.path.join(self.image_path, path)).convert('RGB')\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        # convert caption (string) to word index\n",
    "        tokens = nltk.tokenize.word_tokenize(str(caption).lower())\n",
    "        caption = []\n",
    "        caption.append(vocab('<start>'))\n",
    "        caption.extend([vocab(token) for token in tokens])\n",
    "        caption.append(vocab('<end>'))\n",
    "        target = torch.Tensor(caption)\n",
    "        \n",
    "        return image, target\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.RandomCrop(CROP_SIZE),\n",
    "                                transforms.RandomHorizontalFlip(),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean = (0.485, 0.456, 0.406),\n",
    "                                                     std = (0.229, 0.224, 0.225))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_loader = get_data_loader(COCODataset, IMAGE_PATH, CAPTION_PATH, \n",
    "                              vocabulary, transform, BATCH_SIZE, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build [Image Captioning](https://arxiv.org/pdf/1411.4555.pdf) Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncoderCNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding_size):\n",
    "        \n",
    "        super(EncoderCNN, self).__init__()\n",
    "        \n",
    "        # resnet = models.resnet152(pretrained=True) # use pre-trained resnet model\n",
    "        # modules = list(resnet.children())[:-1] # remove the last fully-connected layer\n",
    "        # self.resnet_layer = nn.Sequential(*modules)\n",
    "        # self.fc_layer = nn.Linear(resnet.fc.in_features, embedding_size)\n",
    "        \n",
    "        # simple way to use pre-trained model in pytorch\n",
    "        resnet = models.resnet152(pretrained=True)\n",
    "        resnet.classifier = nn.Linear(in_features=resnet.fc.in_features, out_features=embedding_size)\n",
    "        \n",
    "        self.resnet_layer = resnet\n",
    "        self.norm = nn.BatchNorm1d(embedding_size, momentum=0.01)\n",
    "        \n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        self.fc_layer.weight.data.normal_(0.0, 0.02)\n",
    "        self.fc_layer.bias.data.fill_(0)\n",
    "        \n",
    "    def forward(self, images):\n",
    "        \n",
    "        feature_vectors = self.resnet_layer(images)\n",
    "        feature_vectors = Variable(feature_vectors.data)\n",
    "        feature_vectors = feature_vectors.view(feature_vectors.size(0), -1)\n",
    "        feature_vectors = self.norm(self.fc_layer(feature_vectors))\n",
    "        \n",
    "        return feature_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding_size, hidden_size, vocab_size, n_layers):\n",
    "        \n",
    "        super(DecoderRNN, self).__init__()\n",
    "        \n",
    "        self.embedding_layer = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.lstm_layer = nn.LSTM(embedding_size, hidden_size, n_layers, batch_first=True)\n",
    "        self.fc_layer = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        self.embedding_layer.weight.data.uniform_(-0.1, 0.1)\n",
    "        self.fc_layer.weight.data.uniform_(-0.1, 0.1)\n",
    "        self.fc_layer.bias.data.fill_(0)\n",
    "        \n",
    "    def sample(self, features, states=None):\n",
    "        \n",
    "        sampled_ids = []\n",
    "        inputs = features.unsqueeze(1)\n",
    "        \n",
    "        max_sampling_length = 20\n",
    "        \n",
    "        for i in range(max_sampling_length):\n",
    "            # hiddens shape: (batch_size, 1, hidden_size), states shape: (batch_size, vocab_size)\n",
    "            hiddens, states = self.lstm_layer(inputs, states)\n",
    "            outputs = self.fc_layer(hiddens.squeeze(1))\n",
    "            prediction = outputs.max(1)[1]\n",
    "            sampled_ids.append(prediction)\n",
    "            inputs = self.embedding_layer(prediction)\n",
    "            inputs = inputs.unsqueeze(1)\n",
    "        \n",
    "        sampled_ids = torch.cat(sampled_ids, 1)\n",
    "        return sampled_ids.squeeze()\n",
    "        \n",
    "    def forward(self, feature_vectors, source_captions, lengths):\n",
    "        \n",
    "        embeds = self.embedding_layer(source_captions)\n",
    "        embeds = torch.cat((feature_vectors.unsqueeze(1), embeds), 1)\n",
    "        packed = pack_padded_sequence(embeds, lengths, batch_first=True)\n",
    "        \n",
    "        hiddens, _ = self.lstm_layer(packed)\n",
    "        outputs = self.fc_layer(hiddens[0])\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize Image Captioning Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder = EncoderCNN(EMBEDDING_SIZE)\n",
    "encoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decoder = DecoderRNN(EMBEDDING_SIZE, HIDDEN_SIZE, len(vocabulary), N_LAYERS)\n",
    "decoder.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ce_loss = nn.CrossEntropyLoss()\n",
    "ce_loss.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = list(decoder.parameters()) + list(encoder.fc_layer.parameters()) + list(encoder.norm.parameters())\n",
    "optimizer = torch.optim.Adam(params, lr=LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train The Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "losses_history = []\n",
    "total_loss_print = 0; total_loss_plot = 0\n",
    "\n",
    "print_every = 10\n",
    "plot_every = 100\n",
    "save_every = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder.train()\n",
    "decoder.train()\n",
    "\n",
    "print('Training the network...')\n",
    "for epoch in range(1, N_EPOCHS+1):\n",
    "    \n",
    "    for i, (images, captions, lengths) in enumerate(data_loader):\n",
    "        \n",
    "        # this is only the matter of pytorch issue\n",
    "        images = resize_image_due_to_pytorch_issue(images)\n",
    "        images = torch.from_numpy(images)\n",
    "        \n",
    "        # set mini-batch datasets\n",
    "        images = images.to(device); captions = captions.to(device);\n",
    "        targets = pack_padded_sequence(captions, lengths, batch_first=True)[0]\n",
    "        \n",
    "        encoder.zero_grad()\n",
    "        decoder.zero_grad()\n",
    "        \n",
    "        # forward propagation\n",
    "        features = encoder(images)\n",
    "        outputs = decoder(features, captions, lengths)\n",
    "        \n",
    "        # calculate losses\n",
    "        loss = ce_loss(outputs, targets)\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        # accumulate losses\n",
    "        total_loss_print += loss\n",
    "        total_loss_plot += loss\n",
    "        \n",
    "        if i % print_every == 0:\n",
    "            avg_loss_print = total_loss_print / print_every\n",
    "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Perplexity: {:5.4f}'\n",
    "                  .format(epoch, N_EPOCHS, i, len(data_loader), avg_loss_print, np.exp(loss.item())))\n",
    "            total_loss_print = 0\n",
    "            \n",
    "        if epoch % plot_every == 0:\n",
    "            avg_loss_plot = total_loss_plot / plot_every\n",
    "            losses_history.append(avg_loss_plot)\n",
    "            total_loss_plot = 0\n",
    "            \n",
    "        # save the model checkpoints\n",
    "        if (i+1) % save_every == 0:\n",
    "            torch.save(encoder.state_dict(), os.path.join(WEIGHT_PATH, f'encoder-{epoch}-{i+1}.hdf5'))\n",
    "            torch.save(decoder.state_dict(), os.path.join(WEIGHT_PATH, f'decoder-{epoch}-{i+1}.hdf5'))            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "show_plot_evaluation(losses_history, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate The Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def caption_image(image_path, encoder_path, decoder_path, \n",
    "                  crop_size, embedding_size, hidden_size, vocabulary, n_layers):\n",
    "    \n",
    "    # prepare image\n",
    "    transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                    transforms.Normalize(mean = (0.485, 0.456, 0.406),\n",
    "                                                         std = (0.229, 0.224, 0.225))])\n",
    "    \n",
    "    image = load_image(image_path, crop_size, transform)\n",
    "    \n",
    "    # this is only the matter of pytorch issue\n",
    "    images = resize_image_due_to_pytorch_issue(np.asarray(images))\n",
    "    images = torch.from_numpy(images)\n",
    "    \n",
    "    image.to(device)\n",
    "    \n",
    "    # build models\n",
    "    encoder = EncoderCNN(embedding_size)\n",
    "    encoder.to(device)\n",
    "    \n",
    "    decoder = DecoderRNN(embedding_size, hidden_size, len(vocabulary), n_layers)\n",
    "    decoder.to(device)\n",
    "    \n",
    "    # load the trained model parameters\n",
    "    encoder.load_state_dict(torch.load(encoder_path))\n",
    "    decoder.load_state_dict(torch.load(decoder_path))\n",
    "    \n",
    "    # generate an caption from the image\n",
    "    feature_vectors = encoder(image)\n",
    "    sampled_ids = decoder.sample(feature_vectors)\n",
    "    sampled_ids = sampled_ids[0].cpu().numpy()\n",
    "    \n",
    "    # convert word_ids to words\n",
    "    sampled_caption = []\n",
    "    for word_id in sampled_ids:\n",
    "        word = vocabulary.index2word[word_id]\n",
    "        sampled_caption.append(word)\n",
    "        \n",
    "        if word == '<end>': break\n",
    "            \n",
    "    image_caption = ' '.join(sampled_caption).capitalize()\n",
    "    \n",
    "    image = Image.open(image_path)\n",
    "    plt.imshow(np.asarray(image))\n",
    "    plt.title(image_caption)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SAMPLE_IMAGE_PATH = 'images/'\n",
    "ENCODER_PATH = 'weights/'\n",
    "DECODER_PATH = 'weights/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "caption_image(SAMPLE_IMAGE_PATH, ENCODER_PATH, DECODER_PATH, \n",
    "              CROP_SIZE, EMBEDDING_SIZE, HIDDEN_SIZE, vocabulary, N_LAYERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
