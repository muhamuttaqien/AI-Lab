{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Captioning Using Deep Learning With Attention Mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "from scipy.misc import imread, imresize\n",
    "from collections import Counter\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "if is_cuda: device = torch.device('cuda')\n",
    "else: device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_input_files(datasets, karpathy_json_path, image_dir, output_dir, captions_per_image, min_word_freq, max_length=100):\n",
    "    \n",
    "    assert datasets in {'coco', 'flickr8k', 'flickr30k'}\n",
    "    \n",
    "    # read Karpathy's json\n",
    "    with open(karpathy_json_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "        \n",
    "    # read image paths and captions for each image\n",
    "    train_image_paths = []\n",
    "    train_image_captions = []\n",
    "    valid_image_paths = []\n",
    "    valid_image_captions = []\n",
    "    test_image_paths = []\n",
    "    test_image_captions = []\n",
    "    \n",
    "    word_freq = Counter()\n",
    "    \n",
    "    for image in tqdm_notebook(data['images']):\n",
    "        captions = []\n",
    "        for sentence in image['sentences']:\n",
    "            word_freq.update(sentence['tokens'])\n",
    "            if len(sentence['tokens']) <= max_length:\n",
    "                captions.append(sentence['tokens'])\n",
    "                \n",
    "        if len(captions) == 0:\n",
    "            continue\n",
    "            \n",
    "        path = os.path.join(image_dir, image['filepath'], image['filename']) if datasets == 'coco' \\\n",
    "                                                                             else os.path.join(image_dir, image['filename'])\n",
    "        \n",
    "        if image['split'] in {'train', 'restval'}:\n",
    "            train_image_paths.append(path)\n",
    "            train_image_captions.append(captions)\n",
    "        elif image['split'] in {'val'}:\n",
    "            valid_image_paths.append(path)\n",
    "            valid_image_captions.append(captions)\n",
    "        elif image['split'] in {'test'}:\n",
    "            test_image_paths.append(path)\n",
    "            test_image_captions.append(captions)\n",
    "            \n",
    "    # sanity check\n",
    "    assert len(train_image_paths) == len(train_image_captions)\n",
    "    assert len(valid_image_paths) == len(valid_image_captions)\n",
    "    assert len(test_image_paths) == len(test_image_captions)\n",
    "    \n",
    "    # create vocabulary\n",
    "    words = [word for word in word_freq.keys() if word_freq[word] > min_word_freq]\n",
    "    word_vocab = { key: value + 1 for value, key in enumerate(words)}\n",
    "    word_vocab['<unk>'] = len(word_vocab) + 1\n",
    "    word_vocab['<start>'] = len(word_vocab) + 1\n",
    "    word_vocab['<end>'] = len(word_vocab) + 1\n",
    "    word_vocab['<pad>'] = 0\n",
    "    \n",
    "    # create a base/ root name for all output files\n",
    "    base_filename = datasets + '_' + str(captions_per_image) + '_cap_per_img_' + str(min_word_freq) + '_min_word_freq'\n",
    "    \n",
    "    # save word vocabulary to a JSON\n",
    "    with open(os.path.join(output_dir, 'data/WORD_VOCAB_' + base_filename + '.json'), 'w') as file:\n",
    "        json.dump(word_vocab, file)\n",
    "        \n",
    "    # sample captions for each image, save images to HDF5 file and captions and their lengths to JSON files\n",
    "    random.seed(9)\n",
    "    for image_paths, image_captions, split in [(train_image_paths, train_image_captions, 'TRAIN'),\n",
    "                                              (valid_image_paths, valid_image_captions, 'VALID'),\n",
    "                                              (test_image_paths, test_image_captions, 'TEST')]:\n",
    "        \n",
    "        with h5py.File(os.path.join(output_dir, 'data/' + split + '_IMAGES_' + base_filename + '.hdf5'), 'a') as file:\n",
    "            \n",
    "            # make a note of the number of captions we are sampling per image\n",
    "            file.attrs['captions_per_image'] = captions_per_image\n",
    "            \n",
    "            # create dataset inside HDF5 file to store images\n",
    "            images = file.create_dataset('./datasets/images', (len(image_paths), 3, 256, 256), dtype='uint8')\n",
    "            \n",
    "            print(f'\\nReading {split} images and captions, storing to file...\\n')\n",
    "            \n",
    "            encoded_captions = []\n",
    "            captions_length = []\n",
    "            \n",
    "            for i, path in enumerate(image_paths):\n",
    "                \n",
    "                # sample captions\n",
    "                if len(image_captions[i]) < captions_per_image:\n",
    "                    captions = image_captions[i] + [random.choice(image_captions[i]) for _ in range(captions_per_image - len(image_captions[i]))]\n",
    "                else:\n",
    "                    captions = random.sample(image_captions[i], k=captions_per_image)\n",
    "                    \n",
    "                # sanity check\n",
    "                assert len(captions) == captions_per_image\n",
    "                \n",
    "                # read images\n",
    "                image = imread(image_paths[i])\n",
    "                if len(image.shape) == 2:\n",
    "                    image = image[:, :, np.newaxis]\n",
    "                    image = np.concatenate([image, image, image], axis=2)\n",
    "                image = imresize(image, (256, 256))\n",
    "                image = image.transpose(2, 0, 1)\n",
    "                \n",
    "                # sanity check\n",
    "                assert image.shape == (3, 256, 256)\n",
    "                assert np.max(image) <= 255\n",
    "                \n",
    "                # save image to HDF5 file\n",
    "                images[i] = image\n",
    "                \n",
    "                for j, caption in enumerate(captions):\n",
    "                    # encode captions\n",
    "                    encoded_caption = [word_vocab['<start>']] + [word_vocab.get(word, word_vocab['<unk>']) for word in caption] +\\\n",
    "                                      [word_vocab['<end>']] + [word_vocab['<pad>']] * (max_length - len(caption))\n",
    "                        \n",
    "                    # find caption lengths\n",
    "                    caption_length = len(caption) + 2\n",
    "                    \n",
    "                    encoded_captions.append(encoded_caption)\n",
    "                    captions_length.append(caption_length)\n",
    "            \n",
    "            # sanity check\n",
    "            assert images.shape[0] * captions_per_image == len(encoded_captions) == len(captions_length)\n",
    "            \n",
    "            # save encoded captions and their lengths to JSON files\n",
    "            with open(os.path.join(output_dir, 'data/' + split + '_CAPTIONS_' + base_filename + '.json'), 'w') as file:\n",
    "                json.dump(encoded_captions, file)\n",
    "            \n",
    "            with open(os.path.join(output_dir, 'data/' + split + '_CAPLENS_' + base_filename + '.json'), 'w') as file:\n",
    "                json.dump(captions_length, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "create_input_files(datasets='coco', karpathy_json_path='./datasets/karpathy_captions/datasets_coco.json',\n",
    "                   image_dir='./datasets/', output_dir='./datasets/',\n",
    "                   captions_per_image=5,\n",
    "                   min_word_freq=5,\n",
    "                   max_length=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CaptionDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, data_folder, data_name, split, transform=None):\n",
    "        \n",
    "        super(CaptionDataset, self).__init__()\n",
    "        \n",
    "        self.split = split\n",
    "        assert self.split in {'TRAIN', 'VALID', 'TEST'}\n",
    "        \n",
    "        # open hdf5 file where images are stored\n",
    "        self.hdf5 = h5py.File(os.path.join(data_folder, '/data' + self.split + '_IMAGES_' + data_name + '.hdf5'), 'r')\n",
    "        self.images = self.hdf5['images']\n",
    "        \n",
    "        # captions per image\n",
    "        self.cpi = self.hdf5.attrs['captions_per_image']\n",
    "        \n",
    "        # load encoded captions (completely into memory)\n",
    "        with open(os.path.join(data_folder, 'data/' + self.split + '_CAPTIONS_' + data_name + '.json'), 'r') as file:\n",
    "            self.captions = json.load(file)\n",
    "            \n",
    "        # load captions lengths (completely into memory)\n",
    "        with open(os.path.join(data_folder, 'data'/ + self.split + '_CAPLENS_' + data_name + '.json'), 'r') as file:\n",
    "            self.caplens = json.load(file)\n",
    "            \n",
    "        # pytorch transformation pipeline for the image (normalizing, etc.)\n",
    "        self.transform = transform\n",
    "        \n",
    "        # total number of data points\n",
    "        self.dataset_size = len(self.captions)\n",
    "        \n",
    "    def __getitem_(self, i):\n",
    "        \n",
    "        # remember, the Nth caption corresponds to the (N // captions_per_image)th image\n",
    "        image = torch.FloatTensor(self.images[i // self.cpi] / 255.)\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        caption = torch.LongTensor(self.captions[i])\n",
    "        caplen = torch.LongTensor([self.caplens[i]])\n",
    "        \n",
    "        if self.split is 'TRAIN':\n",
    "            return image, caption, caplen\n",
    "        else:\n",
    "            # for validation of testing, also return all 'captions_per_image' captions to find BLEU-4 score\n",
    "            all_captions = torch.LongTensor(\n",
    "                self.captions[((i // self.cpi) * self.cpi) : (((i // self.cpi) * self.cpi) + self.cpi)])\n",
    "            return image, caption, caplen, all_captions\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.dataset_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build [Image Captioning](https://arxiv.org/pdf/1411.4555.pdf) Network with [Attention](https://arxiv.org/pdf/1502.03044.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncoderCNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, image_size=14):\n",
    "        \n",
    "        super(EncoderCNN, self).__init__()\n",
    "        \n",
    "        self.image_size = image_size\n",
    "        \n",
    "        # import pre-trained ImageNet ResNet-101\n",
    "        resnet = torchvision.models.resnet101(pretrained=True)\n",
    "        \n",
    "        # remove linear and pool layers\n",
    "        modules = list(resnet.children())[:-2]\n",
    "        self.resnet_layer = nn.Sequential(*modules)\n",
    "        \n",
    "        # resize image to fixed size to allow input image of variable size\n",
    "        self.adaptive_pool_layer = nn.AdaptiveAvgPool2d((image_size, image_size))\n",
    "        \n",
    "        # this will enable or disable the calculation of gradients for the Encoder's parameters\n",
    "        self.fine_tune()\n",
    "        \n",
    "    def fine_tune(self, is_fine_tune=True):\n",
    "        \n",
    "        for param in self.resnet_layer.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # if fine-tuning, then only fine-tune convolutional blocks 2 through 4\n",
    "        for child in list(self.resnet_layer.children())[5:]:\n",
    "            for param in child.parameters():\n",
    "                param.requires_grad = is_fine_tune\n",
    "                \n",
    "    def forward(self, images):\n",
    "        \n",
    "        feature_vectors = self.resnet_layer(images) # (batch_size, 2048, image_size/ 32, image_size/ 32)\n",
    "        feature_vectors = self.adaptive_pool_layer(feature_vectors) # (batch_size, 2048, image_size/ 32, image_size/ 32)\n",
    "        feature_vectors = feature_vectors.permute(0, 2, 3, 1) # (batch_size, image_size, image_size, 2048)\n",
    "        \n",
    "        return feature_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    \n",
    "    def __init__(self, encoder_dim, decoder_dim, attention_dim):\n",
    "        \n",
    "        super(Attention, self).__init__()\n",
    "        \n",
    "        self.encoder_attention_layer = nn.Linear(encoder_dim, attention_dim) # linear layer to transform encoded image\n",
    "        self.decoder_attention_layer = nn.Linear(decoder_dim, attention_dim) # linear layer to transform decoder's output\n",
    "        self.total_attention_layer = nn.Linear(attention_dim, 1) # linear layer to calculate values to be softmax-ed\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1) # softmax layer to calculate weights\n",
    "        \n",
    "    def forward(self, encoder_output, decoder_hidden):\n",
    "        \n",
    "        encoder_attention = self.encoder_attention_layer(encoder_output) # (batch_size, num_pixels, attention_dim)\n",
    "        decoder_attention = self.decoder_attention_layer(decoder_hidden) # (batch_size, attention_dim)\n",
    "        total_attention = self.total_attention_layer(self.relu(encoder_attention + decoder_attention.unsqueeze(1))).squeeze(2) # (batch_size, num_pixels)\n",
    "        alpha = self.softmax(total_attention) # (batch_size, num_pixels)\n",
    "        attention_weighted_encoding = (encoder_attention * alpha.unsqueeze(2)).sum(dim=1) # (batch_size, encoder_dim)\n",
    "        \n",
    "        return attention_weighted_encoding, alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionDecoderRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, attention_dim, embedding_dim, decoder_dim, vocab_size, encoder_dim=2048, dropout=0.5):\n",
    "        \n",
    "        super(AttentionDecoderRNN, self).__init__()\n",
    "        \n",
    "        self.attention_dim = attention_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.decoder_dim = decoder_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.encoder_dim = encoder_dim\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        # init attention network\n",
    "        self.attention = Attention(encoder_dim, decoder_dim, attention_dim)\n",
    "        \n",
    "        self.embedding_layer = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.dropout = nn.Dropout(self.dropout)\n",
    "        self.decode_step = nn.LSTMCell(embedding_dim + encoder_dim, decoder_dim, bias=True)\n",
    "        self.init_hidden = nn.Linear(encoder_dim, decoder_dim) # linear layer to find initial hidden state of LSTM\n",
    "        self.init_cell = nn.Linear(encoder_dim, decoder_dim) # linear layer to find initial cell state of LSTM\n",
    "        self.beta = nn.Linear(decoder_dim, encoder_dim) # linear layer to create a sigmoid-activated gate\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.fc_layer = nn.Linear(decoder_dim, vocab_size) # linear layer to find scores over vocabulary\n",
    "        \n",
    "        self.init_weights() # initialize some layers with the uniform distribution\n",
    "        \n",
    "    def init_weights(self):\n",
    "        \n",
    "        self.embedding_layer.weight.data.uniform_(-0.1, 0.1)\n",
    "        self.fc.weight.data.uniform_(-0.1, 0.1)\n",
    "        self.fc.bias.data.fill_(0)\n",
    "\n",
    "    def init_state(self, encoder_output):\n",
    "        \n",
    "        mean_encoder_output = encoder_output.mean(dim=1)\n",
    "        hidden = self.init_hidden(mean_encoder_output)\n",
    "        cell = self.init_cell(mean_encoder_output)\n",
    "        \n",
    "        return hidden, cell\n",
    "    \n",
    "    def load_pretrained_embeddings(self, embeddings):\n",
    "        \n",
    "        self.embedding_layer.weight = nn.Parameter(embeddings)\n",
    "        \n",
    "    def fine_tune_embeddings(self, is_fine_tune=True):\n",
    "        \n",
    "        for param in self.embedding_layer.parameters():\n",
    "            param.requires_grad_ = is_fine_tune\n",
    "            \n",
    "    def forward(self, encoder_output, encoded_captions, caption_lengths):\n",
    "        \n",
    "        batch_size = encoder_output.size()\n",
    "        encoder_dim = encoder_output.size(-1)\n",
    "        vocab_size = self.vocab_size\n",
    "        \n",
    "        # flatten image\n",
    "        encoder_output = encoder_output.view(batch_size, -1, encoder_dim) # (batch_size, num_pixels, encoder_dim)\n",
    "        num_pixels = encoder_output.size(1)\n",
    "        \n",
    "        # sort input data by decreasing lengths\n",
    "        caption_lengths, sort_id = caption_lengths.squeeze(1).sort(dim=0, descending=True)\n",
    "        encoder_output = encoder_output[sort_id]\n",
    "        encoded_captions = encoded_captions[sort_id]\n",
    "        \n",
    "        # embedding\n",
    "        embeddings = self.embedding_layer(encoded_captions) # (batch_size, max_caption_length, embedding_dim)\n",
    "        \n",
    "        # init LSTM state\n",
    "        decoder_hidden, decoder_cell = self.init_state(encoder_output) # (batch_size, decoder_dim)\n",
    "        \n",
    "        # since generation process finished as soon as model generate <end> so decoding lengths are actual lengths - 1\n",
    "        decode_lengths = (caption_lengths - 1).tolist()\n",
    "        \n",
    "        # create tensors to hold word prediction scores and alphas\n",
    "        predictions = torch.zeros(batch_size, max(decode_lengths), vocab_size).to(device)\n",
    "        alphas = torch.zeros(batch_size, max(decode_lengths), num_pixels).to(device)\n",
    "        \n",
    "        # at each time-step, decode by attention weights the encoder's output based on the decoder's previous hidden state\n",
    "        # then generate a new word in the decoder with the previous word and the attention-weighted encoding\n",
    "        \n",
    "        for d_time in range(max(decode_lengths)):\n",
    "            batch_size_time = sum([length > d_time for length in decode_lengths])\n",
    "            attention_weighted_encoding, alpha = self.attention(encoder_output[:batch_size_time],\n",
    "                                                                decoder_hidden[:batch_size_time])\n",
    "            \n",
    "            gate = self.sigmoid(self.beta(decoder_hidden[:batch_size_time])) # (batch_size_time, encoder_dim)\n",
    "            attention_weighted_encoding = gate * attention_weighted_encoding\n",
    "            \n",
    "            decoder_hidden, decoder_cell = self.decode_step(\n",
    "                torch.cat([embeddings[:batch_size_time, d_time, :], attention_weighted_encoding], dim=1),\n",
    "                (decoder_hidden[:batch_size_time], decoder_cell[:batch_size_time])) # (batch_size_time, decoder_dim)\n",
    "            \n",
    "            prediction = self.fc_layer(self.dropout(decoder_hidden)) # (batch_size_time, vocab_size)\n",
    "            predictions[:batch_size_time, d_time, :] = prediction\n",
    "            alphas[:batch_size_time, d_time, :] = alpha\n",
    "            \n",
    "        return predictions, encoded_captions, decode_lengths, alphas, sort_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
