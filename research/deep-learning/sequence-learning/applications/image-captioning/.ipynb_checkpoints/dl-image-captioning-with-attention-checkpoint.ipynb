{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Captioning Using Deep Learning With Attention Mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "from scipy.misc import imread, imresize\n",
    "from collections import Counter\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import corpus_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model configs\n",
    "EMBEDDING_SIZE = 512\n",
    "ATTENTION_SIZE = 512\n",
    "DECODER_SIZE = 512\n",
    "DROPOUT = 0.5\n",
    "\n",
    "# training configs\n",
    "N_EPOCHS = 129\n",
    "BATCH_SIZE = 32\n",
    "ENCODER_LR = 1e-4 # learning rate for encoder if fine-tuning\n",
    "DECODER_LR = 4e-4 # learning rate for decoder\n",
    "GRAD_CLIP = 5.\n",
    "ALPHA_C = 1.\n",
    "FINE_TUNE_ENCODER = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data configs\n",
    "DATA_FOLDER = './datasets/' # folder with data files saved by create_input_files\n",
    "DATA_NAME = 'coco_5_cap_per_img_5_min_word_freq'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "if is_cuda: device = torch.device('cuda')\n",
    "else: device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_gradient(optimizer, grad_clip):\n",
    "    \n",
    "    for group in optimizer.param_groups:\n",
    "        for param in group['params']:\n",
    "            if param.grad is not None: param.grad.data.clamp_(-grad_clip, grad_clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(data_name, epoch, epochs_since_improvement, encoder, decoder, \n",
    "                    encoder_optimizer, decoder_optimizer, bleu4, is_best):\n",
    "    \n",
    "    state = { 'epoch': epoch,\n",
    "              'epochs_since_improvement': epochs_since_improvement,\n",
    "              'bleu-4': bleu4,\n",
    "              'encoder': encoder,\n",
    "              'decoder': decoder,\n",
    "              'encoder_optimizer': encoder_optimizer,\n",
    "              'decoder_optimizer': decoder_optimizer }\n",
    "    \n",
    "    if not os.path.exists('./weights/'): os.makedirs('./weights/')\n",
    "    filename = './weights/checkpoint_' + data_name + '.pth.tar'\n",
    "    torch.save(state, filename)\n",
    "    \n",
    "    # if this checkpoint is the best so far, store a copy so it doesn't get overwritten by a worse checkpoint\n",
    "    if is_best: torch.save(state, 'BEST_' + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "        \n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, shrink_factor):\n",
    "    \n",
    "    print('\\nDecaying Learning rate...')\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = param_group['lr'] * shrink_factor\n",
    "    print(f'The new learning rate is {optimizer.param_groups[0][\"lr\"]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(scores, targets, k):\n",
    "    \n",
    "    batch_size = targets.size(0)\n",
    "    _, index = scores.topk(k, 1, True, True)\n",
    "    correct = index.eq(targets.view(-1, 1).expand_as(index))\n",
    "    correct_total = correct.view(-1).float().sum()\n",
    "    \n",
    "    return correct_total.item() * (100.0 / batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_input_files(datasets, karpathy_json_path, image_dir, output_dir, captions_per_image, min_word_freq, max_length=100):\n",
    "    \n",
    "    assert datasets in {'coco', 'flickr8k', 'flickr30k'}\n",
    "    \n",
    "    # read Karpathy's json\n",
    "    with open(karpathy_json_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "        \n",
    "    # read image paths and captions for each image\n",
    "    train_image_paths = []\n",
    "    train_image_captions = []\n",
    "    valid_image_paths = []\n",
    "    valid_image_captions = []\n",
    "    test_image_paths = []\n",
    "    test_image_captions = []\n",
    "    \n",
    "    word_freq = Counter()\n",
    "    \n",
    "    for image in tqdm_notebook(data['images']):\n",
    "        captions = []\n",
    "        for sentence in image['sentences']:\n",
    "            word_freq.update(sentence['tokens'])\n",
    "            if len(sentence['tokens']) <= max_length:\n",
    "                captions.append(sentence['tokens'])\n",
    "                \n",
    "        if len(captions) == 0:\n",
    "            continue\n",
    "            \n",
    "        path = os.path.join(image_dir, image['filepath'], image['filename']) if datasets == 'coco' \\\n",
    "                                                                             else os.path.join(image_dir, image['filename'])\n",
    "        \n",
    "        if image['split'] in {'train', 'restval'}:\n",
    "            train_image_paths.append(path)\n",
    "            train_image_captions.append(captions)\n",
    "        elif image['split'] in {'val'}:\n",
    "            valid_image_paths.append(path)\n",
    "            valid_image_captions.append(captions)\n",
    "        elif image['split'] in {'test'}:\n",
    "            test_image_paths.append(path)\n",
    "            test_image_captions.append(captions)\n",
    "            \n",
    "    # sanity check\n",
    "    assert len(train_image_paths) == len(train_image_captions)\n",
    "    assert len(valid_image_paths) == len(valid_image_captions)\n",
    "    assert len(test_image_paths) == len(test_image_captions)\n",
    "    \n",
    "    # create vocabulary\n",
    "    words = [word for word in word_freq.keys() if word_freq[word] > min_word_freq]\n",
    "    word_vocab = { key: value + 1 for value, key in enumerate(words)}\n",
    "    word_vocab['<unk>'] = len(word_vocab) + 1\n",
    "    word_vocab['<start>'] = len(word_vocab) + 1\n",
    "    word_vocab['<end>'] = len(word_vocab) + 1\n",
    "    word_vocab['<pad>'] = 0\n",
    "    \n",
    "    # create a base/ root name for all output files\n",
    "    base_filename = datasets + '_' + str(captions_per_image) + '_cap_per_img_' + str(min_word_freq) + '_min_word_freq'\n",
    "    \n",
    "    # save word vocabulary to a JSON\n",
    "    with open(os.path.join(output_dir, 'data/' + 'WORD_VOCAB_' + base_filename + '.json'), 'w') as file:\n",
    "        json.dump(word_vocab, file)\n",
    "        \n",
    "    # sample captions for each image, save images to HDF5 file and captions and their lengths to JSON files\n",
    "    random.seed(9)\n",
    "    for image_paths, image_captions, split in [(train_image_paths, train_image_captions, 'TRAIN'),\n",
    "                                              (valid_image_paths, valid_image_captions, 'VALID'),\n",
    "                                              (test_image_paths, test_image_captions, 'TEST')]:\n",
    "        \n",
    "        with h5py.File(os.path.join(output_dir, 'data/' + split + '_IMAGES_' + base_filename + '.hdf5'), 'a') as file:\n",
    "            \n",
    "            # make a note of the number of captions we are sampling per image\n",
    "            file.attrs['captions_per_image'] = captions_per_image\n",
    "            \n",
    "            # create dataset inside HDF5 file to store images\n",
    "            images = file.create_dataset('./datasets/images', (len(image_paths), 3, 256, 256), dtype='uint8')\n",
    "            \n",
    "            print(f'\\nReading {split} images and captions, storing to file...\\n')\n",
    "            \n",
    "            encoded_captions = []\n",
    "            captions_length = []\n",
    "            \n",
    "            for i, path in enumerate(image_paths):\n",
    "                \n",
    "                # sample captions\n",
    "                if len(image_captions[i]) < captions_per_image:\n",
    "                    captions = image_captions[i] + [random.choice(image_captions[i]) for _ in range(captions_per_image - len(image_captions[i]))]\n",
    "                else:\n",
    "                    captions = random.sample(image_captions[i], k=captions_per_image)\n",
    "                    \n",
    "                # sanity check\n",
    "                assert len(captions) == captions_per_image\n",
    "                \n",
    "                # read images\n",
    "                image = imread(image_paths[i])\n",
    "                if len(image.shape) == 2:\n",
    "                    image = image[:, :, np.newaxis]\n",
    "                    image = np.concatenate([image, image, image], axis=2)\n",
    "                image = imresize(image, (256, 256))\n",
    "                image = image.transpose(2, 0, 1)\n",
    "                \n",
    "                # sanity check\n",
    "                assert image.shape == (3, 256, 256)\n",
    "                assert np.max(image) <= 255\n",
    "                \n",
    "                # save image to HDF5 file\n",
    "                images[i] = image\n",
    "                \n",
    "                for j, caption in enumerate(captions):\n",
    "                    # encode captions\n",
    "                    encoded_caption = [word_vocab['<start>']] + [word_vocab.get(word, word_vocab['<unk>']) for word in caption] +\\\n",
    "                                      [word_vocab['<end>']] + [word_vocab['<pad>']] * (max_length - len(caption))\n",
    "                        \n",
    "                    # find caption lengths\n",
    "                    caption_length = len(caption) + 2\n",
    "                    \n",
    "                    encoded_captions.append(encoded_caption)\n",
    "                    captions_length.append(caption_length)\n",
    "            \n",
    "            # sanity check\n",
    "            assert images.shape[0] * captions_per_image == len(encoded_captions) == len(captions_length)\n",
    "            \n",
    "            # save encoded captions and their lengths to JSON files\n",
    "            with open(os.path.join(output_dir, 'data/' + split + '_CAPTIONS_' + base_filename + '.json'), 'w') as file:\n",
    "                json.dump(encoded_captions, file)\n",
    "            \n",
    "            with open(os.path.join(output_dir, 'data/' + split + '_CAPLENS_' + base_filename + '.json'), 'w') as file:\n",
    "                json.dump(captions_length, file)\n",
    "\n",
    "    return word_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vocab = create_input_files(datasets='coco', karpathy_json_path='./datasets/karpathy_captions/datasets_coco.json',\n",
    "                                image_dir='./datasets/', output_dir='./datasets/',\n",
    "                                captions_per_image=5,\n",
    "                                min_word_freq=5,\n",
    "                                max_length=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vocab_file = os.path.join(DATA_FOLDER, 'data/' + 'WORD_VOCAB_' + DATA_NAME + '.json')\n",
    "with open(word_vocab_file, 'r') as file: word_vocab = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaptionDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, data_folder, data_name, split, transform=None):\n",
    "        \n",
    "        super(CaptionDataset, self).__init__()\n",
    "        \n",
    "        self.split = split\n",
    "        assert self.split in {'TRAIN', 'VALID', 'TEST'}\n",
    "        \n",
    "        # open hdf5 file where images are stored\n",
    "        import h5py\n",
    "        \n",
    "        self.hdf5 = h5py.File(os.path.join(data_folder, 'data/' + self.split + '_IMAGES_' + data_name + '.hdf5'), 'r')\n",
    "        self.images = self.hdf5['images']\n",
    "        \n",
    "        # captions per image\n",
    "        self.cpi = self.hdf5.attrs['captions_per_image']\n",
    "        \n",
    "        # load encoded captions (completely into memory)\n",
    "        with open(os.path.join(data_folder, 'data/' + self.split + '_CAPTIONS_' + data_name + '.json'), 'r') as file:\n",
    "            self.captions = json.load(file)\n",
    "            \n",
    "        # load captions lengths (completely into memory)\n",
    "        with open(os.path.join(data_folder, 'data'/ + self.split + '_CAPLENS_' + data_name + '.json'), 'r') as file:\n",
    "            self.caplens = json.load(file)\n",
    "            \n",
    "        # pytorch transformation pipeline for the image (normalizing, etc.)\n",
    "        self.transform = transform\n",
    "        \n",
    "        # total number of data points\n",
    "        self.dataset_size = len(self.captions)\n",
    "        \n",
    "    def __getitem_(self, i):\n",
    "        \n",
    "        # remember, the Nth caption corresponds to the (N // captions_per_image)th image\n",
    "        image = torch.FloatTensor(self.images[i // self.cpi] / 255.)\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        caption = torch.LongTensor(self.captions[i])\n",
    "        caplen = torch.LongTensor([self.caplens[i]])\n",
    "        \n",
    "        if self.split is 'TRAIN':\n",
    "            return image, caption, caplen\n",
    "        else:\n",
    "            # for validation of testing, also return all 'captions_per_image' captions to find BLEU-4 score\n",
    "            all_captions = torch.LongTensor(\n",
    "                self.captions[((i // self.cpi) * self.cpi) : (((i // self.cpi) * self.cpi) + self.cpi)])\n",
    "            return image, caption, caplen, all_captions\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.dataset_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean = (0.485, 0.456, 0.406),\n",
    "                                                     std = (0.229, 0.224, 0.225))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(CaptionDataset(DATA_FOLDER, DATA_NAME, split='TRAIN', \n",
    "                                                          transform=transform), batch_size=BATCH_SIZE, \n",
    "                                                          shuffle=True, num_workers=2, pin_memory=True)\n",
    "\n",
    "valid_loader = torch.utils.data.DataLoader(CaptionDataset(DATA_FOLDER, DATA_NAME, split='VALID',\n",
    "                                                          trasform=transform), batch_size=BATCH_SIZE, \n",
    "                                                          shuffle=True, num_workers=2, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build [Image Captioning](https://arxiv.org/pdf/1411.4555.pdf) Network with [Attention](https://arxiv.org/pdf/1502.03044.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderCNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, image_size=14):\n",
    "        \n",
    "        super(EncoderCNN, self).__init__()\n",
    "        \n",
    "        self.image_size = image_size\n",
    "        \n",
    "        # import pre-trained ImageNet ResNet-101\n",
    "        resnet = torchvision.models.resnet101(pretrained=True)\n",
    "        \n",
    "        # remove linear and pool layers\n",
    "        modules = list(resnet.children())[:-2]\n",
    "        self.resnet_layer = nn.Sequential(*modules)\n",
    "        \n",
    "        # resize image to fixed size to allow input image of variable size\n",
    "        self.adaptive_pool_layer = nn.AdaptiveAvgPool2d((image_size, image_size))\n",
    "        \n",
    "        # this will enable or disable the calculation of gradients for the Encoder's parameters\n",
    "        self.fine_tune()\n",
    "        \n",
    "    def fine_tune(self, is_fine_tune=True):\n",
    "        \n",
    "        for param in self.resnet_layer.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # if fine-tuning, then only fine-tune convolutional blocks 2 through 4\n",
    "        for child in list(self.resnet_layer.children())[5:]:\n",
    "            for param in child.parameters():\n",
    "                param.requires_grad = is_fine_tune\n",
    "                \n",
    "    def forward(self, images):\n",
    "        \n",
    "        feature_vectors = self.resnet_layer(images) # (batch_size, 2048, image_size/ 32, image_size/ 32)\n",
    "        feature_vectors = self.adaptive_pool_layer(feature_vectors) # (batch_size, 2048, image_size/ 32, image_size/ 32)\n",
    "        feature_vectors = feature_vectors.permute(0, 2, 3, 1) # (batch_size, image_size, image_size, 2048)\n",
    "        \n",
    "        return feature_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    \n",
    "    def __init__(self, encoder_size, decoder_size, attention_size):\n",
    "        \n",
    "        super(Attention, self).__init__()\n",
    "        \n",
    "        self.encoder_attention_layer = nn.Linear(encoder_size, attention_size) # linear layer to transform encoded image\n",
    "        self.decoder_attention_layer = nn.Linear(decoder_size, attention_size) # linear layer to transform decoder's output\n",
    "        self.total_attention_layer = nn.Linear(attention_size, 1) # linear layer to calculate values to be softmax-ed\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1) # softmax layer to calculate weights\n",
    "        \n",
    "    def forward(self, encoder_output, decoder_hidden):\n",
    "        \n",
    "        encoder_attention = self.encoder_attention_layer(encoder_output) # (batch_size, num_pixels, attention_size)\n",
    "        decoder_attention = self.decoder_attention_layer(decoder_hidden) # (batch_size, attention_size)\n",
    "        total_attention = self.total_attention_layer(self.relu(encoder_attention + decoder_attention.unsqueeze(1))).squeeze(2) # (batch_size, num_pixels)\n",
    "        alpha = self.softmax(total_attention) # (batch_size, num_pixels)\n",
    "        attention_weighted_encoding = (encoder_attention * alpha.unsqueeze(2)).sum(dim=1) # (batch_size, encoder_size)\n",
    "        \n",
    "        return attention_weighted_encoding, alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionDecoderRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, attention_size, embedding_size, decoder_size, vocab_size, encoder_size=2048, dropout=0.5):\n",
    "        \n",
    "        super(AttentionDecoderRNN, self).__init__()\n",
    "        \n",
    "        self.attention_size = attention_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.decoder_size = decoder_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.encoder_size = encoder_size\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        # init attention network\n",
    "        self.attention_layer = Attention(encoder_size, decoder_size, attention_size)\n",
    "        \n",
    "        self.embedding_layer = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.dropout = nn.Dropout(self.dropout)\n",
    "        self.decode_step = nn.LSTMCell(embedding_size + encoder_size, decoder_size, bias=True)\n",
    "        self.init_hidden = nn.Linear(encoder_size, decoder_size) # linear layer to find initial hidden state of LSTM\n",
    "        self.init_cell = nn.Linear(encoder_size, decoder_size) # linear layer to find initial cell state of LSTM\n",
    "        self.f_beta = nn.Linear(decoder_size, encoder_size) # linear layer to create a sigmoid-activated gate\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.fc_layer = nn.Linear(decoder_size, vocab_size) # linear layer to find scores over vocabulary\n",
    "        \n",
    "        self.init_weights() # initialize some layers with the uniform distribution\n",
    "        \n",
    "    def init_weights(self):\n",
    "        \n",
    "        self.embedding_layer.weight.data.uniform_(-0.1, 0.1)\n",
    "        self.fc.weight.data.uniform_(-0.1, 0.1)\n",
    "        self.fc.bias.data.fill_(0)\n",
    "\n",
    "    def init_state(self, encoder_outputs):\n",
    "        \n",
    "        mean_encoder_outputs = encoder_outputs.mean(dim=1)\n",
    "        hidden = self.init_hidden(mean_encoder_outputs)\n",
    "        cell = self.init_cell(mean_encoder_outputs)\n",
    "        \n",
    "        return hidden, cell\n",
    "    \n",
    "    def load_pretrained_embeddings(self, embeddings):\n",
    "        \n",
    "        self.embedding_layer.weight = nn.Parameter(embeddings)\n",
    "        \n",
    "    def fine_tune_embeddings(self, is_fine_tune=True):\n",
    "        \n",
    "        for param in self.embedding_layer.parameters():\n",
    "            param.requires_grad_ = is_fine_tune\n",
    "            \n",
    "    def forward(self, encoder_outputs, encoded_captions, caption_lengths):\n",
    "        \n",
    "        batch_size = encoder_outputs.size()\n",
    "        encoder_size = encoder_outputs.size(-1)\n",
    "        vocab_size = self.vocab_size\n",
    "        \n",
    "        # flatten image\n",
    "        encoder_outputs = encoder_outputs.view(batch_size, -1, encoder_size) # (batch_size, num_pixels, encoder_size)\n",
    "        num_pixels = encoder_outputs.size(1)\n",
    "        \n",
    "        # sort input data by decreasing lengths\n",
    "        caption_lengths, sort_id = caption_lengths.squeeze(1).sort(dim=0, descending=True)\n",
    "        encoder_outputs = encoder_outputs[sort_id]\n",
    "        encoded_captions = encoded_captions[sort_id]\n",
    "        \n",
    "        # embedding\n",
    "        embeddings = self.embedding_layer(encoded_captions) # (batch_size, max_caption_length, embedding_size)\n",
    "        \n",
    "        # init LSTM state\n",
    "        decoder_hidden, decoder_cell = self.init_state(encoder_outputs) # (batch_size, decoder_size)\n",
    "        \n",
    "        # since generation process finished as soon as model generate <end> so decoding lengths are actual lengths - 1\n",
    "        decode_lengths = (caption_lengths - 1).tolist()\n",
    "        \n",
    "        # create tensors to hold word prediction scores and alphas\n",
    "        predictions = torch.zeros(batch_size, max(decode_lengths), vocab_size).to(device)\n",
    "        alphas = torch.zeros(batch_size, max(decode_lengths), num_pixels).to(device)\n",
    "        \n",
    "        # at each time-step, decode by attention weights the encoder's output based on the decoder's previous hidden state\n",
    "        # then generate a new word in the decoder with the previous word and the attention-weighted encoding\n",
    "        \n",
    "        for d_time in range(max(decode_lengths)):\n",
    "            batch_size_time = sum([length > d_time for length in decode_lengths])\n",
    "            attention_weighted_encoding, alpha = self.attention_layer(encoder_outputs[:batch_size_time],\n",
    "                                                                decoder_hidden[:batch_size_time])\n",
    "            \n",
    "            gate = self.sigmoid(self.f_beta(decoder_hidden[:batch_size_time])) # (batch_size_time, encoder_size)\n",
    "            attention_weighted_encoding = gate * attention_weighted_encoding\n",
    "            \n",
    "            decoder_hidden, decoder_cell = self.decode_step(\n",
    "                torch.cat([embeddings[:batch_size_time, d_time, :], attention_weighted_encoding], dim=1),\n",
    "                (decoder_hidden[:batch_size_time], decoder_cell[:batch_size_time])) # (batch_size_time, decoder_size)\n",
    "            \n",
    "            prediction = self.fc_layer(self.dropout(decoder_hidden)) # (batch_size_time, vocab_size)\n",
    "            predictions[:batch_size_time, d_time, :] = prediction\n",
    "            alphas[:batch_size_time, d_time, :] = alpha\n",
    "            \n",
    "        return predictions, encoded_captions, decode_lengths, alphas, sort_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize Image Captioning Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = EncoderCNN(); encoder.fine_tune(FINE_TUNE_ENCODER)\n",
    "encoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = AttentionDecoderRNN(attention_size=ATTENTION_SIZE, embedding_size=EMBEDDING_SIZE, decoder_size=DECODER_SIZE,\n",
    "                              vocab_size=len(word_vocab), dropout=DROPOUT)\n",
    "\n",
    "decoder.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ce_loss = nn.CrossEntropyLoss()\n",
    "ce_loss.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_optimizer = torch.optim.Adam(params=filter(lambda p: p.requires_grad, encoder.parameters()),\n",
    "                                     lr=ENCODER_LR) if FINE_TUNE_ENCODER else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_optimizer = torch.optim.Adam(params=filter(lambda p: p.requires_grad, decoder.parameters()),\n",
    "                                     lr=DECODER_LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train The Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 0\n",
    "epochs_since_improvement = 0\n",
    "best_bleu4 = 0.\n",
    "print_every = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(train_loader, encoder, decoder, ce_loss, encoder_optimizer, decoder_optimizer, epoch):\n",
    "    \n",
    "    # set train mode for the networks\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    \n",
    "    start = time.time()\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top5_accuracy = AverageMeter()\n",
    "    \n",
    "    for i, (images, captions, caplens) in enumerate(train_loader):\n",
    "        \n",
    "         # set mini-batch datasets\n",
    "        images = images.to(device); captions = captions.to(device); caplens = caplens.to(device);\n",
    "        \n",
    "        encoder.zero_grad()\n",
    "        decoder.zero_grad()\n",
    "        \n",
    "        # encoding process: forward propagation\n",
    "        features = encoder(images)\n",
    "        scores, encoded_captions, decode_lengths, alphas, sort_id = decoder(images, captions, caplens)\n",
    "        \n",
    "        # since the network decode starting with <start>, the targets are all words after <start>, up to <end>\n",
    "        targets = encoded_captions[:, 1:]\n",
    "        \n",
    "        # remove timesteps that the network doesn't decode at, or are pads\n",
    "        scores, _ = pack_padded_sequence(scores, decode_lengths, batch_first=True)\n",
    "        targets, _ = pack_padded_sequence(targets, decode_lengths, batch_first=True)\n",
    "        \n",
    "        # calculate loss\n",
    "        loss = ce_loss(scores, targets)\n",
    "        loss.backward()\n",
    "        \n",
    "        # add doublt stochastic attention regularization\n",
    "        loss += ALPHA_C * ((1. - alphas.sum(dim=1)) ** 2).mean()\n",
    "        \n",
    "        # clip gradients\n",
    "        clip_gradient(encoder_optimizer, GRAD_CLIP)\n",
    "        clip_gradient(decoder_optimizer, GRAD_CLIP)\n",
    "        \n",
    "        # update weights\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "        \n",
    "        # keep track of metrics\n",
    "        top5 = calculate_accuracy(scores, targets, k=5)\n",
    "        top5_accuracy.update(top5, sum(decode_lengths))\n",
    "        losses.update(loss.item(), sum(decode_lengths))\n",
    "        batch_time.update(time.time() - start)\n",
    "        \n",
    "        start = time.time()\n",
    "        \n",
    "        # print status\n",
    "        if i % print_every == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Batch Time: {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Data Load Time: {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'Loss: {loss.val: .4f} ({loss.avg:.4f})\\t'\n",
    "                  'Top-5 Accuracy: {top5.val:.3f} ({top5.avg:.3f})'.format(epoch, i, len(train_loader),\n",
    "                                                                           batch_time=batch_time,\n",
    "                                                                           data_time=data_time, loss=losses,\n",
    "                                                                           top5=top5_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_network(valid_loader, encoder, decoder, ce_loss):\n",
    "    \n",
    "    # set eval mode for the networks\n",
    "    decoder.eval()\n",
    "    if encoder is not None: encoder.eval()\n",
    "\n",
    "    start = time.time()\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top5_accuracy = AverageMeter()\n",
    "    \n",
    "    references = list() # references as true captions for calculating BLEU-4 score\n",
    "    hypotheses = list() # hypotheses as predictions\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for i, (images, captions, caplens, allcaps) in enumerate(valid_loader):\n",
    "            \n",
    "            images = images.to(device); captions = captions.to(device); caplens = caplens.to(device);\n",
    "            \n",
    "            # forward propagation\n",
    "            if encoder is not None: images = encoder(images)\n",
    "            scores, encoded_captions, decode_lengths, alphas, sort_id = decoder(images, captions, caplens)\n",
    "            \n",
    "            # since the network decode starting with <start>, the targets are all words after <start>, up to <end>\n",
    "            targets = encoded_captions[:, 1:]\n",
    "            \n",
    "            # remove timesteps that the network doesn't decode at, or are pads\n",
    "            scores_copy = scores.clone()\n",
    "            scores, _ = pack_padded_sequence(scores, decode_lengths, batch_first=True)\n",
    "            targets, _ = pack_padded_sequence(targets, decode_lengths, batch_first=True)\n",
    "            \n",
    "            # calculate loss\n",
    "            loss = ce_loss(scores, targets)\n",
    "            loss.backward()\n",
    "            \n",
    "            # add doublt stochastic attention regularization\n",
    "            loss += ALPHA_C * ((1. - alphas.sum(dim=1)) ** 2).mean()\n",
    "\n",
    "            # keep track of metrics\n",
    "            top5 = calculate_accuracy(scores, targets, k=5)\n",
    "            top5_accuracy.update(top5, sum(decode_lengths))\n",
    "            losses.update(loss.item(), sum(decode_lengths))\n",
    "            batch_time.update(time.time() - start)\n",
    "            \n",
    "            start = time.time()\n",
    "            \n",
    "            if i % print_every == 0:\n",
    "                print('Validation: [{0}/{1}]\\t'\n",
    "                      'Batch Time: {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                      'Loss: {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                      'Top-5 Accuracy: {top5.val:.3f} ({top5.avg:.3f})\\t'.format(i, len(valid_loader),\n",
    "                                                                                 batch_time=batch_time,\n",
    "                                                                                 loss=losses,\n",
    "                                                                                 top5=top5_accuracy))\n",
    "        \n",
    "            # store references (true captions), and hypothesis (prediction) for each image\n",
    "            # if for n images, the network has n hypyothesis, and references a, b, c... for each image then the network need \n",
    "            # references = [[ref1a, ref1b, ref1c], [ref2a, ref2b, ref2c], ...], \n",
    "            # hypotheses = [[ref1a, ref1b, ref1c], [ref2a, ref2b, ref2c], ...], \n",
    "            \n",
    "            # references\n",
    "            # because images were sorted in the decoder\n",
    "            allcaps = allcaps[sort_id] \n",
    "            for j in range(allcaps.shape[0]):\n",
    "                image_caps = allcaps[j].tolist()\n",
    "                image_captions = list(\n",
    "                    map(lambda chaption: [word for word in chaption if word not in {word_vocab['<start>'], word_vocab['<pad>']}],\n",
    "                        img_caps)) # remove <start> and <pad> tokens\n",
    "                references.append(image_captions)\n",
    "                \n",
    "            # hypotheses\n",
    "            _, preds = torch.max(scores_copy, dim=2)\n",
    "            preds = preds.tolist()\n",
    "            temp_preds = list()\n",
    "            for j, p in enumerate(preds):\n",
    "                temp_preds.append(preds[j][:decode_lengths[j]]) # remove <pad> token\n",
    "            preds = temp_preds\n",
    "            hypotheses.extend(preds)\n",
    "            \n",
    "            # sanity check\n",
    "            assert len(references) == len(hypotheses)\n",
    "            \n",
    "        # calculate BLEU-4 scores\n",
    "        bleu4 = corpus_bleu(references, hypotheses)\n",
    "        \n",
    "        print('\\n * LOSS: {loss.avg:.3f}, TOP-5 ACCURACY: {top5.avg:.3f}, BLEU-4: {bleu}\\n'.format(\n",
    "               loss=losses,\n",
    "               top5=top5_accuracy,\n",
    "               bleu=bleu4))\n",
    "        \n",
    "        return bleu4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training the network...')\n",
    "for epoch in range(1, N_EPOCHS+1):\n",
    "    \n",
    "    if epochs_since_improvement > 0 and epochs_since_improvement % 8 ==0:\n",
    "        adjust_learning_rate(decoder_optimizer, 0.8)\n",
    "        if FINE_TUNE_ENCODER: adjust_learning_rate(encoder_optimizer, 0.8)\n",
    "    \n",
    "    # run one epoch's training\n",
    "    train_network(train_loader=train_loader, encoder=encoder, decoder=decoder, ce_loss=ce_loss,\n",
    "                  encoder_optimizer=encoder_optimizer, decoder_optimizer=decoder_optimizer, epoch=epoch)\n",
    "    \n",
    "    # run one epoch's validation\n",
    "    recent_bleu4 = validate_network(valid_loader=valid_loader, encoder=encoder, decoder=decoder, ce_loss=ce_loss)\n",
    "    \n",
    "    # check if there was an improvement\n",
    "    is_best = recent_bleu4 > best_bleu4\n",
    "    best_bleu4 = max(recent_bleu4, best_bleu4)\n",
    "    \n",
    "    if not is_best:\n",
    "        epochs_since_improvement += 1\n",
    "        print(f'\\nEpochs since last improvement: {epochs_since_improvement}\\n')\n",
    "    else:\n",
    "        epochs_since_improvement = 0\n",
    "        \n",
    "    # save checkpoints\n",
    "    save_checkpoint(data_name, epoch, epochs_since_improvement, \n",
    "                    encoder, decoder, encoder_optimizer, decoder_optimizer, recent_bleu4, is_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate The Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BEAM_SIZE = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_network(beam_size):\n",
    "    \n",
    "    transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean = (0.485, 0.456, 0.406),\n",
    "                                                     std = (0.229, 0.224, 0.225))])\n",
    "    \n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        CaptionDataset(DATA_FOLDER, DATA_NAME, 'TEST', transform=transform, \n",
    "                       batch_size=1, shuffle=True, num_workers=1, pin_memory=True))\n",
    "    \n",
    "    references = list()\n",
    "    hypotheses = list()\n",
    "    \n",
    "    for i, (images, captions, caplens, allcaps) in enumerate(test_loader, desc=f'EVALUATING AT BEAM SIZE {str(beam_size)}'):\n",
    "        \n",
    "        k = beam_size\n",
    "        vocab_size = len(word_vocab)\n",
    "        \n",
    "        images = images.to(device); captions = captions.to(device); caplens = caplens.to(device);\n",
    "        \n",
    "        # encoding process: forward propagation\n",
    "        features = encoder(image) # (1, encoder_image_size, encoder_image_size, encoder_size)\n",
    "        encoder_image_size = features.size(1)\n",
    "        encoder_size = features.size(3)\n",
    "        \n",
    "        # flatten features/ encoding outputs\n",
    "        encoder_outputs = features.view(1, -1, encoder_size) # (1, num_pixels, encoder_size)\n",
    "        num_pixels = encoder_outputs.size(1)\n",
    "        \n",
    "        # treat the problem as having a batch size of k\n",
    "        encoder_outputs = encoder_outputs.expand(k, num_pixels, encoder_size) # (k, num_pixels, encoder_size)\n",
    "        \n",
    "        # tensor to store top k previous words at each step (starting from just <start> token)\n",
    "        k_prev_words = torch.LongTensor([[word_vocab['<start>']]] * k).to(device) # (k, 1)\n",
    "        \n",
    "        # tensor to store top k sequences (starting from just <start> token)\n",
    "        sequences = k_prev_words # (k, 1)\n",
    "        \n",
    "        # tensor to store top k sequences' scores (starting from 0 values)\n",
    "        top_k_scores = torch.zeros(k, 1).to(device) # (k, 1)\n",
    "        \n",
    "        # lists to store completed sequences and scores\n",
    "        complete_sequences = list()\n",
    "        complete_sequences_scores = list()\n",
    "        \n",
    "        # decoding process\n",
    "        step = 1\n",
    "        hidden, cell = decoder.init_state(encoder_outputs)\n",
    "        \n",
    "        # s is a number less than or equal to k, since sequences are removed from this process once they hit <end> token\n",
    "        while True:\n",
    "            \n",
    "            embeddings = decoder.embedding_layer(k_prev_words).squeeze(1) # (s, embedding_size)\n",
    "            attention_weighted_encoding, _ = decoder.attention_layer(encoder_outputs, hidden) # (s, encoder_size), (s, num_pixels)\n",
    "            \n",
    "            gate = decoder.sigmoid(decoder.f_beta(hidden)) # gating scalar, (s, encoder_size)\n",
    "            attention_weighted_encoding = gate * attention_weighted_encoding\n",
    "            \n",
    "            hidden, cell = decoder.decode_step(torch.cat([embeddings, attention_weighted_encoding], dim=1), (hidden, cell)) # (s, decoder_size)\n",
    "            \n",
    "            scores = decoder.fc_layer(hidden) # (s, vocab_size)\n",
    "            scores = F.log_softmax(scores, dim=1)\n",
    "            \n",
    "            # add scores\n",
    "            scores = top_k_scores.expand_as(scores) + scores # (s, vocab_size)\n",
    "            \n",
    "            # for the first step, all k points will have the same scores (since same k previous words, hidden, cell)\n",
    "            if step == 1:\n",
    "                top_k_scores, top_k_words = scores[0].topk(k, 0, True, True) # (s)\n",
    "            else:\n",
    "                # unroll and find top scores, and their unrolled indices\n",
    "                top_k_scores, top_k_words = scores.view(-1).topk(k, 0, True, True) # (s)\n",
    "                \n",
    "            # convert unrolled indices to actual indices of scores\n",
    "            prev_word_indices = top_k_words / vocab_size # (s)\n",
    "            next_word_indices = top_k_words % vocab_size # (s)\n",
    "            \n",
    "            # add new words to sequences\n",
    "            sequences = torch.cat([sequences[prev_word_indices], next_word_indices.unsqueeze(1)], dim=1) # (s, step+1)\n",
    "            \n",
    "            incomplete_indices = [indices for indices, next_word in enumerate(next_word_indices) if\n",
    "                                  next_word != word_vocab['<end>']]\n",
    "            complete_indices = list(set(range(len(next_word_indices))) - set(incomplete_indices))\n",
    "            \n",
    "            # set aside complete sequences\n",
    "            if len(complete_indices) > 0:\n",
    "                complete_sequences.extend(sequences[complete_indices].tolist())\n",
    "                complete_sequences_scores.extend(top_k_scores[complete_indices])\n",
    "            k -= len(complete_indices) # reduce beam length accordingly\n",
    "            \n",
    "            # process with incomplete sequences\n",
    "            if k==0: break\n",
    "                \n",
    "            sequences = sequences[incomplete_indices]\n",
    "            hidden = hidden[prev_word_indices[incomplete_indices]]\n",
    "            cell = cell[prev_word_indices[incomplete_indices]]\n",
    "            encoder_outputs = encoder_outputs[prev_word_indices[incomplete_indices]]\n",
    "            top_k_scores = top_k_scores[incomplete_indices].unsqueeze(1)\n",
    "            k_prev_words = next_word_indices[incomplete_indices].unsqueeze(1)\n",
    "            \n",
    "            # break if thins have been going on too long\n",
    "            if step > 50: break\n",
    "            step += 1\n",
    "            \n",
    "        i = complete_sequences_scores.index(max(complete_sequences_scores))\n",
    "        sequence = complete_sequences[i]\n",
    "        \n",
    "        # references\n",
    "        image_caps = allcaps[0].tolist()\n",
    "        image_captions = list(\n",
    "            map(lambda chaption: [word for word in chaption if word not in {word_vocab['<start>'], word_vocab['<pad>']}],\n",
    "                img_caps)) # remove <start> and <pad> tokens\n",
    "        references.append(image_captions)\n",
    "        \n",
    "        # hypotheses\n",
    "        hypotheses.append([word for word in sequence if word not in {word_vocab['<start>'], word_vocab['<end>'], word_vocab['<pad>']}])\n",
    "        \n",
    "        assert len(references) == len(hypotheses)\n",
    "        \n",
    "    # calculate BLEU-4 scores\n",
    "    bleu4 = corpus_bleu(references, hypotheses)\n",
    "    \n",
    "    return bleu4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_network(BEAM_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
