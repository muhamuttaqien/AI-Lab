{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Captioning Using Deep Learning With Attention Mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "from scipy.misc import imread, imresize\n",
    "from collections import Counter\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "if is_cuda: device = torch.device('cuda')\n",
    "else: device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_input_files(datasets, karpathy_json_path, image_dir, output_dir, captions_per_image, min_word_freq, max_length=100):\n",
    "    \n",
    "    assert datasets in {'coco', 'flickr8k', 'flickr30k'}\n",
    "    \n",
    "    # read Karpathy's json\n",
    "    with open(karpathy_json_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "        \n",
    "    # read image paths and captions for each image\n",
    "    train_image_paths = []\n",
    "    train_image_captions = []\n",
    "    valid_image_paths = []\n",
    "    valid_image_captions = []\n",
    "    test_image_paths = []\n",
    "    test_image_captions = []\n",
    "    \n",
    "    word_freq = Counter()\n",
    "    \n",
    "    for image in tqdm_notebook(data['images']):\n",
    "        captions = []\n",
    "        for sentence in image['sentences']:\n",
    "            word_freq.update(sentence['tokens'])\n",
    "            if len(sentence['tokens']) <= max_length:\n",
    "                captions.append(sentence['tokens'])\n",
    "                \n",
    "        if len(captions) == 0:\n",
    "            continue\n",
    "            \n",
    "        path = os.path.join(image_dir, image['filepath'], image['filename']) if datasets == 'coco' \\\n",
    "                                                                             else os.path.join(image_dir, image['filename'])\n",
    "        \n",
    "        if image['split'] in {'train', 'restval'}:\n",
    "            train_image_paths.append(path)\n",
    "            train_image_captions.append(captions)\n",
    "        elif image['split'] in {'val'}:\n",
    "            valid_image_paths.append(path)\n",
    "            valid_image_captions.append(captions)\n",
    "        elif image['split'] in {'test'}:\n",
    "            test_image_paths.append(path)\n",
    "            test_image_captions.append(captions)\n",
    "            \n",
    "    # sanity check\n",
    "    assert len(train_image_paths) == len(train_image_captions)\n",
    "    assert len(valid_image_paths) == len(valid_image_captions)\n",
    "    assert len(test_image_paths) == len(test_image_captions)\n",
    "    \n",
    "    # create vocabulary\n",
    "    words = [word for word in word_freq.keys() if word_freq[word] > min_word_freq]\n",
    "    word_vocab = { key: value + 1 for value, key in enumerate(words)}\n",
    "    word_vocab['<unk>'] = len(word_vocab) + 1\n",
    "    word_vocab['<start>'] = len(word_vocab) + 1\n",
    "    word_vocab['<end>'] = len(word_vocab) + 1\n",
    "    word_vocab['<pad>'] = 0\n",
    "    \n",
    "    # create a base/ root name for all output files\n",
    "    base_filename = datasets + '_' + str(captions_per_image) + '_cap_per_img_' + str(min_word_freq) + '_min_word_freq'\n",
    "    \n",
    "    # save word vocabulary to a JSON\n",
    "    with open(os.path.join(output_dir, 'data/WORD_VOCAB_' + base_filename + '.json'), 'w') as file:\n",
    "        json.dump(word_vocab, file)\n",
    "        \n",
    "    # sample captions for each image, save images to HDF5 file and captions and their lengths to JSON files\n",
    "    random.seed(9)\n",
    "    for image_paths, image_captions, split in [(train_image_paths, train_image_captions, 'TRAIN'),\n",
    "                                              (valid_image_paths, valid_image_captions, 'VALID'),\n",
    "                                              (test_image_paths, test_image_captions, 'TEST')]:\n",
    "        \n",
    "        with h5py.File(os.path.join(output_dir, 'data/' + split + '_IMAGES_' + base_filename + '.hdf5'), 'a') as file:\n",
    "            \n",
    "            # make a note of the number of captions we are sampling per image\n",
    "            file.attrs['captions_per_image'] = captions_per_image\n",
    "            \n",
    "            # create dataset inside HDF5 file to store images\n",
    "            images = file.create_dataset('./datasets/images', (len(image_paths), 3, 256, 256), dtype='uint8')\n",
    "            \n",
    "            print(f'\\nReading {split} images and captions, storing to file...\\n')\n",
    "            \n",
    "            encoded_captions = []\n",
    "            captions_length = []\n",
    "            \n",
    "            for i, path in enumerate(image_paths):\n",
    "                \n",
    "                # sample captions\n",
    "                if len(image_captions[i]) < captions_per_image:\n",
    "                    captions = image_captions[i] + [random.choice(image_captions[i]) for _ in range(captions_per_image - len(image_captions[i]))]\n",
    "                else:\n",
    "                    captions = random.sample(image_captions[i], k=captions_per_image)\n",
    "                    \n",
    "                # sanity check\n",
    "                assert len(captions) == captions_per_image\n",
    "                \n",
    "                # read images\n",
    "                image = imread(image_paths[i])\n",
    "                if len(image.shape) == 2:\n",
    "                    image = image[:, :, np.newaxis]\n",
    "                    image = np.concatenate([image, image, image], axis=2)\n",
    "                image = imresize(image, (256, 256))\n",
    "                image = image.transpose(2, 0, 1)\n",
    "                \n",
    "                # sanity check\n",
    "                assert image.shape == (3, 256, 256)\n",
    "                assert np.max(image) <= 255\n",
    "                \n",
    "                # save image to HDF5 file\n",
    "                images[i] = image\n",
    "                \n",
    "                for j, caption in enumerate(captions):\n",
    "                    # encode captions\n",
    "                    encoded_caption = [word_vocab['<start>']] + [word_vocab.get(word, word_vocab['<unk>']) for word in caption] +\\\n",
    "                                      [word_vocab['<end>']] + [word_vocab['<pad>']] * (max_length - len(caption))\n",
    "                        \n",
    "                    # find caption lengths\n",
    "                    caption_length = len(caption) + 2\n",
    "                    \n",
    "                    encoded_captions.append(encoded_caption)\n",
    "                    captions_length.append(caption_length)\n",
    "            \n",
    "            # sanity check\n",
    "            assert images.shape[0] * captions_per_image == len(encoded_captions) == len(captions_length)\n",
    "            \n",
    "            # save encoded captions and their lengths to JSON files\n",
    "            with open(os.path.join(output_dir, 'data/' + split + '_CAPTIONS_' + base_filename + '.json'), 'w') as file:\n",
    "                json.dump(encoded_captions, file)\n",
    "            \n",
    "            with open(os.path.join(output_dir, 'data/' + split + '_CAPLENS_' + base_filename + '.json'), 'w') as file:\n",
    "                json.dump(captions_length, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "create_input_files(datasets='coco', karpathy_json_path='./datasets/karpathy_captions/datasets_coco.json',\n",
    "                   image_dir='./datasets/', output_dir='./datasets/',\n",
    "                   captions_per_image=5,\n",
    "                   min_word_freq=5,\n",
    "                   max_length=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CaptionDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, data_folder, data_name, split, transform=None):\n",
    "        \n",
    "        super(CaptionDataset, self).__init__()\n",
    "        \n",
    "        self.split = split\n",
    "        assert self.split in {'TRAIN', 'VALID', 'TEST'}\n",
    "        \n",
    "        # open hdf5 file where images are stored\n",
    "        self.hdf5 = h5py.File(os.path.join(data_folder, '/data' + self.split + '_IMAGES_' + data_name + '.hdf5'), 'r')\n",
    "        self.images = self.hdf5['images']\n",
    "        \n",
    "        # captions per image\n",
    "        self.cpi = self.hdf5.attrs['captions_per_image']\n",
    "        \n",
    "        # load encoded captions (completely into memory)\n",
    "        with open(os.path.join(data_folder, 'data/' + self.split + '_CAPTIONS_' + data_name + '.json'), 'r') as file:\n",
    "            self.captions = json.load(file)\n",
    "            \n",
    "        # load captions lengths (completely into memory)\n",
    "        with open(os.path.join(data_folder, 'data'/ + self.split + '_CAPLENS_' + data_name + '.json'), 'r') as file:\n",
    "            self.caplens = json.load(file)\n",
    "            \n",
    "        # pytorch transformation pipeline for the image (normalizing, etc.)\n",
    "        self.transform = transform\n",
    "        \n",
    "        # total number of data points\n",
    "        self.dataset_size = len(self.captions)\n",
    "        \n",
    "    def __getitem_(self, i):\n",
    "        \n",
    "        # remember, the Nth caption corresponds to the (N // captions_per_image)th image\n",
    "        image = torch.FloatTensor(self.images[i // self.cpi] / 255.)\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        caption = torch.LongTensor(self.captions[i])\n",
    "        caplen = torch.LongTensor([self.caplens[i]])\n",
    "        \n",
    "        if self.split is 'TRAIN':\n",
    "            return image, caption, caplen\n",
    "        else:\n",
    "            # for validation of testing, also return all 'captions_per_image' captions to find BLEU-4 score\n",
    "            all_captions = torch.LongTensor(\n",
    "                self.captions[((i // self.cpi) * self.cpi) : (((i // self.cpi) * self.cpi) + self.cpi)])\n",
    "            return image, caption, caplen, all_captions\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.dataset_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build [Image Captioning](https://arxiv.org/pdf/1411.4555.pdf) Network with [Attention](https://arxiv.org/pdf/1502.03044.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncoderCNN(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super(EncoderCNN, self).__init__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self):\n",
    "        \n",
    "        super(Attention, self).__init__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AttentionDecoderRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super(AttentionDecoderRNN, self).__init__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
