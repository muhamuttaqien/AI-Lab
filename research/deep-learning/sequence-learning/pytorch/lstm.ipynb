{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Long Short-Term Memory (LSTM), Amazon Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import bz2\n",
    "from collections import Counter\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = bz2.BZ2File('./datasets/amazon-reviews/train.ft.txt.bz2')\n",
    "test_file = bz2.BZ2File('./datasets/amazon-reviews/test.ft.txt.bz2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = train_file.readlines()\n",
    "test_file = test_file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training reviews: 3600000\n",
      "Number of test reviews: 400000\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of training reviews: \" + str(len(train_file)))\n",
    "print(\"Number of test reviews: \" + str(len(test_file)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# limit datasets will be used for training the network\n",
    "n_train = 100000 # try 800000 to get much better results\n",
    "n_test = 20000 # try 200000 to get much better results\n",
    "\n",
    "train_file = [x.decode('utf-8') for x in train_file[:n_train]]\n",
    "test_file = [x.decode('utf-8') for x in test_file[:n_test]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__label__2 Stuning even for the non-gamer: This sound track was beautiful! It paints the senery in your mind so well I would recomend it even to people who hate vid. game music! I have played the game Chrono Cross but out of all of the games I have ever played it has the best music! It backs away from crude keyboarding and takes a fresher step with grate guitars and soulful orchestras. It would impress anyone who cares to listen! ^_^\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(train_file[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting labels from sentences\n",
    "train_labels = [0 if x.split(' ')[0] == '__label__1' else 1 for x in train_file]\n",
    "train_sentences = [x.split(' ', 1)[1][:-1].lower() for x in train_file]\n",
    "\n",
    "test_labels = [0 if x.split(' ')[0] == '__label__1' else 1 for x in test_file]\n",
    "test_sentences = [x.split(' ', 1)[1][:-1].lower() for x in test_file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simply cleaning data\n",
    "for i in range(len(train_sentences)):\n",
    "    train_sentences[i] = re.sub('\\d', '0', train_sentences[i])\n",
    "    \n",
    "for i in range(len(test_sentences)):\n",
    "    test_sentences[i] = re.sub('\\d', '0', test_sentences[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modifying URLs to <url>\n",
    "for i in range(len(train_sentences)):\n",
    "    if 'www.' in train_sentences[i] or 'http:' in train_sentences[i] or 'https:' in train_sentences[i] or '.com' in train_sentences[i]:\n",
    "        train_sentences[i] = re.sub(r'([^ ]+(?<=\\.[a-z]{3}))', '<url>', train_sentences[i])\n",
    "        \n",
    "for i in range(len(test_sentences)):\n",
    "    if 'www.' in test_sentences[i] or 'http:' in test_sentences[i] or 'https:' in test_sentences[i] or '.com' in test_sentences[i]:\n",
    "        test_sentences[i] = re.sub(r'([^ ]+(?<=\\.[a-z]{3}))', '<url>', test_sentences[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_file, test_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0%.......................... successfully tokenized.\n",
      "20.0%.......................... successfully tokenized.\n",
      "40.0%.......................... successfully tokenized.\n",
      "60.0%.......................... successfully tokenized.\n",
      "80.0%.......................... successfully tokenized.\n",
      "100%.......................... successfully tokenized.\n"
     ]
    }
   ],
   "source": [
    "# create dictionary that will map a word to the number of times it appeared in all the training sentences\n",
    "words = Counter()\n",
    "for i, sentence in enumerate(train_sentences):\n",
    "    # the sentences will be stored as a list of words/ tokens\n",
    "    train_sentences[i] = []\n",
    "    # tokenizing the words\n",
    "    # task of splitting a sentence into individual tokens, which can be words or punctuation, etc.\n",
    "    for word in nltk.word_tokenize(sentence):\n",
    "        words.update([word.lower()])\n",
    "        train_sentences[i].append(word)\n",
    "    if i%20000 == 0:\n",
    "        print(str((i*100)/ n_train) + '%.......................... successfully tokenized.')\n",
    "print('100%.......................... successfully tokenized.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Unnecessary Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing the words that only appear once (typos, unexisting words)\n",
    "words = {word:n for word, n in words.items() if n>1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pad Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorting the words according to the number of appearances, with the most common word being first\n",
    "words = sorted(words, key=words.get, reverse=True)\n",
    "\n",
    "# adding padding and unknown to our vocabulary so that they will be assigned an index\n",
    "words = ['_PAD', '_UNK'] + words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionaries to store the word to index mappings and vice versa\n",
    "word2idx = {word:idx for idx,word in enumerate(words)}\n",
    "idx2word = {idx:word for idx,word in enumerate(words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sentence in enumerate(train_sentences):\n",
    "    # looking up the mapping dictionary and assigning the index to the respective words\n",
    "    train_sentences[i] = [word2idx[word] if word in word2idx else 0 for word in sentence]\n",
    "    \n",
    "for i, sentence in enumerate(test_sentences):\n",
    "    # for test sentences, we have to tokenize the sentences as well\n",
    "    test_sentences[i] = [word2idx[word.lower()] if word.lower() in word2idx else 0 for word in nltk.word_tokenize(sentence)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repad Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a function that either shortens sentences or pads sentences with 0 to a fixed length\n",
    "def pad_input(sentences, seq_len):\n",
    "    features = np.zeros((len(sentences), seq_len), dtype=int)\n",
    "    for ii, review in enumerate(sentences):\n",
    "        if len(review) != 0:\n",
    "            features[ii, -len(review):] = np.array(review)[:seq_len]\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 200 # the length that the sentences will be padded/ shortened to\n",
    "train_sentences = pad_input(train_sentences, seq_len)\n",
    "test_sentences = pad_input(test_sentences, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting our labels into numpy array\n",
    "train_labels = np.array(train_labels)\n",
    "test_labels = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_frac = 0.5\n",
    "split_id = int(split_frac * len(test_sentences))\n",
    "valid_sentences, test_sentences = test_sentences[:split_id], test_sentences[split_id:]\n",
    "valid_labels, test_labels = test_labels[:split_id], test_labels[split_id:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "if is_cuda: device = torch.device('cuda')\n",
    "else: device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(torch.from_numpy(train_sentences), torch.from_numpy(train_labels))\n",
    "valid_dataset = TensorDataset(torch.from_numpy(valid_sentences), torch.from_numpy(valid_labels))\n",
    "test_dataset = TensorDataset(torch.from_numpy(test_sentences), torch.from_numpy(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=BATCH_SIZE)\n",
    "valid_loader = DataLoader(valid_dataset, shuffle=True, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(test_dataset, shuffle=True, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape:torch.Size([16, 200])\n",
      "Output shape:torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(train_loader)\n",
    "sample_x, sample_y = dataiter.next()\n",
    "\n",
    "print('Input shape:' + str(sample_x.shape))\n",
    "print('Output shape:' + str(sample_y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(word2idx) + 1\n",
    "output_size = 1\n",
    "embedding_dim = 400\n",
    "hidden_dim = 512\n",
    "n_layers = 2\n",
    "\n",
    "LR = 0.005\n",
    "N_EPOCHS = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build LSTM Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/lstm-architecture.png' width=50% />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, embedding_dim, hidden_dim, output_size, n_layers, drop_prob=0.5):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.output_size = output_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        # optionally, we can use pre-trained word embeddings such as GloVe or fastText\n",
    "        self.embedding_layer = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm_layer = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.fc_layer = nn.Linear(hidden_dim, output_size)\n",
    "        \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device),\n",
    "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device))\n",
    "        \n",
    "        return hidden\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        \n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        x = x.long()\n",
    "        embeds = self.embedding_layer(x)\n",
    "        lstm_out, hidden = self.lstm_layer(embeds, hidden)\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "        \n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "        output = self.fc_layer(lstm_out)\n",
    "        output = self.sigmoid(output)\n",
    "        \n",
    "        output = output.view(batch_size, -1)\n",
    "        output = output[:,-1]\n",
    "        \n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize LSTM Network with hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM(\n",
       "  (embedding_layer): Embedding(62615, 400)\n",
       "  (lstm_layer): LSTM(400, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (sigmoid): Sigmoid()\n",
       "  (fc_layer): Linear(in_features=512, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm = LSTM(vocab_size, embedding_dim, hidden_dim, output_size, n_layers)\n",
    "lstm.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "bce_Loss = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(lstm.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train LSTM Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1... Step: 1000/6250... Loss: 0.157439... Valid Loss: 0.315429\n",
      "Validation loss decreased (inf --> 0.315429). Saving model...\n",
      "Epoch: 1... Step: 2000/6250... Loss: 0.056990... Valid Loss: 0.299483\n",
      "Validation loss decreased (0.315429 --> 0.299483). Saving model...\n",
      "Epoch: 1... Step: 3000/6250... Loss: 0.354859... Valid Loss: 0.274866\n",
      "Validation loss decreased (0.299483 --> 0.274866). Saving model...\n",
      "Epoch: 1... Step: 4000/6250... Loss: 0.356864... Valid Loss: 0.275680\n",
      "Epoch: 1... Step: 5000/6250... Loss: 0.243332... Valid Loss: 0.316531\n",
      "Epoch: 1... Step: 6000/6250... Loss: 0.657070... Valid Loss: 0.275620\n",
      "Epoch: 2... Step: 1000/6250... Loss: 0.149035... Valid Loss: 0.279540\n",
      "Epoch: 2... Step: 2000/6250... Loss: 0.526219... Valid Loss: 0.264183\n",
      "Validation loss decreased (0.274866 --> 0.264183). Saving model...\n",
      "Epoch: 2... Step: 3000/6250... Loss: 0.034330... Valid Loss: 0.267391\n",
      "Epoch: 2... Step: 4000/6250... Loss: 1.013978... Valid Loss: 0.361436\n",
      "Epoch: 2... Step: 5000/6250... Loss: 0.233981... Valid Loss: 0.258560\n",
      "Validation loss decreased (0.264183 --> 0.258560). Saving model...\n",
      "Epoch: 2... Step: 6000/6250... Loss: 0.403091... Valid Loss: 0.284681\n"
     ]
    }
   ],
   "source": [
    "print_every = 1000\n",
    "clip = 5\n",
    "valid_loss_min = np.Inf\n",
    "\n",
    "lstm.train()\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    counter = 0\n",
    "    \n",
    "    hidden = lstm.init_hidden(BATCH_SIZE)\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        counter += 1\n",
    "        hidden = tuple([e.data for e in hidden])\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        lstm_out, hidden = lstm(inputs.float(), hidden)\n",
    "        loss = bce_Loss(lstm_out.squeeze(), labels.float())\n",
    "        \n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(lstm.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        if counter%print_every == 0:\n",
    "            valid_hidden = lstm.init_hidden(BATCH_SIZE)\n",
    "            valid_losses = []\n",
    "            \n",
    "            lstm.eval()\n",
    "            for inputs, labels in valid_loader:\n",
    "                hidden = tuple([e.data for e in valid_hidden])\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                lstm_out, hidden = lstm(inputs, hidden)\n",
    "                valid_loss = bce_Loss(lstm_out.squeeze(), labels.float())\n",
    "                valid_losses.append(valid_loss.item())\n",
    "                \n",
    "            lstm.train()\n",
    "            print('Epoch: {}...'.format(epoch+1),\n",
    "                  'Step: {}/{}...'.format(counter, len(train_loader)),\n",
    "                  'Loss: {:.6f}...'.format(loss.item()),\n",
    "                  'Valid Loss: {:.6f}'.format(np.mean(valid_losses)))\n",
    "            \n",
    "            if np.mean(valid_losses) <= valid_loss_min:\n",
    "                torch.save(lstm.state_dict(), './weights/lstm_epoch{}_loss{}'.format(epoch+1, np.mean(valid_losses)))\n",
    "                print('Validation loss decreased ({:.6f} --> {:.6f}). Saving model...'.format(valid_loss_min, np.mean(valid_losses)))\n",
    "                valid_loss_min = np.mean(valid_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test LSTM Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm.load_state_dict(torch.load('./weights/lstm_epoch2_loss0.2585601684451103'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_correct = 0\n",
    "\n",
    "test_hidden = lstm.init_hidden(BATCH_SIZE)\n",
    "test_losses = []\n",
    "\n",
    "lstm.eval()\n",
    "for inputs, labels in test_loader:\n",
    "    hidden = tuple([e.data for e in test_hidden])\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "    lstm_out, hidden = lstm(inputs.float(), hidden)\n",
    "    test_loss = bce_Loss(lstm_out.squeeze(), labels.float())\n",
    "    test_losses.append(test_loss.item())\n",
    "    \n",
    "    pred = torch.round(lstm_out.squeeze())\n",
    "    correct_tensor = pred.eq(labels.float().view_as(pred))\n",
    "    correct = np.squeeze(correct_tensor.cpu().numpy())\n",
    "    n_correct += np.sum(correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.261\n",
      "Test Accuracy: 89.570%\n"
     ]
    }
   ],
   "source": [
    "print('Test Loss: {:.3f}'.format(np.mean(test_losses)))\n",
    "test_accuracy = n_correct/ len(test_loader.dataset)\n",
    "print('Test Accuracy: {:.3f}%'.format(test_accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
