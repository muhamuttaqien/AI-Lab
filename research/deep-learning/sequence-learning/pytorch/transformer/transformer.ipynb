{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer: A Novel [Google](https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html) Language Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchtext import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils import create_masks, show_plot_evaluation, translate_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from Iterator import batch_size_fn, Iterator\n",
    "from Tokenizer import Tokenizer\n",
    "from Embedder import Embedder\n",
    "from PositionalEncoder import PositionalEncoder\n",
    "from Sublayers import Norm, MultiHeadSelfAttention, FeedForward\n",
    "from Layers import EncoderLayer, DecoderLayer\n",
    "from Scheduler import CosineWithRestarts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N_EPOCHS = 100\n",
    "N_LAYERS = 6\n",
    "N_HEADS = 8\n",
    "D_MODEL = EMBEDDING_DIM = 512 # this can be the length of the longest sentence in our training dataset\n",
    "MAX_LENGTH = 80\n",
    "\n",
    "DROPOUT = 0.1\n",
    "BATCH_SIZE = 1500\n",
    "LR = 0.0001\n",
    "\n",
    "BETA1 = 0.9\n",
    "BETA2 = 0.98\n",
    "EPS = 1e-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "if is_cuda: device = torch.device('cuda')\n",
    "else: device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SOURCE_LANG = 'en'\n",
    "TARGET_LANG = 'fr'\n",
    "\n",
    "SOURCE_DATA = open('./datasets/english.txt').read().strip().split('\\n')\n",
    "TARGET_DATA = open('./datasets/french.txt').read().strip().split('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "token_source = Tokenizer(SOURCE_LANG)\n",
    "token_target = Tokenizer(TARGET_LANG)\n",
    "\n",
    "SOURCE_FIELD = data.Field(lower=True, tokenize=token_source.tokenize)\n",
    "TARGET_FIELD = data.Field(lower=True, tokenize=token_target.tokenize, init_token='<SOS>', eos_token='<EOS>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SOURCE</th>\n",
       "      <th>TARGET</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Va !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Run!</td>\n",
       "      <td>Cours !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Run!</td>\n",
       "      <td>Courez !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fire!</td>\n",
       "      <td>Au feu !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Help!</td>\n",
       "      <td>À l'aide !</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  SOURCE      TARGET\n",
       "0    Go.        Va !\n",
       "1   Run!     Cours !\n",
       "2   Run!    Courez !\n",
       "3  Fire!    Au feu !\n",
       "4  Help!  À l'aide !"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data = {'SOURCE': [line for line in SOURCE_DATA], 'TARGET': [line for line in TARGET_DATA]}\n",
    "df_datasets = pd.DataFrame(raw_data, columns=['SOURCE', 'TARGET'])\n",
    "df_datasets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mask = (df_datasets['SOURCE'].str.count(' ') < MAX_LENGTH) & (df_datasets['TARGET'].str.count(' ') < MAX_LENGTH)\n",
    "df_datasets = df_datasets.loc[mask]\n",
    "\n",
    "df_datasets.to_csv('./datasets/translate_transformer_temp.csv', index=False)\n",
    "data_fields = [('SOURCE', SOURCE_FIELD), ('TARGET', TARGET_FIELD)]\n",
    "train = data.TabularDataset('./datasets/translate_transformer_temp.csv', format='csv', fields=data_fields)\n",
    "train_iter = Iterator(train, batch_size=BATCH_SIZE, device=device, repeat=False, \n",
    "                      sort_key=lambda x: (len(x.SOURCE), len(x.TARGET)),\n",
    "                      batch_size_fn=batch_size_fn, train=True, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SOURCE_FIELD.build_vocab(train)\n",
    "TARGET_FIELD.build_vocab(train)\n",
    "\n",
    "pickle.dump(SOURCE_FIELD, open('datasets/SOURCE.pkl', 'wb'))\n",
    "pickle.dump(TARGET_FIELD, open('datasets/TARGET.pkl', 'wb'))\n",
    "\n",
    "SOURCE_PAD = SOURCE_FIELD.vocab.stoi['<PAD>']\n",
    "TARGET_PAD = TARGET_FIELD.vocab.stoi['<PAD>']\n",
    "\n",
    "for i, b in enumerate(train_iter): \n",
    "    TRAIN_LENGTH = i\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "source_vocab_size = len(SOURCE_FIELD.vocab)\n",
    "target_vocab_size = len(TARGET_FIELD.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build [Transformer](https://arxiv.org/pdf/1706.03762.pdf) Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, d_model, N, heads, dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "            \n",
    "        # function for duplicating encoder layers\n",
    "        def get_clones(module, N):\n",
    "            return nn.ModuleList([copy.deepcopy(module) for i in range(N)])\n",
    "\n",
    "        self.N = N\n",
    "        self.embedding_layer = Embedder(vocab_size, d_model)\n",
    "        self.pe = PositionalEncoder(d_model, dropout=dropout)\n",
    "        self.encoder_layer = get_clones(EncoderLayer(d_model, heads, dropout), N)\n",
    "        self.norm = Norm(d_model)\n",
    "    \n",
    "    def forward(self, source_seq, mask):\n",
    "        \n",
    "        source_seq = source_seq.to(device)\n",
    "        \n",
    "        x = self.embedding_layer(source_seq)\n",
    "        x = self.pe(x)\n",
    "        for i in range(self.N):\n",
    "            x = self.encoder_layer[i](x, mask, device)\n",
    "            \n",
    "        x = self.norm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, d_model, N, heads, dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        # function for duplicating decoder layers\n",
    "        def get_clones(module, N):\n",
    "            return nn.ModuleList([copy.deepcopy(module) for i in range(N)])\n",
    "        \n",
    "        self.N = N\n",
    "        self.embedding_layer = Embedder(vocab_size, d_model)\n",
    "        self.pe = PositionalEncoder(d_model, dropout=dropout)\n",
    "        self.decoder_layer = get_clones(DecoderLayer(d_model, heads, dropout), N)\n",
    "        self.norm = Norm(d_model)\n",
    "        \n",
    "    def forward(self, target_seq, encoder_outputs, source_mask, target_mask):\n",
    "        \n",
    "        target_seq = target_seq.to(device)\n",
    "        \n",
    "        x = self.embedding_layer(target_seq)\n",
    "        x = self.pe(x)\n",
    "        for i in range(self.N):\n",
    "            x = self.decoder_layer[i](x, encoder_outputs, source_mask, target_mask, device)\n",
    "            \n",
    "        x = self.norm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \n",
    "    def __init__(self, source_vocab, target_vocab, d_model, N, heads, dropout):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        self.encoder = Encoder(source_vocab, d_model, N, heads, dropout)\n",
    "        self.decoder = Decoder(target_vocab, d_model, N, heads, dropout)\n",
    "        self.fc_layer = nn.Linear(d_model, target_vocab)\n",
    "        \n",
    "    # this transformer don't perform softmax on the output\n",
    "    # the process will be handled automatically by our loss function\n",
    "    def forward(self, source_seq, target_seq, source_mask, target_mask):\n",
    "        \n",
    "        encoder_outputs = self.encoder(source_seq, source_mask)\n",
    "        decoder_output = self.decoder(target_seq, encoder_outputs, source_mask, target_mask)\n",
    "        output = self.fc_layer(decoder_output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize Transformer Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transformer = Transformer(source_vocab_size, target_vocab_size, D_MODEL, N_LAYERS, N_HEADS, DROPOUT)\n",
    "transformer.to(device)\n",
    "for p in transformer.parameters(): \n",
    "    if p.dim() > 1: nn.init.xavier_uniform_(p)      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ce_loss(predictions, labels, ignore_index):\n",
    "    \n",
    "    loss = F.cross_entropy(predictions.view(-1, predictions.size(-1)), labels, ignore_index=ignore_index)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=LR, betas=(BETA1, BETA2), eps=EPS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scheduler = CosineWithRestarts(optimizer=optimizer, T_max=TRAIN_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Transformer Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the network...\n",
      "Time: 2m, Epoch: 1 [####################] 100%, Loss = 5.148\n",
      "Epoch 1 complete, Loss: 5.148\n",
      "Time: 4m, Epoch: 2 [####################] 100%, Loss = 4.620\n",
      "Epoch 2 complete, Loss: 4.620\n",
      "Time: 6m, Epoch: 3 [####################] 100%, Loss = 4.121\n",
      "Epoch 3 complete, Loss: 4.121\n",
      "Time: 9m, Epoch: 4 [####################] 100%, Loss = 3.681\n",
      "Epoch 4 complete, Loss: 3.681\n",
      "Time: 11m, Epoch: 5 [####################] 100%, Loss = 3.494\n",
      "Epoch 5 complete, Loss: 3.494\n",
      "Time: 13m, Epoch: 6 [####################] 100%, Loss = 3.249\n",
      "Epoch 6 complete, Loss: 3.249\n",
      "Time: 16m, Epoch: 7 [####################] 100%, Loss = 3.063\n",
      "Epoch 7 complete, Loss: 3.063\n",
      "Time: 18m, Epoch: 8 [####################] 100%, Loss = 2.865\n",
      "Epoch 8 complete, Loss: 2.865\n",
      "Time: 20m, Epoch: 9 [####################] 100%, Loss = 2.594\n",
      "Epoch 9 complete, Loss: 2.594\n",
      "Time: 23m, Epoch: 10 [####################] 100%, Loss = 2.411\n",
      "Epoch 10 complete, Loss: 2.411\n",
      "Time: 25m, Epoch: 11 [####################] 100%, Loss = 2.263\n",
      "Epoch 11 complete, Loss: 2.263\n",
      "Time: 27m, Epoch: 12 [####################] 100%, Loss = 2.186\n",
      "Epoch 12 complete, Loss: 2.186\n",
      "Time: 30m, Epoch: 13 [####################] 100%, Loss = 2.036\n",
      "Epoch 13 complete, Loss: 2.036\n",
      "Time: 32m, Epoch: 14 [####################] 100%, Loss = 1.956\n",
      "Epoch 14 complete, Loss: 1.956\n",
      "Time: 34m, Epoch: 15 [####################] 100%, Loss = 1.762\n",
      "Epoch 15 complete, Loss: 1.762\n",
      "Time: 37m, Epoch: 16 [####################] 100%, Loss = 1.722\n",
      "Epoch 16 complete, Loss: 1.722\n",
      "Time: 39m, Epoch: 17 [####################] 100%, Loss = 1.651\n",
      "Epoch 17 complete, Loss: 1.651\n",
      "Time: 41m, Epoch: 18 [####################] 100%, Loss = 1.559\n",
      "Epoch 18 complete, Loss: 1.559\n",
      "Time: 44m, Epoch: 19 [####################] 100%, Loss = 1.493\n",
      "Epoch 19 complete, Loss: 1.493\n",
      "Time: 46m, Epoch: 20 [####################] 100%, Loss = 1.435\n",
      "Epoch 20 complete, Loss: 1.435\n",
      "Time: 48m, Epoch: 21 [####################] 100%, Loss = 1.396\n",
      "Epoch 21 complete, Loss: 1.396\n",
      "Time: 51m, Epoch: 22 [####################] 100%, Loss = 1.299\n",
      "Epoch 22 complete, Loss: 1.299\n",
      "Time: 53m, Epoch: 23 [####################] 100%, Loss = 1.184\n",
      "Epoch 23 complete, Loss: 1.184\n",
      "Time: 55m, Epoch: 24 [####################] 100%, Loss = 1.225\n",
      "Epoch 24 complete, Loss: 1.225\n",
      "Time: 58m, Epoch: 25 [####################] 100%, Loss = 1.140\n",
      "Epoch 25 complete, Loss: 1.140\n",
      "Time: 60m, Epoch: 26 [####################] 100%, Loss = 1.146\n",
      "Epoch 26 complete, Loss: 1.146\n",
      "Time: 62m, Epoch: 27 [####################] 100%, Loss = 1.089\n",
      "Epoch 27 complete, Loss: 1.089\n",
      "Time: 65m, Epoch: 28 [####################] 100%, Loss = 1.006\n",
      "Epoch 28 complete, Loss: 1.006\n",
      "Time: 67m, Epoch: 29 [####################] 100%, Loss = 0.970\n",
      "Epoch 29 complete, Loss: 0.970\n",
      "Time: 69m, Epoch: 30 [####################] 100%, Loss = 0.980\n",
      "Epoch 30 complete, Loss: 0.980\n",
      "Time: 72m, Epoch: 31 [####################] 100%, Loss = 0.921\n",
      "Epoch 31 complete, Loss: 0.921\n",
      "Time: 74m, Epoch: 32 [####################] 100%, Loss = 0.944\n",
      "Epoch 32 complete, Loss: 0.944\n",
      "Time: 76m, Epoch: 33 [####################] 100%, Loss = 0.897\n",
      "Epoch 33 complete, Loss: 0.897\n",
      "Time: 79m, Epoch: 34 [####################] 100%, Loss = 0.833\n",
      "Epoch 34 complete, Loss: 0.833\n",
      "Time: 81m, Epoch: 35 [####################] 100%, Loss = 0.855\n",
      "Epoch 35 complete, Loss: 0.855\n",
      "Time: 84m, Epoch: 36 [####################] 100%, Loss = 0.782\n",
      "Epoch 36 complete, Loss: 0.782\n",
      "Time: 86m, Epoch: 37 [####################] 100%, Loss = 0.821\n",
      "Epoch 37 complete, Loss: 0.821\n",
      "Time: 88m, Epoch: 38 [####################] 100%, Loss = 0.798\n",
      "Epoch 38 complete, Loss: 0.798\n",
      "Time: 91m, Epoch: 39 [####################] 100%, Loss = 0.742\n",
      "Epoch 39 complete, Loss: 0.742\n",
      "Time: 93m, Epoch: 40 [####################] 100%, Loss = 0.705\n",
      "Epoch 40 complete, Loss: 0.705\n",
      "Time: 95m, Epoch: 41 [####################] 100%, Loss = 0.731\n",
      "Epoch 41 complete, Loss: 0.731\n",
      "Time: 98m, Epoch: 42 [####################] 100%, Loss = 0.705\n",
      "Epoch 42 complete, Loss: 0.705\n",
      "Time: 100m, Epoch: 43 [####################] 100%, Loss = 0.738\n",
      "Epoch 43 complete, Loss: 0.738\n",
      "Time: 102m, Epoch: 44 [####################] 100%, Loss = 0.650\n",
      "Epoch 44 complete, Loss: 0.650\n",
      "Time: 105m, Epoch: 45 [####################] 100%, Loss = 0.665\n",
      "Epoch 45 complete, Loss: 0.665\n",
      "Time: 107m, Epoch: 46 [####################] 100%, Loss = 0.666\n",
      "Epoch 46 complete, Loss: 0.666\n",
      "Time: 109m, Epoch: 47 [####################] 100%, Loss = 0.646\n",
      "Epoch 47 complete, Loss: 0.646\n",
      "Time: 112m, Epoch: 48 [####################] 100%, Loss = 0.601\n",
      "Epoch 48 complete, Loss: 0.601\n",
      "Time: 114m, Epoch: 49 [####################] 100%, Loss = 0.621\n",
      "Epoch 49 complete, Loss: 0.621\n",
      "Time: 116m, Epoch: 50 [####################] 100%, Loss = 0.593\n",
      "Epoch 50 complete, Loss: 0.593\n",
      "Time: 119m, Epoch: 51 [####################] 100%, Loss = 0.561\n",
      "Epoch 51 complete, Loss: 0.561\n",
      "Time: 121m, Epoch: 52 [####################] 100%, Loss = 0.560\n",
      "Epoch 52 complete, Loss: 0.560\n",
      "Time: 123m, Epoch: 53 [####################] 100%, Loss = 0.578\n",
      "Epoch 53 complete, Loss: 0.578\n",
      "Time: 126m, Epoch: 54 [####################] 100%, Loss = 0.579\n",
      "Epoch 54 complete, Loss: 0.579\n",
      "Time: 128m, Epoch: 55 [####################] 100%, Loss = 0.548\n",
      "Epoch 55 complete, Loss: 0.548\n",
      "Time: 131m, Epoch: 56 [####################] 100%, Loss = 0.532\n",
      "Epoch 56 complete, Loss: 0.532\n",
      "Time: 133m, Epoch: 57 [####################] 100%, Loss = 0.537\n",
      "Epoch 57 complete, Loss: 0.537\n",
      "Time: 135m, Epoch: 58 [####################] 100%, Loss = 0.526\n",
      "Epoch 58 complete, Loss: 0.526\n",
      "Time: 138m, Epoch: 59 [####################] 100%, Loss = 0.485\n",
      "Epoch 59 complete, Loss: 0.485\n",
      "Time: 140m, Epoch: 60 [####################] 100%, Loss = 0.504\n",
      "Epoch 60 complete, Loss: 0.504\n",
      "Time: 142m, Epoch: 61 [####################] 100%, Loss = 0.490\n",
      "Epoch 61 complete, Loss: 0.490\n",
      "Time: 145m, Epoch: 62 [####################] 100%, Loss = 0.497\n",
      "Epoch 62 complete, Loss: 0.497\n",
      "Time: 147m, Epoch: 63 [####################] 100%, Loss = 0.483\n",
      "Epoch 63 complete, Loss: 0.483\n",
      "Time: 149m, Epoch: 64 [####################] 100%, Loss = 0.470\n",
      "Epoch 64 complete, Loss: 0.470\n",
      "Time: 151m, Epoch: 65 [####################] 100%, Loss = 0.479\n",
      "Epoch 65 complete, Loss: 0.479\n",
      "Time: 154m, Epoch: 66 [####################] 100%, Loss = 0.478\n",
      "Epoch 66 complete, Loss: 0.478\n",
      "Time: 156m, Epoch: 67 [####################] 100%, Loss = 0.456\n",
      "Epoch 67 complete, Loss: 0.456\n",
      "Time: 158m, Epoch: 68 [####################] 100%, Loss = 0.460\n",
      "Epoch 68 complete, Loss: 0.460\n",
      "Time: 161m, Epoch: 69 [####################] 100%, Loss = 0.443\n",
      "Epoch 69 complete, Loss: 0.443\n",
      "Time: 163m, Epoch: 70 [####################] 100%, Loss = 0.428\n",
      "Epoch 70 complete, Loss: 0.428\n",
      "Time: 165m, Epoch: 71 [####################] 100%, Loss = 0.429\n",
      "Epoch 71 complete, Loss: 0.429\n",
      "Time: 168m, Epoch: 72 [####################] 100%, Loss = 0.428\n",
      "Epoch 72 complete, Loss: 0.428\n",
      "Time: 172m, Epoch: 74 [####################] 100%, Loss = 0.410\n",
      "Epoch 74 complete, Loss: 0.410\n",
      "Time: 175m, Epoch: 75 [####################] 100%, Loss = 0.430\n",
      "Epoch 75 complete, Loss: 0.430\n",
      "Time: 177m, Epoch: 76 [####################] 100%, Loss = 0.423\n",
      "Epoch 76 complete, Loss: 0.423\n",
      "Time: 179m, Epoch: 77 [####################] 100%, Loss = 0.412\n",
      "Epoch 77 complete, Loss: 0.412\n",
      "Time: 182m, Epoch: 78 [####################] 100%, Loss = 0.414\n",
      "Epoch 78 complete, Loss: 0.414\n",
      "Time: 184m, Epoch: 79 [####################] 100%, Loss = 0.406\n",
      "Epoch 79 complete, Loss: 0.406\n",
      "Time: 186m, Epoch: 80 [####################] 100%, Loss = 0.406\n",
      "Epoch 80 complete, Loss: 0.406\n",
      "Time: 188m, Epoch: 81 [####################] 100%, Loss = 0.382\n",
      "Epoch 81 complete, Loss: 0.382\n",
      "Time: 191m, Epoch: 82 [####################] 100%, Loss = 0.393\n",
      "Epoch 82 complete, Loss: 0.393\n",
      "Time: 193m, Epoch: 83 [####################] 100%, Loss = 0.384\n",
      "Epoch 83 complete, Loss: 0.384\n",
      "Time: 195m, Epoch: 84 [####################] 100%, Loss = 0.406\n",
      "Epoch 84 complete, Loss: 0.406\n",
      "Time: 198m, Epoch: 85 [####################] 100%, Loss = 0.397\n",
      "Epoch 85 complete, Loss: 0.397\n",
      "Time: 200m, Epoch: 86 [####################] 100%, Loss = 0.397\n",
      "Epoch 86 complete, Loss: 0.397\n",
      "Time: 202m, Epoch: 87 [####################] 100%, Loss = 0.386\n",
      "Epoch 87 complete, Loss: 0.386\n",
      "Time: 205m, Epoch: 88 [####################] 100%, Loss = 0.369\n",
      "Epoch 88 complete, Loss: 0.369\n",
      "Time: 207m, Epoch: 89 [####################] 100%, Loss = 0.380\n",
      "Epoch 89 complete, Loss: 0.380\n",
      "Time: 209m, Epoch: 90 [####################] 100%, Loss = 0.376\n",
      "Epoch 90 complete, Loss: 0.376\n",
      "Time: 212m, Epoch: 91 [####################] 100%, Loss = 0.366\n",
      "Epoch 91 complete, Loss: 0.366\n",
      "Time: 214m, Epoch: 92 [####################] 100%, Loss = 0.371\n",
      "Epoch 92 complete, Loss: 0.371\n",
      "Time: 216m, Epoch: 93 [####################] 100%, Loss = 0.371\n",
      "Epoch 93 complete, Loss: 0.371\n",
      "Time: 219m, Epoch: 94 [####################] 100%, Loss = 0.374\n",
      "Epoch 94 complete, Loss: 0.374\n",
      "Time: 221m, Epoch: 95 [####################] 100%, Loss = 0.361\n",
      "Epoch 95 complete, Loss: 0.361\n",
      "Time: 223m, Epoch: 96 [####################] 100%, Loss = 0.370\n",
      "Epoch 96 complete, Loss: 0.370\n",
      "Time: 226m, Epoch: 97 [####################] 100%, Loss = 0.360\n",
      "Epoch 97 complete, Loss: 0.360\n",
      "Time: 228m, Epoch: 98 [####################] 100%, Loss = 0.362\n",
      "Epoch 98 complete, Loss: 0.362\n",
      "Time: 230m, Epoch: 99 [####################] 100%, Loss = 0.358\n",
      "Epoch 99 complete, Loss: 0.358\n",
      "Time: 232m, Epoch: 100 [####################] 100%, Loss = 0.372\n",
      "Epoch 100 complete, Loss: 0.372\n"
     ]
    }
   ],
   "source": [
    "temp = tick = time.time()\n",
    "\n",
    "losses_history = []\n",
    "print_every = 100\n",
    "\n",
    "print('Training the network...')\n",
    "transformer.train()\n",
    "\n",
    "for epoch in range(1, N_EPOCHS+1):\n",
    "    \n",
    "    total_loss = 0\n",
    "    for i, batch in enumerate(train_iter):\n",
    "        \n",
    "        source = batch.SOURCE.transpose(0,1)\n",
    "        target = batch.TARGET.transpose(0,1)\n",
    "        target_input = target[:, :-1]\n",
    "        source_mask, target_mask = create_masks(source, target_input, SOURCE_PAD, TARGET_PAD, device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        predictions = transformer(source, target_input, source_mask, target_mask)\n",
    "        labels = target[:, 1:].contiguous().view(-1)\n",
    "        loss = ce_loss(predictions, labels, ignore_index=TARGET_PAD)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        if i % print_every == 0:\n",
    "            p = int(100 * i / TRAIN_LENGTH)\n",
    "            avg_loss = total_loss/ print_every\n",
    "            print('Time: %dm, Epoch: %d [%s%s] %d%%, Loss = %.3f' %\\\n",
    "                  ((time.time() - tick)//60, epoch, ''.join('#'*(p//5)), ''.join(' '*(20-(p//5))), p, avg_loss), end='\\r')\n",
    "            \n",
    "            total_loss = 0\n",
    "    \n",
    "    # plot average loss for evaluation\n",
    "    losses_history.append(avg_loss)\n",
    "    print('Time: %dm, Epoch: %d [%s%s] %d%%, Loss = %.3f\\nEpoch %d complete, Loss: %.03f' %\\\n",
    "        ((time.time() - tick)//60, epoch, ''.join('#'*(100//5)), ''.join(' '*(20-(100//5))), 100, avg_loss, epoch, avg_loss))\n",
    "          \n",
    "    # save models per each epoch\n",
    "    if not os.path.exists('./weights/'): os.makedirs('./weights/')\n",
    "    torch.save(transformer.state_dict(), 'weights/transformer.hdf5')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_plot_evaluation(losses_history, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate The Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> À quoi penses tu?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = 'what are you thinking?'\n",
    "\n",
    "phrase = translate_text(text, transformer, SOURCE_FIELD, TARGET_FIELD, MAX_LENGTH, device)\n",
    "print('> ' + phrase + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> J'adore l'intelligence artificielle.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = 'i love artificial intelligence'\n",
    "\n",
    "phrase = translate_text(text, transformer, SOURCE_FIELD, TARGET_FIELD, MAX_LENGTH, device)\n",
    "print('> ' + phrase + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Vous êtes merveilleux.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = 'you are wonderful!'\n",
    "\n",
    "phrase = translate_text(text, transformer, SOURCE_FIELD, TARGET_FIELD, MAX_LENGTH, device)\n",
    "print('> ' + phrase + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Nombre pour l'avenir sont brillant.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = 'for the brighter future!'\n",
    "\n",
    "phrase = translate_text(text, transformer, SOURCE_FIELD, TARGET_FIELD, MAX_LENGTH, device)\n",
    "print('> ' + phrase + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Si nous étions en mesure de voyager autour des gens, le peuvent bien acheter le week end dernier?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = 'If we wanted to travel around the world, could we buy the tickets tomorrow?'\n",
    "\n",
    "phrase = translate_text(text, transformer, SOURCE_FIELD, TARGET_FIELD, MAX_LENGTH, device)\n",
    "print('> ' + phrase + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
