{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Embedder import Embedder\n",
    "from PositionalEncoder import PositionalEncoder\n",
    "from Layers import EncoderLayer, DecoderLayer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 2\n",
    "N_LAYERS = 6\n",
    "N_HEADS = 8\n",
    "\n",
    "DROPOUT = 0.1\n",
    "BATCH_SIZE = 1500\n",
    "LR = 0.0001\n",
    "\n",
    "BETAS1= 0.9\n",
    "BETAS2= 0.98\n",
    "EPS =1e-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "if is_cuda: device = torch.device('cuda')\n",
    "else: device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE_DATA = open('./datasets/french.txt').read().strip().split('\\n')\n",
    "TARGET_DATA = open('./datasets/english.txt').read().strip().split('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize.py, batch.py, process.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build [Transformer](https://arxiv.org/pdf/1706.03762.pdf) Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, d_model, N, heads, dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "            \n",
    "        def get_clones(module, N):\n",
    "            return nn.ModuleList([copy.deepcopy(module) for i in range(N)])\n",
    "\n",
    "        self.N = N\n",
    "        self.embedding_layer = Embedder(vocab_size, d_model)\n",
    "        self.pe = PositionalEncoder(d_model, dropout=dropout)\n",
    "        self.encoder_layer = get_clones(EncoderLayer(d_model, heads, dropout), N)\n",
    "        self.norm = Norm(d_model)\n",
    "    \n",
    "    def forward(self, input_seq, mask):\n",
    "        \n",
    "        x = self.embedding_layer(input_seq)\n",
    "        x = self.pe(x)\n",
    "        for i in range(self.N):\n",
    "            x = self.encoder_layer[i](x, mask)\n",
    "            \n",
    "        x = self.norm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, d_model, N, heads, dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        def get_clones(module, N):\n",
    "            return nn.ModuleList([copy.deepcopy(module) for i in range(N)])\n",
    "        \n",
    "        self.N = N\n",
    "        self.embedding_layer = Embedder(vocab_size, d_model)\n",
    "        self.pe = PositionalEncoder(d_model, dropout=dropout)\n",
    "        self.decoder_layer = get_clones(DecoderLayer(d_model, heads, dropout), N)\n",
    "        self.norm = Norm(d_model)\n",
    "        \n",
    "    def forward(self, target_seq, encoder_outputs, source_mask, target_mask):\n",
    "        \n",
    "        x = self.embedding_layer(target_seq)\n",
    "        x = self.pe(x)\n",
    "        for i in range(self.N):\n",
    "            x = self.decoder_layer[i](x, encoder_outputs, source_mask, target_mask)\n",
    "            \n",
    "        x = self.norm()\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \n",
    "    def __init__(self, source_vocab, target_vocab, d_model, N, heads, dropout):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        self.encoder = Encoder(source_vocab, d_model, N, heads, dropout)\n",
    "        self.decoder = Decoder(target_vocab, d_model, N, heads, dropout)\n",
    "        slef.fc_layer = nn.Linear(d_model, target_vocab)\n",
    "        \n",
    "    def forward(self, source_seq, target_seq, source_mask, target_mask):\n",
    "        \n",
    "        encoder_outputs = self.encoder(source_seq, source_mask)\n",
    "        decoder_output = self.decoder(target_seq, encoder_outputs, source_mask, target_mask)\n",
    "        output = self.fc_layer(decoder_output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize Transformer Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(source_vocab, target_vocab, d_model, n_layers, heads, dropout)\n",
    "transformer.to(device)\n",
    "for p in transformer.parameters():\n",
    "    if p.dim() > 1: nn.init.xavier_uniform_(p)      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
