{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import Tokenizer, Iterator, batch_size_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchtext import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Embedder import Embedder\n",
    "from PositionalEncoder import PositionalEncoder\n",
    "from Sublayers import Norm, MultiHeadSelfAttention, FeedForward\n",
    "from Layers import EncoderLayer, DecoderLayer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 2\n",
    "N_LAYERS = 6\n",
    "N_HEADS = 8\n",
    "D_MODEL = EMBEDDING_DIM = 512\n",
    "MAX_LENGTH = 80\n",
    "\n",
    "DROPOUT = 0.1\n",
    "BATCH_SIZE = 1500\n",
    "LR = 0.0001\n",
    "\n",
    "BETAS1= 0.9\n",
    "BETAS2= 0.98\n",
    "EPS =1e-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "if is_cuda: device = torch.device('cuda')\n",
    "else: device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE_LANG = 'fr'\n",
    "TARGET_LANG = 'en'\n",
    "\n",
    "SOURCE_DATA = open('./datasets/french.txt').read().strip().split('\\n')\n",
    "TARGET_DATA = open('./datasets/english.txt').read().strip().split('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_source = Tokenizer(SOURCE_LANG)\n",
    "tok_target = Tokenizer(TARGET_LANG)\n",
    "\n",
    "SOURCE_FIELD = data.Field(lower=True, tokenize=tok_source.tokenize)\n",
    "TARGET_FIELD = data.Field(lower=True, tokenize=tok_target.tokenize, init_token='<SOS>', eos_token='<EOS>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SOURCE</th>\n",
       "      <th>TARGET</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Va !</td>\n",
       "      <td>Go.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cours !</td>\n",
       "      <td>Run!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Courez !</td>\n",
       "      <td>Run!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Au feu !</td>\n",
       "      <td>Fire!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>À l'aide !</td>\n",
       "      <td>Help!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       SOURCE TARGET\n",
       "0        Va !    Go.\n",
       "1     Cours !   Run!\n",
       "2    Courez !   Run!\n",
       "3    Au feu !  Fire!\n",
       "4  À l'aide !  Help!"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data = {'SOURCE': [line for line in SOURCE_DATA], 'TARGET': [line for line in TARGET_DATA]}\n",
    "df_datasets = pd.DataFrame(raw_data, columns=['SOURCE', 'TARGET'])\n",
    "df_datasets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (df_datasets['SOURCE'].str.count(' ') < MAX_LENGTH) & (df_datasets['TARGET'].str.count(' ') < MAX_LENGTH)\n",
    "df_datasets = df_datasets.loc[mask]\n",
    "\n",
    "df_datasets.to_csv('./datasets/translate_transformer_temp.csv', index=False)\n",
    "data_fields = [('SOURCE', SOURCE_FIELD), ('TARGET', TARGET_FIELD)]\n",
    "train = data.TabularDataset('./datasets/translate_transformer_temp.csv', format='csv', fields=data_fields)\n",
    "train_iter = Iterator(train, batch_size=BATCH_SIZE, device=device, repeat=False, \n",
    "                      sort_key=lambda x: (len(x.SOURCE), len(x.TARGET)),\n",
    "                      batch_size_fn=batch_size_fn, train=True, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE_FIELD.build_vocab(train)\n",
    "TARGET_FIELD.build_vocab(train)\n",
    "\n",
    "pickle.dump(SOURCE_FIELD, open('datasets/SOURCE.pkl', 'wb'))\n",
    "pickle.dump(TARGET_FIELD, open('datasets/TARGET.pkl', 'wb'))\n",
    "\n",
    "SOURCE_PAD = SOURCE_FIELD.vocab.stoi['<pad>']\n",
    "TARGET_PAD = TARGET_FIELD.vocab.stoi['<pad>']\n",
    "\n",
    "for i, b in enumerate(train_iter): \n",
    "    TRAIN_LENGTH = i\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_vocab_size = len(SOURCE_FIELD.vocab)\n",
    "target_vocab_size = len(TARGET_FIELD.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build [Transformer](https://arxiv.org/pdf/1706.03762.pdf) Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, d_model, N, heads, dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "            \n",
    "        def get_clones(module, N):\n",
    "            return nn.ModuleList([copy.deepcopy(module) for i in range(N)])\n",
    "\n",
    "        self.N = N\n",
    "        self.embedding_layer = Embedder(vocab_size, d_model)\n",
    "        self.pe = PositionalEncoder(d_model, dropout=dropout)\n",
    "        self.encoder_layer = get_clones(EncoderLayer(d_model, heads, dropout), N)\n",
    "        self.norm = Norm(d_model)\n",
    "    \n",
    "    def forward(self, input_seq, mask):\n",
    "        \n",
    "        x = self.embedding_layer(input_seq)\n",
    "        x = self.pe(x)\n",
    "        for i in range(self.N):\n",
    "            x = self.encoder_layer[i](x, mask)\n",
    "            \n",
    "        x = self.norm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, d_model, N, heads, dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        def get_clones(module, N):\n",
    "            return nn.ModuleList([copy.deepcopy(module) for i in range(N)])\n",
    "        \n",
    "        self.N = N\n",
    "        self.embedding_layer = Embedder(vocab_size, d_model)\n",
    "        self.pe = PositionalEncoder(d_model, dropout=dropout)\n",
    "        self.decoder_layer = get_clones(DecoderLayer(d_model, heads, dropout), N)\n",
    "        self.norm = Norm(d_model)\n",
    "        \n",
    "    def forward(self, target_seq, encoder_outputs, source_mask, target_mask):\n",
    "        \n",
    "        x = self.embedding_layer(target_seq)\n",
    "        x = self.pe(x)\n",
    "        for i in range(self.N):\n",
    "            x = self.decoder_layer[i](x, encoder_outputs, source_mask, target_mask)\n",
    "            \n",
    "        x = self.norm()\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \n",
    "    def __init__(self, source_vocab, target_vocab, d_model, N, heads, dropout):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        self.encoder = Encoder(source_vocab, d_model, N, heads, dropout)\n",
    "        self.decoder = Decoder(target_vocab, d_model, N, heads, dropout)\n",
    "        self.fc_layer = nn.Linear(d_model, target_vocab)\n",
    "        \n",
    "    def forward(self, source_seq, target_seq, source_mask, target_mask):\n",
    "        \n",
    "        encoder_outputs = self.encoder(source_seq, source_mask)\n",
    "        decoder_output = self.decoder(target_seq, encoder_outputs, source_mask, target_mask)\n",
    "        output = self.fc_layer(decoder_output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize Transformer Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(source_vocab_size, target_vocab_size, D_MODEL, N_LAYERS, N_HEADS, DROPOUT)\n",
    "transformer.to(device)\n",
    "for p in transformer.parameters(): \n",
    "    if p.dim() > 1: nn.init.xavier_uniform_(p)      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
