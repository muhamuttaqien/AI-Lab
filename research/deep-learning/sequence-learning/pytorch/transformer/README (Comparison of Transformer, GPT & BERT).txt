Transformer-Based Models: Transformer-XL, GPT, BERT, GPT-2, XLNet

Transformer-XL
- Larger input length to avoid context fragmentation issue on input sequence

GPT 1
- Fine-tuned tasks
- Decoder-only block/ transformer
- Normal self-attention
- Auto-regression capability

BERT
- Very good at fill-in-the-blanks
- Fine-tuned tasks
- Encoder-only block/ transformer
- Normal self-attention + <MASK>
- Bidirectional capability

GPT 2
- GPT 1
- Very good at writing essays
- Masked self-attention + Mask Interfering mechanism
- 1.5 billion parameters Trained by Web Text 40GB + 100 GPU