{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence To Sequence (Seq2seq), Machine Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "MAX_LENGTH = 10\n",
    "\n",
    "LR = 0.01\n",
    "HIDDEN_DIM = 256\n",
    "DROPOUT = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "if is_cuda: device = torch.device('cuda')\n",
    "else: device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Language:\n",
    "    \n",
    "    def __init__(self, name):\n",
    "        super(Language, self).__init__()\n",
    "        \n",
    "        self.name = name\n",
    "        self.word2count = {}\n",
    "        self.word2index = {}\n",
    "        self.index2word = {0: 'SOS', 1: 'EOS'}\n",
    "        self.n_words = 2\n",
    "    \n",
    "    def add_sentence(self, sentence):\n",
    "        for word in sentence.splot(' '):\n",
    "            self.add_word(word)\n",
    "            \n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicode_to_ascii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize_string(s):\n",
    "    s = unicode_to_ascii(s.lower().strip())\n",
    "    s = re.sub(r'([.!?])', r' \\1', s)\n",
    "    s = re.sub(r'[^a-zA-Z.!?]+', r' ', s)\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_languages(lang1, lang2, reverse=False):\n",
    "    print('Reading lines...')\n",
    "    \n",
    "    # read the file and split into lines\n",
    "    lines = open('data/%s-%s.txt' % (lang1, lang2), encoding='utf-8').read().strip().split('\\n')\n",
    "    \n",
    "    # split every line into pairs and normalize\n",
    "    pairs = [[normalize_string(s) for s in l.split('\\t')] for l in lines]\n",
    "    \n",
    "    # reverse pairs, make language instances\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Language(lang2)\n",
    "        output_lang = Language(lang1)\n",
    "    else:\n",
    "        input_lang = Language(lang1)\n",
    "        output_lang = Language(lang2)\n",
    "    \n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eng_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s \",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \"\n",
    ")\n",
    "\n",
    "def filter_pair(p):\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "           len(p[1].split(' ')) < MAX_LENGTH and \\\n",
    "           p[1].startswith(eng_prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filter_pairs(pairs):\n",
    "    return [pair for pair in pairs if filter_pair(pair)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_data(lang1, lang2, reverse=False):\n",
    "    input_lang, output_lang, pairs = read_languages(lang1, lang2, reverse)\n",
    "    \n",
    "    print('Read %s sentence pairs.' % len(pairs))\n",
    "    pairs = filter_pairs(pairs)\n",
    "    \n",
    "    print('Trimmed to %s sentence pairs.' % len(pairs))\n",
    "    print('Counting words...')\n",
    "    for pair in pairs:\n",
    "        input_lang.add_sentence(pair[0])\n",
    "        output_lang.add_sentence(pair[1])\n",
    "        \n",
    "    print('Counted words:')\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    \n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def indexes_from_sentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tensor_from_sentence(lang, sentence):\n",
    "    indexes = indexes_from_sentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    \n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tensor_from_pair(pair):\n",
    "    input_tensor = tensor_from_sentence(input_lang, pair[0])\n",
    "    target_tensor = tensor_from_sentence(output_lang, pair[1])\n",
    "    \n",
    "    return (input_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_lang, output_lang, pairs = prepare_data('eng', 'fra', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build [Seq2seq](https://arxiv.org/pdf/1409.3215.pdf) Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding_layer = nn.Embedding(input_size, hidden_dim)\n",
    "        self.gru_layer = nn.GRU(hidden_dim, hidden_dim)\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        hidden = torch.zeros(1, 1, self.hidden_dim)\n",
    "        return hidden\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        \n",
    "        x = x.long()\n",
    "        embeds = self.embedding_layer(x)\n",
    "        gru_out, hidden = self.gru_layer(embeds, hidden)\n",
    "        \n",
    "        return gru_out, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_dim, output_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding_layer = nn.Embedding(output_size, hidden_dim)\n",
    "        self.gru_layer = nn.GRU(hidden_dim, hidden_dim)\n",
    "        self.fc_layer = nn.Linear(hidden_dim, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        hidden = torch.zeros(1, 1, self.hidden_dim)\n",
    "        return hidden\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        \n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        x = x.long()\n",
    "        embeds = self.embedding_layer(x)\n",
    "        gru_out, hidden = self.gru_layer(F.relu(embeds), hidden)\n",
    "        gru_out = self.softmax(self.fc_layer(gru_out[0]))\n",
    "        \n",
    "        return gru_out, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AttentionDecoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_dim, output_size, dropout=0.1, max_length=MAX_LENGTH):\n",
    "        super(AttentionDecoder, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_size = output_size\n",
    "        self.dropout = dropout\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        self.embedding_layer = nn.Embedding(self.output_size, self.hidden_dim)\n",
    "        self.attention_layer = nn.Linear(self.hidden_dim * 2, self.max_length)\n",
    "        self.attention_combine_layer = nn.Linear(self.hidden_dim * 2, self.hidden_dim)\n",
    "        self.dropout = nn.Dropout(self.dropout)\n",
    "        self.gru_layer = nn.GRU(self.hidden_dim, self.hidden_dim)\n",
    "        self.fc_layer = nn.Linear(self.hidden_dim, self.output_size)\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        hidden = torch.zeros(1, 1, self.hidden_dim)\n",
    "        return hidden\n",
    "        \n",
    "    def forward(self, x, hidden, encoder_outputs):\n",
    "        \n",
    "        x = x.long()\n",
    "        embeds = self.embedding_layer(x)\n",
    "        embeds = self.dropout(embeds)\n",
    "        \n",
    "        attention_weights = F.softmax(self.attention_layer(torch.cat((embeds[0], hidden[0]), 1)), dim=1)\n",
    "        attention_applied = torch.bmm(attention_weights.unsqueeze(0),\n",
    "                                      encoder_outputs.unsqueeze(0))\n",
    "        \n",
    "        attention_out = torch.cat((embeds[0], attention_applied[0]), 1)\n",
    "        attention_out = self.attention_combine_layer(attention_out).unsqueeze(0)\n",
    "        \n",
    "        attention_out = F.relu(attention_out)\n",
    "        gru_out, hidden = self.gru_layer(attention_out, hidden)\n",
    "        \n",
    "        gru_out = F.log_softmax(self.fc_layer(gru_out[0]), dim=1)\n",
    "        \n",
    "        return gru_out, hidden, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize Seq2seq Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder = Encoder(input_lang.n_words, HIDDEN_DIM)\n",
    "encoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decoder = Decoder(HIDDEN_DIM, output_lang.n_words)\n",
    "decoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "attention_decoder = AttentionDecoder(HIDDEN_DIM, output_lang.n_words, dropout=DROPOUT)\n",
    "attention_decoder.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nll_loss = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder_optimizer = torch.optim.SGD(encoder.parameters(), lr=LR)\n",
    "decoder_optimizer = torch.optim.SGD(decoder.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Seq2seq Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
