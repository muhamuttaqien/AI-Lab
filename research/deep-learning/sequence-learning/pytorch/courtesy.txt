# ----------
# BASIC NLP
# ----------
— NLP Pipeline
https://www.analyticsvidhya.com/blog/2017/01/ultimate-guide-to-understand-implement-natural-language-processing-codes-in-python/

— Word Embeddings
https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/
https://machinelearningmastery.com/what-are-word-embeddings/
https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/
https://medium.com/explore-artificial-intelligence/word2vec-a-baby-step-in-deep-learning-but-a-giant-leap-towards-natural-language-processing-40fe4e8602ba

— NLP Implementation
https://medium.com/@ageitgey/machine-learning-is-fun-part-5-language-translation-with-deep-learning-and-the-magic-of-sequences-2ace0acca0aa
https://medium.com/@ageitgey/natural-language-processing-is-fun-9a0bff37854e

# ----
# RNN
# ----
https://medium.com/@ageitgey/machine-learning-is-fun-part-2-a26a10b68df3
https://www.analyticsvidhya.com/blog/2017/12/introduction-to-recurrent-neural-networks/
https://medium.com/explore-artificial-intelligence/an-introduction-to-recurrent-neural-networks-72c97bf0912
https://blog.floydhub.com/a-beginners-guide-on-recurrent-neural-networks-with-pytorch/

# ------
# LSTM
# ------
https://www.analyticsvidhya.com/blog/2017/12/fundamentals-of-deep-learning-introduction-to-lstm/
https://medium.com/explore-artificial-intelligence/lstm-networks-c300d3cb8ac4
https://medium.com/mlreview/understanding-lstm-and-its-diagrams-37e2f46f1714
https://colah.github.io/posts/2015-08-Understanding-LSTMs/
https://medium.com/datathings/the-magic-of-lstm-neural-networks-6775e8b540cd
https://blog.floydhub.com/long-short-term-memory-from-zero-to-hero-with-pytorch/

# -----
# GRU
# -----
https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21
https://towardsdatascience.com/understanding-gru-networks-2ef37df6c9be
https://blog.floydhub.com/gru-with-pytorch/

# -------------
# TRANSFORMERS
# -------------
https://towardsdatascience.com/transformers-141e32e69591
https://medium.com/inside-machine-learning/what-is-a-transformer-d07dd1fbec04
https://towardsdatascience.com/how-to-code-the-transformer-in-pytorch-24db27c8f9ec
https://www.analyticsvidhya.com/blog/2019/07/pytorch-transformers-nlp-python/
https://blog.floydhub.com/the-transformer-in-pytorch/
https://blog.floydhub.com/gpt2/

# ----------
# Attention
# ----------
https://medium.com/@joealato/attention-in-nlp-734c6fa9d983
https://medium.com/@Alibaba_Cloud/self-attention-mechanisms-in-natural-language-processing-9f28315ff905
https://towardsdatascience.com/intuitive-understanding-of-attention-mechanism-in-deep-learning-6c9482aecf4f
https://distill.pub/2016/augmented-rnns/

# -----------------------
# SC: Text Summarization
# -----------------------
https://towardsdatascience.com/a-quick-introduction-to-text-summarization-in-machine-learning-3d27ccf18a9f
https://towardsdatascience.com/text-summarization-using-deep-learning-6e379ed2e89c
https://becominghuman.ai/text-summarization-in-5-steps-using-nltk-65b21e352b65
https://www.analyticsvidhya.com/blog/2019/06/comprehensive-guide-text-summarization-using-deep-learning-python/
https://blog.floydhub.com/gentle-introduction-to-text-summarization-in-machine-learning/

# ---------------------
# SC: Image Captioning
# ---------------------
https://towardsdatascience.com/image-captioning-with-keras-teaching-computers-to-describe-pictures-c88a46a311b8
https://medium.com/analytics-vidhya/introduction-to-image-caption-generation-using-the-avengers-infinity-war-characters-6f14df09dbe5
https://medium.com/mlreview/multi-modal-methods-image-captioning-from-translation-to-attention-895b6444256e
https://www.analyticsvidhya.com/blog/2018/04/solving-an-image-captioning-task-using-deep-learning/
https://machinelearningmastery.com/develop-a-deep-learning-caption-generation-model-in-python/

# ------------------------------
# SC: Visual Question Answering
# ------------------------------
https://tryolabs.com/blog/2018/03/01/introduction-to-visual-question-answering/
https://towardsdatascience.com/deep-learning-and-visual-question-answering-c8c8093941bc
https://blog.floydhub.com/asking-questions-to-images-with-deep-learning/
https://github.com/tbmoon/basic_vqa
https://github.com/Cadene/vqa.pytorch

# -----------------------
# SC: Speech Recognition
# -----------------------
https://medium.com/@ageitgey/machine-learning-is-fun-part-6-how-to-do-speech-recognition-with-deep-learning-28293c162f7a
https://emerj.com/ai-podcast-interviews/spoken-voice-based-nlp-for-business/
http://www.hexahedria.com/2015/08/03/composing-music-with-recurrent-neural-networks/
https://www.analyticsvidhya.com/blog/2019/07/learn-build-first-speech-to-text-model-python/
https://towardsdatascience.com/ok-google-how-to-do-speech-recognition-f77b5d7cbe0b


# ------------------------------------------------------------
# LESSON LEARNED                                             #
# ------------------------------------------------------------

NOTES OF SEQUENCE LEARNING:

# ---------------------------------
# RNNs (Recurrent Neural Networks)
# ---------------------------------
1. In RNNs, The prior ht is initialized by 0 values

2. There is Whh with weights randomly initialized

3. To implement RNNs, the technique can be one-to-one, one-to-many, many-to-one, many-to-many and many-to-many

4. In RNNs, we may or may not have outputs at each time step

5. RNNs output produced can also be fed back into the model at the next time step if necessary called looping mechanisms

6. RNNs, theoretically, hidden state is init for each time step but in practice it can be done for every batch or epoch and it worked

7. RNNs typically treat the full sequence (word) as one training example so the total error is just the sum of the errors at each time step (chars)

8. All of the weights are actually the same as that RNNs cell is essentially being reused throughout the process and only the input data and hidden state carried forward are unique at each time step

9. RNNs gradient is calculated for each time step with respect to the weight parameter and then combine the gradients of the error for all time steps (If there are 100s of time steps, this would basically take really long for the network to converge suffering vanishing gradients and the values are almost zero)

10. RNNs are forgetful so they cannot handle long dependency like ‘The man who ate my pizza has purple hair’ sentence

11. RNNs formula for generating hidden state is Ht = tanH( Whh*Ht-1 + Wxh*X )

12. RNNs hidden state init in batch shape respects to input’s batch shape

13. RNNs input size and output size is as same as dictionary/ vocabulary size of the tasks

14. In practice we can use until 300 memory cells/ units and more than 1 RNNs layer

# -------------------------------
# LSTMs (Long Short-Term Memory)
# -------------------------------
1. For the simple short-term-memory sentence like ‘The color of the sky is ___‘, RNNs turn out to be quite effective

2. The limitation of RNNs is they transforms the existing information completely and there is no consideration for ‘important’ and ‘not so important’ information

3. LSTMs network is comprised of different memory blocks called cells 

4. LSTMs’ cell state contains long-term memory computed by forget & input gates and hidden state contains short-term memory computed by output memory and these gates called 3 major gating mechanisms (forget, input, output)

5. FORGET GATE is responsible for forget long-term memory/not, long-term memory carried by cell state will be considered whether to remember/ keep or forget/ remove the information from the cell sate | Gforget = sigmoid( Wforget . ( ht-1, xt ) + Bforget )

6. In FORGET GATE, sentence example is ‘Bob is a nice person. Dan on the other hand is evil’ while the gate forgets the subject ‘Bob’ and replaces it with the new subject ‘Dan’

7. INPUT GATE is responsible for adjust long-term memory/not long-term memory carried by cell state will be considered whether to adjust or not based on vector created from the tanH | Ginput1 = sigmoid( Winput1 . (ht-1, xt) + Binput1 ) | Ginput2 = tanH( Winput2 . (ht-1, xt) + Binput2 ) | Ginput1 * Ginput2

8. In INPUT GATE, sentence example is ‘Bob knows swimming. He told me over the phone that he had served the navy for 4 long years’ while the gate set ‘he told over the phone’ as less important information or might be redundant and can be ignored

9. OUTPUT GATE is responsible for select useful information from newly-computed long-term memory/not and short-term memory since not all information that runs along the cell state is fit for being output/ prediction and new short-term memory | Goutput1 = sigmoid( Woutput1 . (ht-1, xt) + Boutput1 ) | Goutput2 = tanH( Woutput2 . Cellt + Boutput2 ) | Goutput1 * Goutput2 

10. In OUTPUT GATE, sentence example is ‘Bob fought single handedly with the enemy and died for his country. For his contributions brave’ while the gate has a strong tendency of the answer being a noun and thus ‘Bob’ could be an output

11. The one of disadvantage of LSTMs is the difficulty of training them meaning it takes a lot of time and system resources (hardware constraint)

# ---------------------------
# GRUs (Gater Recurrent Unit)
# ---------------------------
1. GRUs are quite faster and less extensive of computational resources yet still very new then LSTMs

2. GRUs got rid of the cell state and used only the one hidden state to transfer short-term and long-term information

3. GRUs also only have two gates, a reset gate and update gate

4. GRUs are not washing out the new input every single time but keeps the relevant information and passes it down to the next time steps of the network

5. GRUs' update gate acts similar to the forget and input gate of an LSTM while it decides what information to throw away and what new information to add

6. GRUs' reset gate is another gate to decide how much past information to forget

7. RESET GATE is achieved by multiplying the previous hidden state (ht-1) and current input (xt) with their respective weights (Whidden, Winput) and summing them before passing the sum through a sigmoid function | Greset = sigmoid(  Whidden-reset . ht-1 + Winput-reset . xt ) | r = tanH(Greset x ( Whidden . ht-1 ) + ( winput . xt ) ) | x = Hadamard product

8. UPDATE GATE is to help the model determine how much of the past information stored in the previous hidden state needs to be retrained for the future use | Gupdate = sigmoid( Whidden-update . ht-1 + Winput-update . xt ) | u = Gupdate x ht-1 | x = Hadamard product

9. GRUs consits combining process for RESET GATE and UPDATE GATE and this process will produce new hidden state transfering short-term and long-term information | ht = r x (1 - Gupdate) + u | x = Hadamard product

10. To review, RESET GATE is responsible for deciding which portions of the previous hidden state are to be combined with the current input to propose a new hidden state while UPDATE GATE is responsible for determining how much of the previous hidden state is to be retained and what portion of the new proposed hidden state (derived from the RESET GATE) is to be added to the final hidden state

11. GRUs, similar with its older sibling LSTMs, keep most of the existing hidden state while adding new content on top of it (not replacing the entire content of the hidden state at each time step like RNNs do)

12. The role of the UPDATE GATE in GRUs is very similar to INPUT and FORGET GATES in LSTMs however, the control of new memory content added to the network differs between these two

13. In LSTMs, while FORGET GATE determines which part of the previous cell state to retain, INPUT GATE determines the amount of the new memory to be added and these two gates are independent of each other meaning that the amount of new information added through INPUT GATE is completely independent of the information retained through FORGET GATE

14. As for GRUs, UPDATE GATE is responsible for determining which information from the previous memory to retain and is also responsible for controlling the new memory to be added meaning that the retention of previous memory and addition of new information to the memory in GRUs is NOT independent

### END ###