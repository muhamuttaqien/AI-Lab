# ----------
# BASIC NLP
# ----------
— NLP Pipeline
https://www.analyticsvidhya.com/blog/2017/01/ultimate-guide-to-understand-implement-natural-language-processing-codes-in-python/

— Word Embeddings
https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/
https://medium.com/explore-artificial-intelligence/word2vec-a-baby-step-in-deep-learning-but-a-giant-leap-towards-natural-language-processing-40fe4e8602ba
https://machinelearningmastery.com/what-are-word-embeddings/
https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/

— NLP Implementation
https://medium.com/@ageitgey/machine-learning-is-fun-part-5-language-translation-with-deep-learning-and-the-magic-of-sequences-2ace0acca0aa
https://medium.com/@ageitgey/natural-language-processing-is-fun-9a0bff37854e

# ----
# RNN
# ----
https://medium.com/@ageitgey/machine-learning-is-fun-part-2-a26a10b68df3
https://www.analyticsvidhya.com/blog/2017/12/introduction-to-recurrent-neural-networks/
https://medium.com/explore-artificial-intelligence/an-introduction-to-recurrent-neural-networks-72c97bf0912
https://blog.floydhub.com/a-beginners-guide-on-recurrent-neural-networks-with-pytorch/

# ------
# LSTM
# ------
https://www.analyticsvidhya.com/blog/2017/12/fundamentals-of-deep-learning-introduction-to-lstm/
https://medium.com/explore-artificial-intelligence/lstm-networks-c300d3cb8ac4
https://medium.com/mlreview/understanding-lstm-and-its-diagrams-37e2f46f1714
https://colah.github.io/posts/2015-08-Understanding-LSTMs/
https://medium.com/datathings/the-magic-of-lstm-neural-networks-6775e8b540cd
https://blog.floydhub.com/long-short-term-memory-from-zero-to-hero-with-pytorch/

# -----
# GRU
# -----
https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21
https://towardsdatascience.com/understanding-gru-networks-2ef37df6c9be
https://blog.floydhub.com/gru-with-pytorch/

# -------------
# TRANSFORMERS
# -------------
https://towardsdatascience.com/transformers-141e32e69591
https://medium.com/inside-machine-learning/what-is-a-transformer-d07dd1fbec04
https://towardsdatascience.com/how-to-code-the-transformer-in-pytorch-24db27c8f9ec
https://www.analyticsvidhya.com/blog/2019/07/pytorch-transformers-nlp-python/

# -----------------------
# SC: Text Summarization
# -----------------------
https://towardsdatascience.com/a-quick-introduction-to-text-summarization-in-machine-learning-3d27ccf18a9f
https://towardsdatascience.com/text-summarization-using-deep-learning-6e379ed2e89c
https://becominghuman.ai/text-summarization-in-5-steps-using-nltk-65b21e352b65
https://www.analyticsvidhya.com/blog/2019/06/comprehensive-guide-text-summarization-using-deep-learning-python/
https://blog.floydhub.com/gentle-introduction-to-text-summarization-in-machine-learning/

# ------------------------------
# SC: Visual Question Answering
# ------------------------------
https://tryolabs.com/blog/2018/03/01/introduction-to-visual-question-answering/
https://towardsdatascience.com/deep-learning-and-visual-question-answering-c8c8093941bc
https://blog.floydhub.com/asking-questions-to-images-with-deep-learning/
https://github.com/tbmoon/basic_vqa
https://github.com/Cadene/vqa.pytorch

# ---------------------
# SC: Image Captioning
# ---------------------
https://towardsdatascience.com/image-captioning-with-keras-teaching-computers-to-describe-pictures-c88a46a311b8
https://medium.com/analytics-vidhya/introduction-to-image-caption-generation-using-the-avengers-infinity-war-characters-6f14df09dbe5
https://medium.com/mlreview/multi-modal-methods-image-captioning-from-translation-to-attention-895b6444256e
https://www.analyticsvidhya.com/blog/2018/04/solving-an-image-captioning-task-using-deep-learning/
https://machinelearningmastery.com/develop-a-deep-learning-caption-generation-model-in-python/



# ------------------------------------------------------------
# LESSON LEARNED                                             #
# ------------------------------------------------------------

NOTES OF SEQUENCE LEARNING:

# ---------------------------------
# RNNs (Recurrent Neural Networks)
# ---------------------------------
1. In RNNs, The prior ht is initialized by 0 values
2. There is Whh with weights randomly initialized
3. To implement RNNs, the technique can be one-to-one, one-to-many, many-to-one, many-to-many and many-to-many
4. In RNNs, we may or may not have outputs at each time step
5. RNNs output produced can also be fed back into the model at the next time step if necessary called looping mechanisms
6. RNNs, theoretically, hidden state is init for each time step but in practice it can be done for every batch or epoch and it worked
7. RNNs typically treat the full sequence (word) as one training example so the total error is just the sum of the errors at each time step (chars)
8. All of the weights are actually the same as that RNNs cell is essentially being reused throughout the process and only the input data and hidden state carried forward are unique at each time step
9. RNNs gradient is calculated for each time step with respect to the weight parameter and then combine the gradients of the error for all time steps (If there are 100s of time steps, this would basically take really long for the network to converge suffering vanishing gradients and the values are almost zero)
10. RNNs are forgetful so they cannot handle long dependency like ‘The man who ate my pizza has purple hair’ sentence
11. RNNs formula for generating hidden state is Ht = tanH(Whh*Ht-1 + Wxh*X)
12. RNNs hidden state init in batch shape respects to input’s batch shape
13. RNNs input size and output size is as same as dictionary/ vocabulary size of the tasks
14. In practice we can use until 300 memory cells/ units and more than 1 RNNs layer

# -------------------------------
# LSTMs (Long Short-Term Memory)
# -------------------------------
1. For the simple short-term-memory sentence like ‘The color of the sky is ___‘, RNNs turn out to be quite effective
2. The limitation of RNNs is they transforms the existing information completely and there is no consideration for ‘important’ and ‘not so important’ information
3. LSTMs network is comprised of different memory blocks called cells 
4. LSTMs’ cell state contains long-term memory computed by forget & input gates and hidden state contains short-term memory computed by output memory and these gates called 3 major gating mechanisms (forget, input, output)
5. Forget gate is responsible for forget long-term memory/not, long-term memory carried by cell state will be considered whether to remember/ keep or forget/ remove the information from the cell sate | sigmoid( Wforget . ( ht-1, xt ) + Bforget)
6. In forget gate, sentence example is ‘Bob is a nice person. Dan on the other hand is evil’ while the gate forgets the subject ‘Bob’ and replaces it with the new subject ‘Dan’
7. Input gate is responsible for adjust long-term memory/not | long-term memory carried by cell state will be considered whether to adjust or not based on vector created from the tanH | sigmoid( Winput1 . (ht-1, xt) + Binput1 ) | tanH( Winput2 . (ht-1, xt) + Binput2)
8. In input gate, sentence example is ‘Bob knows swimming. He told me over the phone that he had served the navy for 4 long years’ while the gate set ‘he told over the phone’ as less important information or might be redundant and can be ignored
9. Output gate is responsible for select useful information from newly-computed long-term memory/not and short-term memory since not all information that runs along the cell state is fit for being output/ prediction and new short-term memory | sigmoid( Woutput1 . (ht-1, xt) + Boutput1 ) | tanH( Woutput2 . Cellt + Boutput2 ) | sigmoid_vector * tanH_vector 
10. In output gate, sentence example is ‘Bob fought single handedly with the enemy and died for his country. For his contributions brave’ while the gate has a strong tendency of the answer being a noun and thus ‘Bob’ could be an output
11. The one of disadvantage of LSTMs is the difficulty of training them meaning it takes a lot of time and system resources (hardware constraint)

# ---------------------------
# GRUs (Gater Recurrent Unit)
# ---------------------------