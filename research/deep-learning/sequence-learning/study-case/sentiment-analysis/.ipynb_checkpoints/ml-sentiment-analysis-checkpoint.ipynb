{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning for Sentiment Analysis, Tweets Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import string\n",
    "import nltk\n",
    "import warnings \n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('./datasets/train_tweets.csv')\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x=df_train['label'])\n",
    "plt.title('Racist vs Not-racist Tweets')\n",
    "plt.savefig('./images/plot-racist-vs-not-racist-tweets')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_pattern(text, pattern):\n",
    "    \n",
    "    r = re.findall(pattern, text)\n",
    "    for i in r:\n",
    "        text = re.sub(i, '', text)\n",
    "        \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Removing twitter handles (@user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = '@[\\w]*'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['cleaned_tweet'] = np.vectorize(remove_pattern)(df_train['tweet'], pattern)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Removing punctuations, numbers and special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['cleaned_tweet'] = df_train['cleaned_tweet'].str.replace('[^a-zA-Z#]', ' ')\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Removing very short words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_length = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['cleaned_tweet'] = df_train['cleaned_tweet'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>min_length]))\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) Tokenizing cleaned tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['tokenized_tweet'] = df_train['cleaned_tweet'].apply(lambda x: x.split())\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) Stemming tokenized tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['tokenized_stemmed_tweet'] = df_train['tokenized_tweet'].apply(lambda x: [stemmer.stem(i) for i in x])\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5) Putting the preprocessed result into cleaned tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm_notebook(range(len(df_train['tokenized_stemmed_tweet']))):\n",
    "    df_train['cleaned_tweet'][i] = ' '.join(df_train['tokenized_stemmed_tweet'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv('./datasets/train_tweets_preprocessed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Cleaned Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A) Understanding the common words used in the tweets: WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = ' '.join([text for text in df_train['cleaned_tweet']])\n",
    "word_cloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 7))\n",
    "plt.imshow(word_cloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('The Most Common Words of Tweets')\n",
    "plt.savefig('./images/word-cloud-the-most-common-words')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B) Understanding the common words in non racist/sexist tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_words = ' '.join([text for text in df_train['cleaned_tweet'][df_train['label'] == 0]])\n",
    "word_cloud = WordCloud(width=800, height=500, max_font_size=110, random_state=21).generate(normal_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 7))\n",
    "plt.imshow(word_cloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('The Most Common Non-Racist Words of Tweets')\n",
    "plt.savefig('./images/word-cloud-the-most-common-non-racist-words')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C) Understanding the common words in racist/sexist tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "racist_words = ' '.join([text for text in df_train['cleaned_tweet'][df_train['label'] == 1]])\n",
    "word_cloud = WordCloud(width=800, height=500, max_font_size=110, random_state=21).generate(racist_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 7))\n",
    "plt.imshow(word_cloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('The Most Common Racist Words of Tweets')\n",
    "plt.savefig('./images/word-cloud-the-most-common-racist-words')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### D) Understanding the impact of hashtags on tweets sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hashtags(x):\n",
    "    \n",
    "    hashtags = []\n",
    "    for i in x:\n",
    "        ht = re.findall(r'#(\\w+)', i)\n",
    "        hashtags.append(ht)\n",
    "        \n",
    "    return hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting hashtags from non-racist tweets\n",
    "HT_positive = extract_hashtags(df_train['cleaned_tweet'][df_train['label'] == 0])\n",
    "HT_positive = sum(HT_positive,[])\n",
    "HT_freqdist = nltk.FreqDist(HT_positive)\n",
    "\n",
    "df_HT = pd.DataFrame({'Hashtags': list(HT_freqdist.keys()),\n",
    "                      'Count': list(HT_freqdist.values())})\n",
    "\n",
    "df_HT = df_HT.nlargest(columns='Count', n=10) # select the top 10 most frequent hashtags\n",
    "plt.figure(figsize=(16,5))\n",
    "ax = sns.barplot(data=df_HT, x='Hashtags', y='Count')\n",
    "ax.set(ylabel='Count')\n",
    "plt.savefig('./images/plot-hastags-of-non-racist-words')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting hashtags from non-racist tweets\n",
    "HT_negative = extract_hashtags(df_train['cleaned_tweet'][df_train['label'] == 1])\n",
    "HT_negative = sum(HT_negative,[])\n",
    "HT_freqdist = nltk.FreqDist(HT_negative)\n",
    "\n",
    "df_HT = pd.DataFrame({'Hashtags': list(HT_freqdist.keys()),\n",
    "                      'Count': list(HT_freqdist.values())})\n",
    "\n",
    "df_HT = df_HT.nlargest(columns='Count', n=10) # select the top 10 most frequent hashtags\n",
    "plt.figure(figsize=(16,5))\n",
    "ax = sns.barplot(data=df_HT, x='Hashtags', y='Count')\n",
    "ax.set(ylabel='Count')\n",
    "plt.savefig('./images/plot-hastags-of-racist-words')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Features of Cleaned Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A. Bag-of-Words features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_vectorizer = CountVectorizer(min_df=2, max_df=0.9, max_features=1000, stop_words='english')\n",
    "bow_features = bow_vectorizer.fit_transform(df_train['cleaned_tweet'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B. TF-IDF features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(min_df=2, max_df=0.9, max_features=1000, stop_words='english')\n",
    "tfidf_features = tfidf_vectorizer.fit_transform(df_train['cleaned_tweet'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A) Building model using Bag-of-Words features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing data from extracted features\n",
    "x_train, x_test, y_train, y_test = train_test_split(bow_features, df_train['label'], test_size=0.3, random_state=10)\n",
    "\n",
    "# training the model\n",
    "lreg_model = LogisticRegression()\n",
    "lreg_model.fit(x_train, y_train)\n",
    "\n",
    "# testing the model \n",
    "prediction = lreg_model.predict_proba(x_test)\n",
    "prediction_int = prediction[:,1] >= 0.5 # if prediction is greater than or equal to 0.5 than 1 else 0\n",
    "prediction_int = prediction_int.astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating f1 score metrics\n",
    "model_score = f1_score(yvalid, prediction_int)\n",
    "print(f'Model F1 score: {model_score:.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B) Building model using TF-IDF features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing data from extracted features\n",
    "x_train, x_test, y_train, y_test = train_test_split(tfidf_features, df_train['label'], test_size=0.3, random_state=10)\n",
    "\n",
    "# training the model\n",
    "lreg_model = LogisticRegression()\n",
    "lreg_model.fit(x_train, y_train)\n",
    "\n",
    "# testing the model \n",
    "prediction = lreg_model.predict_proba(x_test)\n",
    "prediction_int = prediction[:,1] >= 0.5 # if prediction is greater than or equal to 0.5 than 1 else 0\n",
    "prediction_int = prediction_int.astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating f1 score metrics\n",
    "model_score = f1_score(yvalid, prediction_int)\n",
    "print(f'Model F1 score: {model_score:.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('./datasets/test_tweets.csv')\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df_data, pattern, min_length, save_as):\n",
    "\n",
    "    df_data['cleaned_tweet'] = np.vectorize(remove_pattern)(df_data['tweet'], pattern)\n",
    "    df_data['cleaned_tweet'] = df_data['cleaned_tweet'].str.replace('[^a-zA-Z#]', ' ')\n",
    "    df_data['cleaned_tweet'] = df_data['cleaned_tweet'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>min_length]))\n",
    "    df_data['tokenized_tweet'] = df_data['cleaned_tweet'].apply(lambda x: x.split())\n",
    "    df_data['tokenized_stemmed_tweet'] = df_data['tokenized_tweet'].apply(lambda x: [stemmer.stem(i) for i in x])\n",
    "    for i in tqdm_notebook(range(len(df_data['tokenized_stemmed_tweet']))): df_data['cleaned_tweet'][i] = ' '.join(df_data['tokenized_stemmed_tweet'][i])\n",
    "    \n",
    "    df_data.to_csv(f'./datasets/{save_as}_tweets_preprocessed.csv')\n",
    "    return df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = preprocess_data(df_test, pattern, min_length, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(df_data, ftype='tfidf'):\n",
    "    \n",
    "    if ftype == 'bow':\n",
    "        bow_vectorizer = CountVectorizer(min_df=2, max_df=0.9, max_features=1000, stop_words='english')\n",
    "        bow_features = bow_vectorizer.fit_transform(df_data['cleaned_tweet'])\n",
    "        \n",
    "        return bow_features\n",
    "    \n",
    "    elif ftype == 'tfidf':\n",
    "        tfidf_vectorizer = TfidfVectorizer(min_df=2, max_df=0.9, max_features=1000, stop_words='english')\n",
    "        tfidf_features = tfidf_vectorizer.fit_transform(df_data['cleaned_tweet'])\n",
    "    \n",
    "        return tfidf_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = extract_data(df_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = lreg_model.predict_proba(test_features)\n",
    "prediction_int = prediction[:,1] >= 0.5 # if prediction is greater than or equal to 0.5 than 1 else 0\n",
    "prediction_int = prediction_int.astype(np.int)\n",
    "df_test['label'] = prediction_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = df_test[['id', 'label']]\n",
    "report.to_csv('./results/report_lreg_model_tfidf.csv', index=False) # writing data to a CSV file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's Next? Try Deep Learning!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
