{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstraction-Based Text Summarization Using Sequence to Sequence Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "      <td>Not as Advertised</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "      <td>Cough Medicine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "      <td>Great taffy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text                Summary\n",
       "1  I have bought several of the Vitality canned d...  Good Quality Dog Food\n",
       "2  Product arrived labeled as Jumbo Salted Peanut...      Not as Advertised\n",
       "3  This is a confection that has been around a fe...  \"Delight\" says it all\n",
       "4  If you are looking for the secret ingredient i...         Cough Medicine\n",
       "5  Great taffy at a great price.  There was a wid...            Great taffy"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# datasets grasp from here www.kaggle.com/snap/amazon-fine-food-reviews\n",
    "df_reviews = pd.read_csv(\"./datasets/amazon-fine-food-reviews.csv\", nrows=100000)\n",
    "df_reviews.index += 1\n",
    "df_reviews[['Text', 'Summary']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Dropping duplicates and NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_reviews = df_reviews.drop_duplicates(subset=['Summary'])\n",
    "df_reviews = df_reviews.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_reviews = df_reviews.drop_duplicates(subset=['Text'])\n",
    "df_reviews = df_reviews.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Cleaning up stop words, contractions, non-alphanumeric and others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle_file = open('../../others/contraction_dictionary.pickle','rb')\n",
    "contraction_dict = pickle.load(pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_up_text(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    sentence = BeautifulSoup(sentence, 'lxml').text\n",
    "    sentence = re.sub(r'\\([^)]*\\)', '', sentence)\n",
    "    sentence = re.sub('\"','', sentence)\n",
    "    sentence = ' '.join([contraction_dict[word] if word in contraction_dict else word for word in sentence.split(' ')])\n",
    "    sentence = re.sub(r\"'s\\b\",'', sentence)\n",
    "    sentence = re.sub('[^a-zA-Z]', ' ', sentence) \n",
    "    \n",
    "    tokens = [word for word in sentence.split() if not word in stop_words]\n",
    "    words = []\n",
    "    for word in tokens:\n",
    "        if len(word) >= 3:\n",
    "            words.append(word)\n",
    "    \n",
    "    return (' '.join(words)).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_text = []\n",
    "for sentence in tqdm_notebook(df_reviews['Text']): cleaned_text.append(clean_up_text(sentence))\n",
    "df_reviews['Cleaned Text'] = cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up_summary(sentence):\n",
    "    sentence = re.sub('\"', '', sentence)\n",
    "    sentence = ' '.join([contraction_dict[word] if word in contraction_dict else word for word in sentence.split(' ')])\n",
    "    sentence = re.sub(r\"'s\\b\",'', sentence)\n",
    "    sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n",
    "    sentence = sentence.lower()\n",
    "    \n",
    "    tokens = sentence.split()\n",
    "    sentence = ''\n",
    "    for word in tokens:\n",
    "        if len(word) > 1:\n",
    "            sentence += word + ' '\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_summary = []\n",
    "for sentence in tqdm_notebook(df_reviews['Summary']): cleaned_summary.append(clean_up_summary(sentence))\n",
    "df_reviews['Cleaned Summary'] = cleaned_summary\n",
    "df_reviews['Cleaned Summary'] = df_reviews['Cleaned Summary'].apply(lambda x : '_START_ ' + x + ' _END_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews[['Text', 'Cleaned Text', 'Summary', 'Cleaned Summary']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print(\"Text:\", df_reviews['Cleaned Text'][i+1])\n",
    "    print(\"Summary:\", df_reviews['Cleaned Summary'][i+1])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Setting maximum length of the reviews and the summary based on the distribution of sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_word_count = []\n",
    "summary_word_count = []\n",
    "\n",
    "for sentence in df_reviews['Cleaned Text']: text_word_count.append(len(sentence.split()))\n",
    "for sentence in df_reviews['Cleaned Summary']: summary_word_count.append(len(sentence.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dist_sequences = pd.DataFrame({ 'Text': text_word_count, 'Summary': summary_word_count })\n",
    "\n",
    "fig = plt.figure(figsize=(15,5))\n",
    "ax1 = fig.add_subplot(1,2,2)\n",
    "plt.title('Length Distribution of Text Sequences')\n",
    "ax1.hist(x= df_dist_sequences['Text'], bins=30, color='Orange')\n",
    "ax2 = fig.add_subplot(1,2,1)\n",
    "plt.title('Length Distribution of Summary Sequences')\n",
    "ax2.hist(x= df_dist_sequences['Summary'], bins = 30, color='Blue')\n",
    "\n",
    "plt.savefig('./images/histogram-length-distribution-of-sequences')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_LEN_TEXT = 80\n",
    "MAX_LEN_SUMMARY = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) Splitting datasets into training and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(df_reviews['Cleaned Text'], df_reviews['Cleaned Summary'], test_size=0.1, shuffle=True, random_state=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5) Preparing The Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing a tokenizer for text on training data\n",
    "X_tokenizer = Tokenizer()\n",
    "X_tokenizer.fit_on_texts(list(X_train))\n",
    "\n",
    "# converting text sequences into integer sequences\n",
    "X_train = X_tokenizer.texts_to_sequences(X_train)\n",
    "X_valid = X_tokenizer.texts_to_sequences(X_valid)\n",
    "\n",
    "# padding zero up to maximum length\n",
    "X_train = pad_sequences(X_train, maxlen=MAX_LEN_TEXT, padding='post')\n",
    "X_valid = pad_sequences(X_valid, maxlen=MAX_LEN_TEXT, padding='post')\n",
    "\n",
    "# calculating vocabulary size\n",
    "X_vocab_size = len(X_tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# preparing a tokenizer for summary on training data\n",
    "y_tokenizer = Tokenizer()\n",
    "y_tokenizer.fit_on_texts(list(y_train))\n",
    "\n",
    "# converting summary sequences into integer sequences\n",
    "y_train = y_tokenizer.texts_to_sequences(y_train)\n",
    "y_valid = y_tokenizer.texts_to_sequences(y_valid)\n",
    "\n",
    "# padding zero up to maximum length\n",
    "y_train = pad_sequences(y_train, maxlen=MAX_LEN_SUMMARY, padding='post')\n",
    "y_valid = pad_sequences(y_valid, maxlen=MAX_LEN_SUMMARY, padding='post')\n",
    "\n",
    "# calculating vocabulary size\n",
    "y_vocab_size = len(y_tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Seq2seq Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
