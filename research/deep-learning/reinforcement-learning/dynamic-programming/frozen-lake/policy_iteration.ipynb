{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from frozenlake import FrozenLakeEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init the environment\n",
    "env = FrozenLakeEnv(map_name=\"8x8\",is_slippery=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check number of states and actions\n",
    "nb_states = env.observation_space.n\n",
    "nb_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_value(env, policy, discount_factor=1.0, threshold=1e-20):\n",
    "    \n",
    "    # initialize value table randomly\n",
    "    value_table = np.zeros((nb_states, 1))\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        new_value_table = np.copy(value_table)\n",
    "        for state in range(nb_states):\n",
    "            \n",
    "            action = int(policy[state])\n",
    "            for next_state_parameters in env.P[state][action]:\n",
    "                \n",
    "                transition_prob, next_state, reward_prob, _ = next_state_parameters\n",
    "                # apply Bellman equation\n",
    "                value_table[state] = transition_prob * (reward_prob + discount_factor * new_value_table[next_state])\n",
    "                \n",
    "        if (np.sum(np.fabs(new_value_table - value_table)) <= threshold): break\n",
    "    \n",
    "    return value_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_policy(value_table, discount_factor=1.0):\n",
    "    \n",
    "    # initialize policy randomly\n",
    "    policy = np.zeros(env.observation_space.n)\n",
    "    for state in range(env.observation_space.n):\n",
    "        \n",
    "        Q_table = np.zeros(env.action_space.n)\n",
    "        for action in range(env.action_space.n):\n",
    "            \n",
    "            for next_state_parameters in env.P[state][action]:\n",
    "                transition_prob, next_state, reward_prob, _ = next_state_parameters\n",
    "                # apply Bellman equation\n",
    "                Q_table[action] += (transition_prob * (reward_prob + discount_factor * value_table[next_state]))\n",
    "        \n",
    "        policy[state] = np.argmax(Q_table)\n",
    "        \n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_ITERATIONS =  200000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "174fd324c7074bf3840595ac6bb3f5cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=200000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "random_policy = np.zeros((nb_states, 1))\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "for i in tqdm_notebook(range(NB_ITERATIONS)):\n",
    "    \n",
    "    new_value_table = compute_value(env, random_policy)\n",
    "    new_policy = extract_policy(new_value_table)\n",
    "    random_policy = new_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped Grid Policy (0=up, 1=right, 2=down, 3=left):\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print('Reshaped Grid Policy (0=up, 1=right, 2=down, 3=left):')\n",
    "print(np.reshape(new_policy, (env.action_space.n, -1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
