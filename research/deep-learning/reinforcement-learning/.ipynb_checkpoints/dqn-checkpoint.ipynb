{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN, Deep Q Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore', UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython: from IPython import display\n",
    "    \n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "if is_cuda: device = torch.device('cuda')\n",
    "else: device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0').unwrapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', \n",
    "                         ('state', 'action', 'next_state', 'reward'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a method for selecting a random batch of transitions for training\n",
    "class ReplayMemory(object):\n",
    "    \n",
    "    def __init__(self, capacity):\n",
    "        \n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "        \n",
    "    def push(self, *args):\n",
    "        \n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "            \n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    \n",
    "    def __init__(self, h, w, outputs):\n",
    "        \n",
    "        super(DQN, self).__init__()\n",
    "        \n",
    "        # CNN will take in the difference between the current and previous screen patches\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=5, stride=2)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=5, stride=2)\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        # number of linear input connections depends on output of conv2d layers\n",
    "        # and therefore the input image size, so compute it\n",
    "        def conv2d_size_outputs(size, kernel_size=5, stride=2):\n",
    "            return (size - (kernel_size - 1) - 1) // stride + 1\n",
    "        \n",
    "        convw = conv2d_size_outputs(conv2d_size_outputs(conv2d_size_outputs(w)))\n",
    "        convh = conv2d_size_outputs(conv2d_size_outputs(conv2d_size_outputs(h)))\n",
    "        linear_input_size = convw * convh * 32\n",
    "        self.head = nn.Linear(linear_input_size, outputs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        \n",
    "        # this will output Q(s,left) and Q(s,right) (where s is the input to the network)\n",
    "        return self.head(x.view(x.size(0), -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resize = T.Compose([T.ToPILImage(),\n",
    "                    T.Resize(40, interpolation=Image.CUBIC),\n",
    "                    T.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cart_location(screen_width):\n",
    "    \n",
    "    world_width = env.x_threshold * 2\n",
    "    scale = screen_width / world_width\n",
    "    \n",
    "    return int(env.state[0] * scale + screen_width / 2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_screen():\n",
    "    \n",
    "    # returned screen requested by gym is 400x600x3, but sometimes larger\n",
    "    # this will transpose it into torch order (C, H, W)\n",
    "    screen = env.render(mode='rgb_array').transpose((2, 0, 1))\n",
    "    \n",
    "    # cart is in the lower half, so strip off the top and bottom of the screen\n",
    "    _, screen_height, screen_width = screen.shape\n",
    "    screen = screen[:, int(screen_height*0.4): int(screen_height*0.8)]\n",
    "    view_width = int(screen_width * 0.6)\n",
    "    cart_location = get_cart_location(screen_width)\n",
    "    \n",
    "    if cart_location < view_width // 2:\n",
    "        slice_range = slice(view_width)\n",
    "    elif cart_location > (screen_width - view_width // 2):\n",
    "        slice_range = slice(-view_width, None)\n",
    "    else:\n",
    "        slice_range = slice(cart_location - view_width // 2,\n",
    "                            cart_location + view_width // 2)\n",
    "        \n",
    "    # strip off the edges, so that we have a square image centered on a cart\n",
    "    screen = screen[:, :, slice_range]\n",
    "    \n",
    "    # convert to float, rescale, convert to torch tensor\n",
    "    screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "    screen = torch.from_numpy(screen)\n",
    "    \n",
    "    # resize and add a batch dimension (B, C, H, W)\n",
    "    return resize(screen).unsqueeze(0).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env.reset()\n",
    "plt.figure(figsize=(5,10))\n",
    "plt.imshow(get_screen().cpu().squeeze(0).permute(1, 2, 0).numpy(),\n",
    "           interpolation='none')\n",
    "plt.title('Example of Extracted Screen')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.999\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 200\n",
    "TARGET_UPDATE = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DQN(\n",
       "  (conv1): Conv2d(3, 16, kernel_size=(5, 5), stride=(2, 2))\n",
       "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv2): Conv2d(16, 32, kernel_size=(5, 5), stride=(2, 2))\n",
       "  (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv3): Conv2d(32, 32, kernel_size=(5, 5), stride=(2, 2))\n",
       "  (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (head): Linear(in_features=512, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get screen size so that we can initialize layers correctly based on shape\n",
    "init_screen = get_screen()\n",
    "_, _, screen_height, screen_width = init_screen.shape\n",
    "\n",
    "# get number of actions from gym action space\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "policy_net = DQN(screen_height, screen_width, n_actions).to(device)\n",
    "target_net = DQN(screen_height, screen_width, n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.RMSprop(policy_net.parameters())\n",
    "memory = ReplayMemory(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_done = 0\n",
    "\n",
    "# this will select an action accordingly to an epsilon greedy policy\n",
    "def select_action(state):\n",
    "    \n",
    "    global steps_done\n",
    "    \n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    \n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            # here the network will pick action with the larger expected reward\n",
    "            return policy_net(state).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[random.randrange(n_actions)]], dtype=torch.long, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_durations = []\n",
    "\n",
    "# a helper for plotting the durations of episodes\n",
    "def plot_durations():\n",
    "    \n",
    "    plt.figure(2)\n",
    "    plt.clf()\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    \n",
    "    plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    \n",
    "    # take 100 episode averages and plot them\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "        \n",
    "    # pause a bit so that plots are updated\n",
    "    plt.pause(0.001)\n",
    "    if is_ipython:\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function first samples a batch, concatenates all the tensors into a single one\n",
    "# then computes Q(st, at) and V(st+1) = maxaQ(st+1, a), and combines them into our loss\n",
    "def train_network():\n",
    "    if len(memory) < BATCH_SIZE: return\n",
    "    \n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    \n",
    "    # this converts batch-array of transitions to transition of batch-arrays\n",
    "    batch = Transition(*zip(*transitions))\n",
    "    \n",
    "    # compute a mask of non-final states and concatenate the batch elements\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                            batch.next_state)), dtype=torch.uint8, device=device)\n",
    "    non_final_next_states = torch.cat([state for state in batch.next_state if state is not None])\n",
    "    \n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "    \n",
    "    # compute the function Q(s_t, a)\n",
    "    # the network computes Q(s_t) then will select the columns of actions (a) taken\n",
    "    # The (a) is the actions which would've been taken for each batch state according to policy_net\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "    \n",
    "    # compute V(s_{t+1}) for all next states\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "    \n",
    "    # compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "    \n",
    "    # compute huber loss\n",
    "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "    \n",
    "    # this will perform optimization for the network\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    \n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train the network...\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 50\n",
    "\n",
    "print('Train the network...')\n",
    "for i_episode in range(num_episodes):\n",
    "    \n",
    "    # initialize the environment and state\n",
    "    env.reset()\n",
    "    last_screen = get_screen()\n",
    "    current_screen = get_screen()\n",
    "    state = current_screen - last_screen\n",
    "    \n",
    "    for t in count():\n",
    "        \n",
    "        # select and perform an action\n",
    "        action = select_action(state)\n",
    "        _, reward, done, _ = env.step(action.item())\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        \n",
    "        # observe new state\n",
    "        last_screen = current_screen\n",
    "        current_screen = get_screen()\n",
    "        if not done:\n",
    "            next_state = current_screen - last_screen\n",
    "        else:\n",
    "            next_state = None\n",
    "            \n",
    "        # store the transition in memory\n",
    "        memory.push(state, action, next_state, reward)\n",
    "        \n",
    "        # move to the next state\n",
    "        state = next_state\n",
    "        \n",
    "        # perform one step of the optimization on the target network\n",
    "        train_network()\n",
    "        \n",
    "        if done:\n",
    "            episode_durations.append(t + 1)\n",
    "            plot_durations()\n",
    "            break\n",
    "            \n",
    "        # update the target network, copying all weghts and biases in DQN\n",
    "        if i_episode % TARGET_UPDATE == 0:\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "            \n",
    "print('Completed!')\n",
    "env.render()\n",
    "env.close()\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
