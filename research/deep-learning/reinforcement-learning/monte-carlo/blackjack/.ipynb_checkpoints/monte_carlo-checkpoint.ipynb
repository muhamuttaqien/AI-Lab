{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "from utils import plot_blackjack_values, plot_policy\n",
    "from blackjack import BlackjackEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# init the environment\n",
    "env = BlackjackEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space: Tuple(Discrete(32), Discrete(11), Discrete(2))\n",
      "Action space: Discrete(2)\n"
     ]
    }
   ],
   "source": [
    "print('Observation space:', env.observation_space)\n",
    "print('Action space:', env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play with a Random Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------\n",
      "PLAY EPISODE 1\n",
      "-------------------\n",
      "Player's card: 11\n",
      "Dealer's card: 6\n",
      "Any ace: False\n",
      "Random action: 0\n",
      "-------------------\n",
      "End game's reward: 1.0\n",
      "*You won :)\n",
      "\n",
      "-------------------\n",
      "PLAY EPISODE 2\n",
      "-------------------\n",
      "Player's card: 14\n",
      "Dealer's card: 10\n",
      "Any ace: False\n",
      "Random action: 1 -> Next card: 24 (Bust!)\n",
      "-------------------\n",
      "End game's reward: -1\n",
      "*You lost :(\n",
      "\n",
      "-------------------\n",
      "PLAY EPISODE 3\n",
      "-------------------\n",
      "Player's card: 12\n",
      "Dealer's card: 2\n",
      "Any ace: False\n",
      "Random action: 1 -> Next card: 21\n",
      "-------------------\n",
      "Player's card: 21\n",
      "Dealer's card: 2\n",
      "Any ace: False\n",
      "Random action: 0\n",
      "-------------------\n",
      "End game's reward: 1.0\n",
      "*You won :)\n",
      "\n",
      "*The reason of lost could be bust or dealer's sum card is more than player's own.\n"
     ]
    }
   ],
   "source": [
    "for i_episode in range(3):\n",
    "\n",
    "    state = env.reset()\n",
    "    print('-------------------')\n",
    "    print(f'PLAY EPISODE {i_episode+1}')\n",
    "    print('-------------------')\n",
    "    while True:\n",
    "        action = env.action_space.sample() # random policy\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        print('Player\\'s card:', state[0])\n",
    "        print('Dealer\\'s card:', state[1])\n",
    "        print('Any ace:', state[2])\n",
    "        if action == 1: # 1 for draw and 0 for stick\n",
    "            if next_state[0] > 21:\n",
    "                print(f'Random action: {action} -> Next card: {next_state[0]} (Bust!)')\n",
    "            else:\n",
    "                print(f'Random action: {action} -> Next card: {next_state[0]}')\n",
    "        else:\n",
    "            print(f'Random action: {action}')\n",
    "        state = next_state\n",
    "        print('-------------------')\n",
    "        if done:\n",
    "            print('End game\\'s reward:', reward)\n",
    "            print('*You won :)\\n') if reward > 0 else print('*You lost :(\\n')\n",
    "            break\n",
    "            \n",
    "print('*The reason of lost could be bust or dealer\\'s sum card is more than player\\'s own.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_game(env, policy, display=True):\n",
    "    \n",
    "    state = env.reset()\n",
    "    episode = []\n",
    "    finished = False\n",
    "    \n",
    "    while not finished:\n",
    "        if display:\n",
    "            clear_output(True)\n",
    "            env.render()\n",
    "            sleep(1)\n",
    "            \n",
    "        timestep = []\n",
    "        timestep.append(state)\n",
    "        action = optimal_policy[state]\n",
    "        \n",
    "        state, reward, finished, info = env.step(action)\n",
    "        timestep.append(action)\n",
    "        timestep.append(reward)\n",
    "        \n",
    "        episode.append(timestep)\n",
    "        \n",
    "    if display:\n",
    "        clear_output(True)\n",
    "        env.render()\n",
    "        sleep(1)\n",
    "        \n",
    "    return episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_policy(env, policy, num_episodes=100):\n",
    "    wins = 0; loss = 0\n",
    "    \n",
    "    print('Now, the agent is playing...')\n",
    "    for i in range(num_episodes):\n",
    "        episode = run_game(env, policy, display=False)[-1][-1]\n",
    "        if episode == 1:\n",
    "            wins += 1\n",
    "                \n",
    "    return wins/ num_episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: MC Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_episode_from_limit_stochastic(env):\n",
    "    \n",
    "    episode = []\n",
    "    state = env.reset()\n",
    "    while True:\n",
    "        probs = [0.8, 0.2] if state[0] > 18 else [0.2, 0.8] # state index 0 is for player\n",
    "        action = np.random.choice(np.arange(2), p=probs)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        state = next_state\n",
    "        episode.append((state, action, reward))\n",
    "        if done:\n",
    "            break\n",
    "    return episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this function will evaluate action value function\n",
    "def mc_prediction_q(env, generate_episode, num_episodes, gamma=0.9):\n",
    "    \n",
    "    # init empty dictionaries of arrays\n",
    "    returns_sum = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "    N = defaultdict(lambda: np.zeros(env.action_space.n)) # total number of times the agent visit particular state action pair over all episodes\n",
    "    Q = defaultdict(lambda: np.zeros(env.action_space.n)) # Q-table which contains the estimate for the actual values\n",
    "    \n",
    "    # loop over episodes\n",
    "    for i_episode in range(1, num_episodes+1):\n",
    "        \n",
    "        # monitor progress\n",
    "        if i_episode % 1000 == 0:\n",
    "            print(\"\\rEpisode {}/{}.\".format(i_episode, num_episodes), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "        # generate an episode\n",
    "        episode = generate_episode(env)\n",
    "        \n",
    "        # obtain the states, actions and rewards\n",
    "        states, actions, rewards = zip(*episode)\n",
    "        \n",
    "        # prepare for discounting\n",
    "        discounts = np.array([gamma**i for i in range(len(rewards)+1)])\n",
    "        \n",
    "        # update the sum of the returns, number of visits, and action-values\n",
    "        # this function estimates for each state-action pair in the episode\n",
    "        for i, state in enumerate(states):\n",
    "            returns_sum[state][actions[i]] += sum(rewards[i:]*discounts[:-(1+i)])\n",
    "            N[state][actions[i]] += 1.0 # in blackjack example, first-visit and every-visit monte carlo predictions are equivalent\n",
    "            Q[state][actions[i]] = returns_sum[state][actions[i]] / N[state][actions[i]]\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 135000/500000."
     ]
    }
   ],
   "source": [
    "# obtain the action-value function\n",
    "Q = mc_prediction_q(env, generate_episode_from_limit_stochastic, num_episodes=500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# obtain the corresponding state-value function\n",
    "V_to_plot = dict((k, (k[0]>18) * (np.dot([0.8, 0.2], v)) + (k[0]<=18) * (np.dot([0.2, 0.8], v))) \\\n",
    "                  for k, v in Q.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the state-value function\n",
    "plot_blackjack_values(V_to_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: MC Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_probability(Q_s, epsilon, nA):\n",
    "    \n",
    "    policy_s = np.ones(nA) * epsilon / nA # epsilon 0 is guaranteed to always select the greedy action\n",
    "    best_a = np.argmax(Q_s)\n",
    "    policy_s[best_a] = 1 - epsilon + (epsilon / nA)\n",
    "    return policy_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episode_from_Q(env, Q, epsilon, nA):\n",
    "    \n",
    "    episode = []\n",
    "    state = env.reset()\n",
    "    while True:\n",
    "        action = np.random.choice(np.arange(nA), p=get_probability(Q[state], epsilon, nA)) \\\n",
    "                                  if state in Q else env.action_space.sample()\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        episode.append((state, action, reward))\n",
    "        state = next_state\n",
    "        if done:\n",
    "            break\n",
    "    return episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_Q(env, episode, Q, alpha, gamma):\n",
    "    \n",
    "    states, actions, rewards = zip(*episode)\n",
    "    # prepare for discounting\n",
    "    discounts = np.array([gamma**i for i in range(len(rewards)+1)])\n",
    "    for i, state in enumerate(states):\n",
    "        old_Q = Q[state][actions[i]]\n",
    "        Q[state][actions[i]] = old_Q + alpha*(sum(rewards[i:]*discounts[:-(1+i)]) - old_Q)\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mc_control(env, num_episodes, alpha, gamma=1.0, eps_start=1.0, eps_decay=.99999, eps_min=0.05):\n",
    "    \n",
    "    nA = env.action_space.n\n",
    "    # initialize empty dictionary of arrays\n",
    "    Q = defaultdict(lambda: np.zeros(nA))\n",
    "    epsilon = eps_start\n",
    "    \n",
    "    # loop over episodes\n",
    "    for i_episode in range(1, num_episodes+1):\n",
    "        # monitor progress\n",
    "        if i_episode % 1000 == 0:\n",
    "            print(\"\\rEpisode {}/{}.\".format(i_episode, num_episodes), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "            \n",
    "        # set the value of decayed-epsilon greedy aprroach\n",
    "        epsilon = max(epsilon*eps_decay, eps_min)\n",
    "        # generate an episode by following epsilon-greedy policy\n",
    "        episode = generate_episode_from_Q(env, Q, epsilon, nA)\n",
    "        # update the action-value function estimate using the episode\n",
    "        Q = update_Q(env, episode, Q, alpha, gamma)\n",
    "    \n",
    "    # determine the policy corresponding to the final action-value function estimate\n",
    "    policy = dict((k, np.argmax(v)) for k, v in Q.items())\n",
    "    return policy, Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain the estimated optimal policy and action-value function\n",
    "optimal_policy, Q = mc_control(env, num_episodes=500000, alpha=2e-2, eps_decay=.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain the corresponding state-value function\n",
    "V = dict((k, np.max(v)) for k, v in Q.items())\n",
    "\n",
    "# plot the state-value function\n",
    "plot_blackjack_values(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the optimal policy\n",
    "plot_policy(optimal_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time To Play!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes=1000\n",
    "score = evaluate_policy(env, optimal_policy, num_episodes=num_episodes)\n",
    "print(f'Policy Score: {score} (Wins {int(score * num_episodes)} of {num_episodes})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
