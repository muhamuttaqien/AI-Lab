{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def view_classify(img, ps, version=\"MNIST\"):\n",
    "    ''' Function for viewing an image and it's predicted classes.\n",
    "    '''\n",
    "    ps = ps.data.numpy().squeeze()\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(figsize=(6,9), ncols=2)\n",
    "    ax1.imshow(img.resize_(1, 28, 28).numpy().squeeze())\n",
    "    ax1.axis('off')\n",
    "    ax2.barh(np.arange(10), ps)\n",
    "    ax2.set_aspect(0.1)\n",
    "    ax2.set_yticks(np.arange(10))\n",
    "    if version == \"MNIST\":\n",
    "        ax2.set_yticklabels(np.arange(10))\n",
    "    elif version == \"Fashion\":\n",
    "        ax2.set_yticklabels(['T-shirt/top',\n",
    "                            'Trouser',\n",
    "                            'Pullover',\n",
    "                            'Dress',\n",
    "                            'Coat',\n",
    "                            'Sandal',\n",
    "                            'Shirt',\n",
    "                            'Sneaker',\n",
    "                            'Bag',\n",
    "                            'Ankle Boot'], size='small');\n",
    "    ax2.set_title('Class Probability')\n",
    "    ax2.set_xlim(0, 1.1)\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,), (0.5,)),\n",
    "                              ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the training data\n",
    "trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits: tensor([[ 0.0999,  0.1044,  0.0696,  0.0628, -0.0431,  0.0703,  0.1026,  0.1474,\n",
      "          0.0503,  0.0183],\n",
      "        [ 0.0678,  0.0988,  0.1199,  0.1045, -0.0105,  0.0739,  0.0625,  0.1381,\n",
      "          0.0761, -0.0504],\n",
      "        [ 0.0580,  0.0465,  0.0398,  0.1099, -0.0443,  0.1226,  0.0271,  0.0727,\n",
      "         -0.0395, -0.0589],\n",
      "        [ 0.0236,  0.0722,  0.0669,  0.1496, -0.0303,  0.0894,  0.0863,  0.1988,\n",
      "          0.0342, -0.0894],\n",
      "        [ 0.1025,  0.0602,  0.1200,  0.0854,  0.0000,  0.1168,  0.0748,  0.2205,\n",
      "          0.1016,  0.0194],\n",
      "        [ 0.0324,  0.1419,  0.0533,  0.1027,  0.0266,  0.1524,  0.0549,  0.1144,\n",
      "         -0.0074, -0.1142],\n",
      "        [ 0.0803,  0.1133,  0.0764,  0.1068, -0.0509,  0.0975,  0.0607,  0.1127,\n",
      "          0.0346, -0.0264],\n",
      "        [ 0.0653,  0.0714,  0.0188,  0.0361, -0.0568,  0.1020,  0.1100,  0.1024,\n",
      "          0.1146, -0.0055],\n",
      "        [ 0.1279,  0.1587,  0.0883,  0.0358, -0.0407,  0.1285,  0.0494,  0.1090,\n",
      "          0.0319, -0.0178],\n",
      "        [ 0.1082,  0.1548,  0.0692,  0.0447, -0.0318,  0.1268,  0.0713,  0.1293,\n",
      "          0.0463, -0.0389],\n",
      "        [ 0.0723,  0.0967,  0.0334,  0.0315, -0.0707,  0.1157,  0.0924,  0.1819,\n",
      "          0.0839,  0.0598],\n",
      "        [ 0.0697,  0.1323,  0.0931,  0.0214, -0.0216,  0.1524,  0.0964,  0.1483,\n",
      "          0.0598, -0.0655],\n",
      "        [ 0.0718,  0.1050,  0.0760, -0.0072, -0.0325,  0.1320,  0.0199,  0.0589,\n",
      "         -0.0085, -0.1056],\n",
      "        [ 0.0424,  0.0597,  0.0746,  0.0839, -0.0580,  0.1362,  0.0663,  0.2032,\n",
      "          0.0793, -0.0324],\n",
      "        [ 0.0631,  0.0275,  0.0965,  0.1399, -0.0337,  0.1122,  0.1392,  0.1907,\n",
      "          0.0733, -0.0713],\n",
      "        [ 0.0892,  0.1405,  0.0298,  0.0500, -0.0123,  0.1051,  0.0478,  0.1467,\n",
      "          0.0273, -0.0911],\n",
      "        [ 0.0230,  0.0853,  0.0071,  0.0880, -0.0181,  0.1248,  0.0166,  0.1118,\n",
      "          0.0414, -0.1043],\n",
      "        [ 0.0355,  0.0694, -0.0072,  0.1041, -0.0645,  0.1399,  0.0675,  0.1026,\n",
      "          0.0381,  0.0364],\n",
      "        [ 0.0212,  0.0767,  0.0980,  0.0700, -0.0088,  0.1463,  0.0716,  0.1634,\n",
      "          0.0356, -0.0772],\n",
      "        [ 0.0290,  0.0653, -0.0114,  0.1282, -0.0142,  0.1018,  0.0419,  0.1618,\n",
      "         -0.0262, -0.1612],\n",
      "        [ 0.1099,  0.1069,  0.0304,  0.0807, -0.0056,  0.0969,  0.1097,  0.0992,\n",
      "          0.0036, -0.0025],\n",
      "        [ 0.0361,  0.0806,  0.0182,  0.0894, -0.0619,  0.0939,  0.0504,  0.0967,\n",
      "          0.0461, -0.0777],\n",
      "        [ 0.1095,  0.0182,  0.0594,  0.0729, -0.0861,  0.0711,  0.0845,  0.1278,\n",
      "          0.1209, -0.0145],\n",
      "        [ 0.0950,  0.0197,  0.0712,  0.1041, -0.0438,  0.0920,  0.0829,  0.0892,\n",
      "          0.0987, -0.0386],\n",
      "        [ 0.0762,  0.1070,  0.0787,  0.0429, -0.0270,  0.1167,  0.0432,  0.0576,\n",
      "          0.0232, -0.1140],\n",
      "        [ 0.0547,  0.0865,  0.0597,  0.1378, -0.0060,  0.1142,  0.0606,  0.1095,\n",
      "          0.0267, -0.0806],\n",
      "        [ 0.0608,  0.0946,  0.0898,  0.1018, -0.0208,  0.0955,  0.0890,  0.1187,\n",
      "          0.0218, -0.0695],\n",
      "        [ 0.0773,  0.0276,  0.0747,  0.0774, -0.0349,  0.1302,  0.0268,  0.1761,\n",
      "          0.0991, -0.0657],\n",
      "        [ 0.1171,  0.1525,  0.0437,  0.0305, -0.1123,  0.0867,  0.0446,  0.1817,\n",
      "          0.0618,  0.0874],\n",
      "        [ 0.0641,  0.0631,  0.1091,  0.1442, -0.0224,  0.1075,  0.0404,  0.1086,\n",
      "          0.1520, -0.1094],\n",
      "        [ 0.0340,  0.0437,  0.0688,  0.0988, -0.0737,  0.0989,  0.1048,  0.2039,\n",
      "          0.0994, -0.0412],\n",
      "        [ 0.0979,  0.0623,  0.0063,  0.0981, -0.0419,  0.0994,  0.1028,  0.0434,\n",
      "          0.0229,  0.0556],\n",
      "        [ 0.0797,  0.1000,  0.1069,  0.0886, -0.0212,  0.0993,  0.0805,  0.0953,\n",
      "          0.0749, -0.0497],\n",
      "        [ 0.0772,  0.1470,  0.0540,  0.0696, -0.0225,  0.0980,  0.0645,  0.1539,\n",
      "          0.0451, -0.0720],\n",
      "        [ 0.0607,  0.1327,  0.0654,  0.0651, -0.0718,  0.0878,  0.1297,  0.1753,\n",
      "          0.1010, -0.0077],\n",
      "        [ 0.0767,  0.1202,  0.0880,  0.1355, -0.0717,  0.1503,  0.0593,  0.1413,\n",
      "          0.0249, -0.0613],\n",
      "        [ 0.1359,  0.0916,  0.0557,  0.1011, -0.0219,  0.0230,  0.1149,  0.0453,\n",
      "          0.0627,  0.0409],\n",
      "        [ 0.0741,  0.0361,  0.0466,  0.0178, -0.0806,  0.0973,  0.0161,  0.1379,\n",
      "          0.0943, -0.0115],\n",
      "        [ 0.0405,  0.1100, -0.0184,  0.0790, -0.0608,  0.1676,  0.0870,  0.1496,\n",
      "          0.0271,  0.0584],\n",
      "        [ 0.0103,  0.0781,  0.0448,  0.0808, -0.1129,  0.1093,  0.0874,  0.1959,\n",
      "          0.0563,  0.0130],\n",
      "        [ 0.1024,  0.0984,  0.0411,  0.0697, -0.0484,  0.0994,  0.0961,  0.1089,\n",
      "          0.0224, -0.0082],\n",
      "        [ 0.0459,  0.1048, -0.0028,  0.0508, -0.0281,  0.0996,  0.0889,  0.1143,\n",
      "          0.0121, -0.0163],\n",
      "        [ 0.0623,  0.1026,  0.0263,  0.0677,  0.0148,  0.1071,  0.0399,  0.1196,\n",
      "         -0.0061, -0.0425],\n",
      "        [ 0.0881,  0.0790,  0.1141,  0.0265, -0.0731,  0.1047,  0.1153,  0.1927,\n",
      "          0.1630, -0.0031],\n",
      "        [ 0.1160,  0.1084,  0.0366, -0.0030,  0.0082,  0.0674,  0.1126,  0.0954,\n",
      "          0.0268, -0.0074],\n",
      "        [ 0.1617,  0.1514,  0.0510,  0.0518, -0.0323,  0.0772,  0.1049,  0.0906,\n",
      "         -0.0195,  0.0488],\n",
      "        [ 0.1073,  0.1476,  0.0294,  0.0939, -0.0084,  0.0509,  0.0820,  0.1754,\n",
      "          0.0611, -0.0370],\n",
      "        [ 0.0773,  0.0187,  0.0840,  0.1311, -0.0743,  0.0830,  0.0785,  0.0604,\n",
      "          0.0358,  0.0072],\n",
      "        [ 0.0587,  0.0257,  0.1183,  0.1353, -0.0093,  0.0449,  0.0504,  0.1617,\n",
      "          0.0808, -0.1085],\n",
      "        [ 0.0437,  0.0575,  0.0795,  0.0923, -0.0335,  0.1393,  0.0858,  0.1211,\n",
      "          0.0190, -0.0852],\n",
      "        [ 0.0187,  0.1072,  0.0672,  0.1087, -0.0447,  0.1251,  0.0774,  0.1792,\n",
      "          0.0374, -0.0782],\n",
      "        [ 0.0403,  0.0718, -0.0029,  0.1053, -0.0044,  0.1193, -0.0057,  0.0615,\n",
      "         -0.0010, -0.0907],\n",
      "        [ 0.0519,  0.0985,  0.1136,  0.0292, -0.0511,  0.1036,  0.0386,  0.2123,\n",
      "          0.1227, -0.0683],\n",
      "        [ 0.0094,  0.1108,  0.0514,  0.0926, -0.0067,  0.0452,  0.0785,  0.1315,\n",
      "          0.0406, -0.1497],\n",
      "        [ 0.1182,  0.0508,  0.0676,  0.0606, -0.0897,  0.0866,  0.1041,  0.1307,\n",
      "          0.1197, -0.0607],\n",
      "        [ 0.0111,  0.0660,  0.0285,  0.0629, -0.0494,  0.0974,  0.0421,  0.1171,\n",
      "          0.0264, -0.0794],\n",
      "        [ 0.0646,  0.0809,  0.0777,  0.1164, -0.0429,  0.1346,  0.0838,  0.1481,\n",
      "          0.0410, -0.0296],\n",
      "        [ 0.1338,  0.1018,  0.0455, -0.0127, -0.0241,  0.0602,  0.1239,  0.1095,\n",
      "          0.0520,  0.0270],\n",
      "        [ 0.1028,  0.0822,  0.0406,  0.1043, -0.0332,  0.0839,  0.0788,  0.1202,\n",
      "          0.0182,  0.0006],\n",
      "        [ 0.0898,  0.1263,  0.1112,  0.0752,  0.0028,  0.0803,  0.1320,  0.1240,\n",
      "          0.0379, -0.0305],\n",
      "        [ 0.0363,  0.0629,  0.0509,  0.0789, -0.0395,  0.0914,  0.0251,  0.1367,\n",
      "          0.1046, -0.0529],\n",
      "        [ 0.0964,  0.0713,  0.0479,  0.0763, -0.0651,  0.1287,  0.1247,  0.1342,\n",
      "          0.0742,  0.0309],\n",
      "        [ 0.1270,  0.0909,  0.0705,  0.0758, -0.0132,  0.0558,  0.1630,  0.1300,\n",
      "          0.0842, -0.0344],\n",
      "        [ 0.0897,  0.1266,  0.0834,  0.0845, -0.0071,  0.1289,  0.1000,  0.1314,\n",
      "          0.0544, -0.0119]], grad_fn=<ThAddmmBackward>)\n",
      "tensor(2.3028, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "# build a feed-forward network\n",
    "model = nn.Sequential(nn.Linear(784, 128),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(128, 64),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(64, 10))\n",
    "\n",
    "# define the loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# get our data\n",
    "images, labels = next(iter(trainloader))\n",
    "\n",
    "# flatten images\n",
    "images = images.view(images.shape[0], -1)\n",
    "\n",
    "# forward pass, get our logits\n",
    "logits = model(images)\n",
    "print('logits:',logits)\n",
    "\n",
    "# calculate the loss with the logits and the labels\n",
    "loss = criterion(logits, labels)\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Negative Log Likelihood Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logps: tensor([[-2.2446, -2.4286, -2.3735, -2.2371, -2.2173, -2.2000, -2.3014, -2.4951,\n",
      "         -2.2062, -2.3695],\n",
      "        [-2.2335, -2.4340, -2.3877, -2.2518, -2.2388, -2.1790, -2.2660, -2.4616,\n",
      "         -2.2430, -2.3728],\n",
      "        [-2.2959, -2.3698, -2.3806, -2.2576, -2.3006, -2.2078, -2.3075, -2.3483,\n",
      "         -2.1728, -2.4115],\n",
      "        [-2.2640, -2.3710, -2.3538, -2.2533, -2.1837, -2.1967, -2.2748, -2.4910,\n",
      "         -2.2814, -2.3964],\n",
      "        [-2.2502, -2.4222, -2.3907, -2.2888, -2.1332, -2.1903, -2.2824, -2.5325,\n",
      "         -2.2387, -2.3592],\n",
      "        [-2.2916, -2.4401, -2.3828, -2.2602, -2.2556, -2.1771, -2.2608, -2.3831,\n",
      "         -2.2221, -2.3856],\n",
      "        [-2.2469, -2.3275, -2.4265, -2.3020, -2.2690, -2.1676, -2.3085, -2.4779,\n",
      "         -2.1828, -2.3603],\n",
      "        [-2.2946, -2.3110, -2.3617, -2.3398, -2.2790, -2.1793, -2.2990, -2.3853,\n",
      "         -2.1934, -2.4084],\n",
      "        [-2.2203, -2.3262, -2.3917, -2.3340, -2.2729, -2.2006, -2.2694, -2.4498,\n",
      "         -2.1713, -2.4313],\n",
      "        [-2.2280, -2.3910, -2.3746, -2.2376, -2.2111, -2.2165, -2.3009, -2.5317,\n",
      "         -2.2223, -2.3611],\n",
      "        [-2.2374, -2.3333, -2.3300, -2.3062, -2.2071, -2.2232, -2.3136, -2.4497,\n",
      "         -2.2138, -2.4471],\n",
      "        [-2.2902, -2.4334, -2.3345, -2.2621, -2.1974, -2.1617, -2.2952, -2.4539,\n",
      "         -2.2530, -2.3859],\n",
      "        [-2.2362, -2.3333, -2.4409, -2.3559, -2.2651, -2.1796, -2.3110, -2.4390,\n",
      "         -2.1535, -2.3550],\n",
      "        [-2.3274, -2.3176, -2.4050, -2.2986, -2.2957, -2.2005, -2.3035, -2.3433,\n",
      "         -2.1527, -2.4106],\n",
      "        [-2.2438, -2.4501, -2.4045, -2.2712, -2.2453, -2.1541, -2.2390, -2.4641,\n",
      "         -2.2379, -2.3653],\n",
      "        [-2.2383, -2.3762, -2.3699, -2.2687, -2.1599, -2.2168, -2.2811, -2.5052,\n",
      "         -2.2193, -2.4452],\n",
      "        [-2.2818, -2.3712, -2.3845, -2.2471, -2.2415, -2.1812, -2.2422, -2.4482,\n",
      "         -2.2765, -2.3841],\n",
      "        [-2.2697, -2.3869, -2.3489, -2.2620, -2.2289, -2.1726, -2.2618, -2.4599,\n",
      "         -2.2796, -2.3896],\n",
      "        [-2.2449, -2.4320, -2.3825, -2.2296, -2.2306, -2.1842, -2.2763, -2.4895,\n",
      "         -2.2223, -2.3826],\n",
      "        [-2.2650, -2.3785, -2.4064, -2.3033, -2.2489, -2.2107, -2.2803, -2.3980,\n",
      "         -2.1566, -2.4147],\n",
      "        [-2.2927, -2.3155, -2.4031, -2.3140, -2.2621, -2.1892, -2.2676, -2.4033,\n",
      "         -2.1910, -2.4183],\n",
      "        [-2.3183, -2.3246, -2.4131, -2.2811, -2.2341, -2.2153, -2.2760, -2.4431,\n",
      "         -2.1695, -2.3860],\n",
      "        [-2.2902, -2.3841, -2.4073, -2.2396, -2.2291, -2.1851, -2.2891, -2.4515,\n",
      "         -2.2299, -2.3553],\n",
      "        [-2.1978, -2.3505, -2.3840, -2.3042, -2.2328, -2.2482, -2.2864, -2.4412,\n",
      "         -2.1789, -2.4424],\n",
      "        [-2.3049, -2.3147, -2.4140, -2.3184, -2.2448, -2.1938, -2.2818, -2.4543,\n",
      "         -2.1743, -2.3603],\n",
      "        [-2.2948, -2.4033, -2.3859, -2.3009, -2.1856, -2.1558, -2.2906, -2.4536,\n",
      "         -2.2446, -2.3512],\n",
      "        [-2.2728, -2.3568, -2.4101, -2.3421, -2.2710, -2.1815, -2.2728, -2.4154,\n",
      "         -2.1587, -2.3815],\n",
      "        [-2.3112, -2.3915, -2.3818, -2.2844, -2.2327, -2.1669, -2.2466, -2.4170,\n",
      "         -2.2450, -2.3803],\n",
      "        [-2.2475, -2.3155, -2.3737, -2.2905, -2.2061, -2.2160, -2.2711, -2.4882,\n",
      "         -2.2276, -2.4299],\n",
      "        [-2.2416, -2.3919, -2.3704, -2.3112, -2.2799, -2.1869, -2.3091, -2.3907,\n",
      "         -2.1687, -2.4094],\n",
      "        [-2.2696, -2.4150, -2.3901, -2.2567, -2.2363, -2.1606, -2.2513, -2.4697,\n",
      "         -2.2503, -2.3686],\n",
      "        [-2.2625, -2.3940, -2.4110, -2.2793, -2.2199, -2.1868, -2.2627, -2.4551,\n",
      "         -2.2178, -2.3764],\n",
      "        [-2.2752, -2.3582, -2.3726, -2.2497, -2.1722, -2.1900, -2.2332, -2.5748,\n",
      "         -2.2799, -2.3808],\n",
      "        [-2.2813, -2.3988, -2.3746, -2.2962, -2.2337, -2.1801, -2.2338, -2.4433,\n",
      "         -2.2312, -2.3885],\n",
      "        [-2.2873, -2.3917, -2.3237, -2.2762, -2.1542, -2.1839, -2.2792, -2.5040,\n",
      "         -2.2672, -2.4071],\n",
      "        [-2.2871, -2.3708, -2.3905, -2.3397, -2.2020, -2.1878, -2.2669, -2.4062,\n",
      "         -2.2009, -2.4098],\n",
      "        [-2.2331, -2.3478, -2.4221, -2.3043, -2.2442, -2.2306, -2.3368, -2.4207,\n",
      "         -2.1531, -2.3692],\n",
      "        [-2.2505, -2.3449, -2.3753, -2.3166, -2.3043, -2.2188, -2.3011, -2.3701,\n",
      "         -2.1440, -2.4322],\n",
      "        [-2.2557, -2.4229, -2.3471, -2.2635, -2.1635, -2.2109, -2.2947, -2.4986,\n",
      "         -2.2386, -2.3775],\n",
      "        [-2.3167, -2.3539, -2.3905, -2.3009, -2.2354, -2.1594, -2.2740, -2.4374,\n",
      "         -2.2261, -2.3639],\n",
      "        [-2.2247, -2.3837, -2.4231, -2.3103, -2.2439, -2.2029, -2.2563, -2.4331,\n",
      "         -2.2015, -2.3843],\n",
      "        [-2.2095, -2.3846, -2.3920, -2.3060, -2.2605, -2.1954, -2.2588, -2.4700,\n",
      "         -2.1932, -2.3993],\n",
      "        [-2.2289, -2.4349, -2.3621, -2.2475, -2.1963, -2.2115, -2.2982, -2.4767,\n",
      "         -2.2241, -2.3917],\n",
      "        [-2.2643, -2.3602, -2.3973, -2.2672, -2.2156, -2.2323, -2.2705, -2.3963,\n",
      "         -2.2032, -2.4534],\n",
      "        [-2.2701, -2.4337, -2.3634, -2.2627, -2.2252, -2.1569, -2.2846, -2.4370,\n",
      "         -2.2697, -2.3591],\n",
      "        [-2.2601, -2.3936, -2.3741, -2.3233, -2.2873, -2.1726, -2.2920, -2.4352,\n",
      "         -2.1568, -2.3693],\n",
      "        [-2.2839, -2.3618, -2.3940, -2.3030, -2.2896, -2.1806, -2.2501, -2.3345,\n",
      "         -2.2433, -2.4080],\n",
      "        [-2.2520, -2.3905, -2.3598, -2.3366, -2.2205, -2.2631, -2.2798, -2.4048,\n",
      "         -2.1925, -2.3505],\n",
      "        [-2.2355, -2.3922, -2.3624, -2.3038, -2.1959, -2.1333, -2.3155, -2.4628,\n",
      "         -2.2608, -2.4115],\n",
      "        [-2.2262, -2.3783, -2.3710, -2.3134, -2.2804, -2.2238, -2.2881, -2.4029,\n",
      "         -2.1965, -2.3699],\n",
      "        [-2.2329, -2.3859, -2.3674, -2.2611, -2.2846, -2.1370, -2.2911, -2.4794,\n",
      "         -2.2701, -2.3566],\n",
      "        [-2.2586, -2.3692, -2.4150, -2.2412, -2.2375, -2.1601, -2.2525, -2.4926,\n",
      "         -2.2429, -2.4055],\n",
      "        [-2.2751, -2.3529, -2.3665, -2.3158, -2.2696, -2.1766, -2.3005, -2.4413,\n",
      "         -2.2056, -2.3498],\n",
      "        [-2.2418, -2.3342, -2.4421, -2.3749, -2.1908, -2.2112, -2.3192, -2.4360,\n",
      "         -2.1304, -2.3991],\n",
      "        [-2.2391, -2.3734, -2.3899, -2.2801, -2.2283, -2.1723, -2.2349, -2.4934,\n",
      "         -2.2324, -2.4322],\n",
      "        [-2.2740, -2.2701, -2.3956, -2.3179, -2.2968, -2.2082, -2.3099, -2.3739,\n",
      "         -2.1703, -2.4394],\n",
      "        [-2.2618, -2.3468, -2.4150, -2.3302, -2.2114, -2.1706, -2.2526, -2.4419,\n",
      "         -2.2278, -2.4081],\n",
      "        [-2.2891, -2.2913, -2.4207, -2.3271, -2.2399, -2.2154, -2.3380, -2.3989,\n",
      "         -2.1373, -2.4060],\n",
      "        [-2.3150, -2.3238, -2.4190, -2.3298, -2.2587, -2.2330, -2.3123, -2.4247,\n",
      "         -2.0585, -2.4063],\n",
      "        [-2.2488, -2.3947, -2.3900, -2.3203, -2.1515, -2.1936, -2.3094, -2.4768,\n",
      "         -2.2129, -2.3766],\n",
      "        [-2.3450, -2.3381, -2.3796, -2.3616, -2.2488, -2.1964, -2.3314, -2.2944,\n",
      "         -2.1138, -2.4617],\n",
      "        [-2.2276, -2.4274, -2.3900, -2.2652, -2.2247, -2.2085, -2.2812, -2.4477,\n",
      "         -2.1926, -2.4050],\n",
      "        [-2.2801, -2.4288, -2.3660, -2.2433, -2.2499, -2.1414, -2.3276, -2.3861,\n",
      "         -2.2420, -2.3973],\n",
      "        [-2.2472, -2.4153, -2.3931, -2.2498, -2.2454, -2.1898, -2.2478, -2.4650,\n",
      "         -2.2216, -2.3938]], grad_fn=<LogSoftmaxBackward>)\n",
      "tensor(2.3299, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(nn.Linear(784, 128),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(128, 64),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(64, 10),\n",
    "                      nn.LogSoftmax(dim=1))\n",
    "\n",
    "# define the loss\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "# get our data\n",
    "images, labels = next(iter(trainloader))\n",
    "\n",
    "# flatten images\n",
    "images = images.view(images.shape[0], -1)\n",
    "\n",
    "# forward pass, get our log-probabilities\n",
    "logps = model(images)\n",
    "print('logps:',logps)\n",
    "\n",
    "# calculate the loss with the logps and the labels\n",
    "loss = criterion(logps, labels)\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get gradients using backward to update the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before backward pass: \n",
      " None\n",
      "After backward pass: \n",
      " tensor([[ 0.0001,  0.0001,  0.0001,  ...,  0.0001,  0.0001,  0.0001],\n",
      "        [-0.0014, -0.0014, -0.0014,  ..., -0.0014, -0.0014, -0.0014],\n",
      "        [ 0.0005,  0.0005,  0.0005,  ...,  0.0005,  0.0005,  0.0005],\n",
      "        ...,\n",
      "        [-0.0026, -0.0026, -0.0026,  ..., -0.0026, -0.0026, -0.0026],\n",
      "        [-0.0012, -0.0012, -0.0012,  ..., -0.0012, -0.0012, -0.0012],\n",
      "        [-0.0030, -0.0030, -0.0030,  ..., -0.0030, -0.0030, -0.0030]])\n"
     ]
    }
   ],
   "source": [
    "print('Before backward pass: \\n', model[0].weight.grad)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print('After backward pass: \\n', model[0].weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "# optimizers require the parameters to optimize and a learning rate\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial weights -  Parameter containing:\n",
      "tensor([[-0.0028, -0.0334,  0.0160,  ..., -0.0031, -0.0083, -0.0170],\n",
      "        [-0.0232,  0.0223,  0.0020,  ..., -0.0091,  0.0205,  0.0212],\n",
      "        [ 0.0277,  0.0170,  0.0269,  ..., -0.0096,  0.0299,  0.0346],\n",
      "        ...,\n",
      "        [-0.0328, -0.0144,  0.0197,  ...,  0.0007,  0.0041,  0.0210],\n",
      "        [-0.0022,  0.0088,  0.0005,  ...,  0.0065, -0.0325,  0.0202],\n",
      "        [-0.0013,  0.0311, -0.0073,  ...,  0.0214, -0.0325, -0.0265]],\n",
      "       requires_grad=True)\n",
      "\n",
      "\n",
      "Gradient - tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0019,  0.0019,  0.0019,  ...,  0.0019,  0.0019,  0.0019],\n",
      "        [ 0.0003,  0.0003,  0.0003,  ...,  0.0003,  0.0003,  0.0003],\n",
      "        ...,\n",
      "        [-0.0015, -0.0015, -0.0015,  ..., -0.0015, -0.0015, -0.0015],\n",
      "        [-0.0007, -0.0007, -0.0007,  ..., -0.0007, -0.0007, -0.0007],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])\n",
      "\n",
      "\n",
      "Updated weights -  Parameter containing:\n",
      "tensor([[-0.0028, -0.0334,  0.0160,  ..., -0.0031, -0.0083, -0.0170],\n",
      "        [-0.0232,  0.0223,  0.0019,  ..., -0.0091,  0.0205,  0.0212],\n",
      "        [ 0.0276,  0.0170,  0.0269,  ..., -0.0096,  0.0298,  0.0346],\n",
      "        ...,\n",
      "        [-0.0328, -0.0143,  0.0197,  ...,  0.0007,  0.0041,  0.0210],\n",
      "        [-0.0022,  0.0088,  0.0005,  ...,  0.0065, -0.0325,  0.0202],\n",
      "        [-0.0013,  0.0311, -0.0073,  ...,  0.0214, -0.0325, -0.0265]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print('Initial weights - ', model[0].weight)\n",
    "\n",
    "images, labels = next(iter(trainloader))\n",
    "images.resize_(64, 784)\n",
    "\n",
    "# clear the gradients, do this because gradients are accumulated\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# forward pass, then backward pass, then update weights\n",
    "output = model(images)\n",
    "loss = criterion(output, labels)\n",
    "loss.backward()\n",
    "print('\\n')\n",
    "print('Gradient -', model[0].weight.grad)\n",
    "\n",
    "# take an update step and few the new weights\n",
    "optimizer.step()\n",
    "\n",
    "print('\\n')\n",
    "print('Updated weights - ', model[0].weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 1.9280028730821508\n",
      "Training loss: 0.8929886843032165\n",
      "Training loss: 0.5305934790482145\n",
      "Training loss: 0.43063837922076936\n",
      "Training loss: 0.3862765329256495\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(nn.Linear(784, 128),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(128, 64),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(64, 10),\n",
    "                      nn.LogSoftmax(dim=1))\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.003)\n",
    "\n",
    "epochs = 5\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for images, labels in trainloader:\n",
    "\n",
    "        # flatten MNIST images into a 784 long vector\n",
    "        images = images.view(images.shape[0], -1)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(images)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    else:\n",
    "        print(f\"Training loss: {running_loss/len(trainloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAADhCAYAAACdkiHQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nskRAAQCy9H+o0v/e2rpPMgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10e6b52b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import helper\n",
    "\n",
    "images, labels = next(iter(trainloader))\n",
    "\n",
    "img = images[0].view(1, 784)\n",
    "# turn off gradients to speed up this part\n",
    "with torch.no_grad():\n",
    "    logps = model(img)\n",
    "    \n",
    "# output of the network are log-probabilities, need to take exponential for probabilities\n",
    "ps = torch.exp(logps)\n",
    "view_classify(img.view(1, 28, 28), ps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
