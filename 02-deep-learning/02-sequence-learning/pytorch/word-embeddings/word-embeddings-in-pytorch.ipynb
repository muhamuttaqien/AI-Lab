{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-trained Word Embeddings in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pre-trained Word Embeddings (GloVe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define documents\n",
    "docs = ['Well done!',\n",
    "        'Good work',\n",
    "        'Great effort',\n",
    "        'nice work',\n",
    "        'Excellent!',\n",
    "        'Weak',\n",
    "        'Poor effort!',\n",
    "        'not good',\n",
    "        'poor work',\n",
    "        'Could have done better.']\n",
    "\n",
    "# define class labels\n",
    "labels = np.array([1,1,1,1,1,0,0,0,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join all the sentences together and extract the unique characters from the combined sentences\n",
    "words = set(' '.join(docs).split())\n",
    "\n",
    "# creating a dictionary that maps integers to the words\n",
    "int2word = dict(enumerate(words))\n",
    "\n",
    "# creating another dictionary that maps characters to integers\n",
    "word2int = { word: idx for idx, word in int2word.items() }\n",
    "\n",
    "# calculate vocab size\n",
    "vocab_size = len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the whole pre-trained word embedding into memory\n",
    "embeddings_glove = dict()\n",
    "f = open('./embeddings/glove.6B.100d.txt')\n",
    "\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0] # you dan filter the embedding for the unique words in your training data to make it faster\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_glove[word] = coefs\n",
    "\n",
    "f.close()\n",
    "print('Loaded %s word vectors from GloVe.' % len(embeddings_glove))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the weight matrix for words in training docs\n",
    "embedding_matrix = np.zeros((vocab_size, 100))\n",
    "for word, i in word2int.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_embedding_layer(embedding_matrix, non_trainable=False):\n",
    "    \n",
    "    n_embeddings, embedding_dim = embedding_matrix.size()\n",
    "    embedding_layer = nn.Embedding(n_embeddings, embedding_dim)\n",
    "    embedding_layer.load_state_dict({'weight': embedding_matrix})\n",
    "    \n",
    "    if non_trainable:\n",
    "        embedding_layer.weight.requires_grad = False\n",
    "        \n",
    "    return embedding_layer, n_embeddings, embedding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GRU(nn.Module):\n",
    "    \n",
    "    # embedding_matrix includes input_size and embedding_dim\n",
    "    def __init__(self, embedding_matrix, hidden_dim, output_size, n_layers):\n",
    "        super(model, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.embedding_layer, self.n_embeddings, self.embedding_dim = create_embedding_layer(embedding_matrix, True)        \n",
    "        self.gru_layer = nn.GRU(embedding_dim, hidden_dim, n_layers, batch_first=True)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc_layer = nn.Linear(hidden_dim, output_size)\n",
    "        \n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_dim)\n",
    "        return Variable(hidden)\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        \n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        x = x.long()\n",
    "        embeds = self.embedding_layer(x)\n",
    "        gru_out, hidden = self.gru_layer(embeds, hidden)\n",
    "        output = self.fc_layer(self.relu(gru_out[:,-1]))\n",
    "        \n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gru = GRU(embedding_matrix, hidden_dim=256, output_size=1, n_layers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
