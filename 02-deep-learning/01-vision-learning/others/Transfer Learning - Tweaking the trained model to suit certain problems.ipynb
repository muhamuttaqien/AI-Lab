{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.applications import vgg16 \n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing.image import load_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pre-trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_model = vgg16.VGG16(weights='imagenet',\n",
    "                        include_top=False,\n",
    "                        input_shape=(224, 224, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup The Data Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = './datasets/train/'\n",
    "validation_dir = './datasets/validation/'\n",
    " \n",
    "n_train = 900 # of total 3302\n",
    "n_validation = 240 # of total 824"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Notice that batch size must match with the amount of train or validation!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the batch size according to your system RAM\n",
    "train_batch_size = 20\n",
    "validation_batch_size = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remember that the original image size is 224 x 224 and the last layer has a shape of 7 x 7 x 512\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_features = np.zeros(shape=(n_train, 7, 7, 512)) #(600, 7, 7, 512)\n",
    "train_labels = np.zeros(shape=(n_train, 3))\n",
    " \n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=train_batch_size,\n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "for inputs_batch, labels_batch in train_generator:\n",
    "    features_batch = vgg_model.predict(inputs_batch)\n",
    "    train_features[i * train_batch_size: (i+1) * train_batch_size] = features_batch\n",
    "    train_labels[i * train_batch_size : (i+1) * train_batch_size] = labels_batch\n",
    "    \n",
    "    i+=1\n",
    "    if i * train_batch_size >= n_train:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = np.reshape(train_features, (n_train, 7 * 7 * 512)) #(600, 25088)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remember that the original image size is 224 x 224 anfd the last layer has a shape of 7 x 7 x 512\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "validation_features = np.zeros(shape=(n_validation, 7, 7, 512)) #(600, 7, 7, 512)\n",
    "validation_labels = np.zeros(shape=(n_validation, 3))\n",
    " \n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    validation_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=validation_batch_size,\n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for inputs_batch, labels_batch in validation_generator:\n",
    "    features_batch = vgg_model.predict(inputs_batch)\n",
    "    validation_features[i * validation_batch_size: (i+1) * validation_batch_size] = features_batch\n",
    "    validation_labels[i * validation_batch_size : (i+1) * validation_batch_size] = labels_batch\n",
    "\n",
    "    i+=1\n",
    "    if i * validation_batch_size >= n_validation:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_features = np.reshape(validation_features, (n_validation, 7 * 7 * 512)) #(160, 25088)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create A New Model (Fully-connected Layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    " \n",
    "model_fcl = models.Sequential()\n",
    "model_fcl.add(layers.Dense(256, activation='relu', input_dim=7 * 7 * 512))\n",
    "model_fcl.add(layers.Dropout(0.5))\n",
    "model_fcl.add(layers.Dense(3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fcl.compile(optimizer=optimizers.RMSprop(lr=2e-4),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_fcl = model_fcl.fit(train_features,\n",
    "                        train_labels,\n",
    "                        epochs=40,\n",
    "                        batch_size=20,\n",
    "                        validation_data=(validation_features, validation_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the Loss Curves\n",
    "plt.figure(figsize=[8,6])\n",
    "plt.plot(history_fcl.history['loss'],'r',linewidth=3.0)\n",
    "plt.plot(history_fcl.history['val_loss'],'b',linewidth=3.0)\n",
    "plt.legend(['Training loss', 'Validation Loss'],fontsize=18)\n",
    "plt.xlabel('Epochs ',fontsize=16)\n",
    "plt.ylabel('Loss',fontsize=16)\n",
    "plt.title('Loss Curves',fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the Accuracy Curves\n",
    "plt.figure(figsize=[8,6])\n",
    "plt.plot(history_fcl.history['acc'],'r',linewidth=3.0)\n",
    "plt.plot(history_fcl.history['val_acc'],'b',linewidth=3.0)\n",
    "plt.legend(['Training Accuracy', 'Validation Accuracy'],fontsize=18)\n",
    "plt.xlabel('Epochs ',fontsize=16)\n",
    "plt.ylabel('Accuracy',fontsize=16)\n",
    "plt.title('Accuracy Curves',fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnames = validation_generator.filenames\n",
    "ground_truth = validation_generator.classes\n",
    "\n",
    "# getting the mapping from class index to class label\n",
    "label_to_index = validation_generator.class_indices\n",
    "idx_to_label = dict((v,k) for k,v in label_to_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model_fcl.predict_classes(validation_features)\n",
    "probabilities = model_fcl.predict(validation_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = np.where(predictions != np.argmax(validation_labels, axis=1))[0]\n",
    "print(\"No of errors = {}/{} ({}%)\".format(len(errors), n_validation, (n_validation-len(errors))/n_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(errors)):\n",
    "    prediction_class = np.argmax(probabilities[errors[i]])\n",
    "    prediction_label = idx_to_label[prediction_class]\n",
    "    \n",
    "    print(\"Original label: {}, Prediction: {}, Confidence: {:.3f}\".format(fnames[errors[i]].split('/')[0],\n",
    "                                                                          prediction_label,\n",
    "                                                                          probabilities[errors[i]][prediction_class]))\n",
    "    \n",
    "    original_image = load_img(\"{}{}\".format(validation_dir, fnames[errors[i]]))\n",
    "    plt.imshow(original_image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model_fcl.save('models/fcl_transfer_learning.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
