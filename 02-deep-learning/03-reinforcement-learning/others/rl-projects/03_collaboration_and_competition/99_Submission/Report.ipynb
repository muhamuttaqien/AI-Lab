{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Udacity - Collaboration and Competition Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This report explains about the result of Collaboration and Competition Project as the 3rd project of Deep Reinforcement Learning [Udacity](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) Course. Written by [muhamuttaqien](https://github.com/muhamuttaqien)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![In Project 2, train an agent to control a robotic arms.](./images/tennis_player.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Learning Algorithm & The Choosen Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Collaboration and Competition project, I tried to solve the environment using one of the most state-of-the-art Reinforcement Learning algorithm, called **MADDPG** (stands for **Multi-Agent Deep Deterministic Policy Gradient**). We can say that this algorithm is an extension of DDPG algorithm for Multi-Agent Reinforcement Learning (MARL) setting. DDPG, as we might remember, is an off policy actor-critic algorithm that uses the concept of target networks where the input of the actor network is the current state while output is a real value or a vector representing an action chosen from a continuous action space as described in the paper [Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments](https://arxiv.org/pdf/1706.02275.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Actor-Critic Architecture\n",
    "    * Similar to single-agent Actor Critic architecture, each agent in MADDPG has it's own actor and critic network. The actor network takes in the current state of agent and output a recommended action for that agent. However the critic part is slightly different from ordinary single-agent DDPG. The critic network of each agent on MADDPG has full visibility on the environment. It not only takes in the observation and action of that particular agent, but also observations and actions of all other agents as well. Critic network has much higher visibility on what is happening while actor network can only access to the observation information of the respective agent. The critic network is active only during training time while absent in running time. The following is the diagram as illustrated on the paper.\n",
    "    \n",
    "<img src='./images/maddpg_framework.png' width='40%'>\n",
    "    \n",
    "2. Experience Replay\n",
    "    * Basically this feature is exactly the same as applied for DQN algorithm where the agent interacts with the environment and at each time step, the agent obtain a state, action, reward and next state tuple, just learn from it and then directly discard it then moves on to the next new tuple in the following time step. A separated experience memory buffer was deployed for each agent. The idea is to make sure there is enough randomness across sampling by avoiding coupling occurrence of experience entries across agents. I implemented this feature by defining dedicated class named `ReplayBuffer.`\n",
    "3. Soft Update\n",
    "    * In order to improve the stability of learning, a target network was deployed for both actor and critic network. Parameters of these target network are softly updated from time to time so that the active/ learning/ local network will have a relatively more stable target to go after. Using a soft updates strategy, the update strategy consists of slowly blending our regular network weights with our target network weights so every time step me make our target network be 99.99 percent of our target networks and only 0.01 percent of our regular network weights and we are slowly mix in our regular network weights into our target network weights.\n",
    "4. Noise for Exploration\n",
    "    * Noise was added to the action recommended by the actor network as a way of exploration. I used [Ornstein-Uhlenbeck](https://en.wikipedia.org/wiki/Ornstein%E2%80%93Uhlenbeck_process) considered as the best algorithm to add noise. Noise with a relatively smaller magnitude of standard deviation was added during the learning cycle when the agents are interactinve with the environment. The magnitude of the noise is slowly decayed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters chosen for optimally training the MADDPG agent was that `1e5` for buffer size. We set that number as maximum memory capacity for the agent to memorize all experiences obtained during learning. When the capacity reached the number of batch size, which is `512`, then the buffer randomly sampled the experiences for the agent to learn from the past. The learning step was performed for every 4 step done by the agent in order to gradually update the parameters (weights) of actor network and critic network by learning rate `1e-4` and `1e-3`, respectively. We set gamma value 0.99 since the agent was supposed to care about future rewards. Lastly, since we use separate network for estimating Q target, we set `1e-3` as tau value so we can gradually copy the parameters of our trained Actor network to update the weight of Critic network and made the learning algorithm more stable. These parameters was defined on `Collaboration_and_Competition_MADDPG.ipynb` file. For introducing exploration behaviour to the agent, I set noise by `0.5` at the beginning of learning process and then gradually and slowly decayed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Architecture for Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model architecture implemented here did't use convolutional neural network that takes in the difference between the current and previous screen patches. Instead, I used a fully connected layer that takes state vectors containing information the agent observed as input for Actor Network and for Critic Network to outputs Q value for each available action can be performed on the environment. I combined Actor & Critic Network by defining one dedicated class called `ActorCriticNetwork` which will be used to init agent in MADDPG setting.\n",
    "\n",
    "Both Actor & Critic networks defined on `model.py` file consists of a number of fully connected layers, with 256 neuron units on the hidden layer of Actor Network and 256 followed by 256 neuron units for Critic Network. Both use rectifier activation function called ReLU to introduce non-linearity for each hidden layers. The function is a simple calculation that returns the value provided as input directly, or the value 0.0 if the input is 0.0 or less. Also, I set the seed on the network to allow reproducible randomness or stochasticity behaviour to the neural network. When the agent was initialized, it automatically set Adam as the optimizer that performs stochastic gradient descent procedure to update network weights during training, and calculating `policy loss` and `value loss` to update the weights of the network so it gradually performed better for the next episodes. All work was super easily done by [PyTorch](http://pytorch.org/) Deep Learning framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Plot of Rewards & Number of Episodes Needed to Solve The Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I set the total of episodes `5000` and max time `1000` for each episode as plotted on the figure below and the result showed that the total scores moderately increased overtime as the more episodes taken by the agent to observe the environment. At the beginning the agent performed horribly and the leaarning was very unstable. But as can be seen on the graph below, the agent performance went up and solved the environment around 2400 episodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/plot_of_maddpg_evaluation.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result, my agent finally solved the task to reach the target with average score 0.503, which is very good. We can see the statistics of obtained scores progress on `Collaboration_and_Competition_MADDPG` jupyter notebook, and also the demonstration of how the successfully trained agent smartly behaves to the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Future Ideas for Improving The Agent's Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to my brief research, I found that the [Twin Delayed DDPG (TD3)](https://arxiv.org/pdf/1802.09477.pdf) or [Distributed Distributional Deterministic Policy Gradients (D4PG)](https://arxiv.org/pdf/1804.08617.pdf) algorithms might be a good improvement for our Multi-Agents environments. I'm also interested to apply [Prioritized Experience Replay](https://arxiv.org/pdf/1511.05952.pdf) as considered as a very good approach to train an agent according to the success history of DQN algorihms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Github Link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My workplace repository can be easily accessed from [muhamuttaqien](https://github.com/muhamuttaqien/AI-Lab/tree/master/02-deep-learning/03-reinforcement-learning/others/rl-projects/03_collaboration_and_competition/99_Submission) Github profile."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
