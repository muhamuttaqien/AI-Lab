{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from time import sleep\n",
    "from IPython.display import clear_output\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "from unityagents import UnityEnvironment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from noise import OUNoise\n",
    "from buffer import ReplayBuffer\n",
    "from model import ActorCriticNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore', UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "if is_cuda: device = torch.device('cuda')\n",
    "else: device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = int(1e5)\n",
    "BATCH_SIZE = 512\n",
    "GAMMA = 0.99\n",
    "TAU = 1e-3\n",
    "PRINT_EVERY = 200\n",
    "UPDATE_EVERY = 2\n",
    "\n",
    "# actor & critic network have different learning rates\n",
    "LR_ACTOR = 1e-4\n",
    "LR_CRITIC = 1e-3\n",
    "WEIGHT_DECAY = 0.0 # L2 weight decay\n",
    "\n",
    "# noise settings\n",
    "NOISE_START = 0.5\n",
    "NOISE_DECAY = 1.0\n",
    "TIME_STOP_NOISE = int(3e4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name='./Tennis.app')\n",
    "\n",
    "brain_name = env.brain_names[0] # get the brain from unity environment\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment Info\n",
      "Number of agents: 2\n",
      "State space: 24\n",
      "Action space: 2\n"
     ]
    }
   ],
   "source": [
    "print('Environment Info')\n",
    "env_info = env.reset(train_mode=False)[brain_name] # reset environment to a new, random state\n",
    "state = env_info.vector_observations\n",
    "action_size = brain.vector_action_space_size\n",
    "\n",
    "print('Number of agents: {}'.format(len(env_info.agents)))\n",
    "print('State space: {}'.format(state.shape[1]))\n",
    "print('Action space: {}'.format(action_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define [MADDPG](https://arxiv.org/pdf/1706.02275.pdf) Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DDPGAgent():\n",
    "    \"\"\"The Agent that will interacts with and learns from the environment.\"\"\"\n",
    "\n",
    "    def __init__(self, agent_id, model, state_size, action_size, seed):\n",
    "        \"\"\"Initialize an DDPG Agent object.\"\"\"\n",
    "        \n",
    "        self.agent_id = agent_id\n",
    "        \n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        \n",
    "        self.lr_actor = LR_ACTOR\n",
    "        self.lr_critic = LR_CRITIC\n",
    "        self.weight_decay = WEIGHT_DECAY\n",
    "        self.tau = TAU\n",
    "        \n",
    "        # initialize actor and critic networks\n",
    "        self.actor = model.actor.to(device)\n",
    "        self.actor_target = model.actor_target.to(device)\n",
    "        \n",
    "        self.critic = model.critic.to(device)\n",
    "        self.critic_target = model.critic_target.to(device)\n",
    "            \n",
    "        # set optimizers\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=self.lr_actor)\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=self.lr_critic, weight_decay=self.weight_decay)\n",
    "        \n",
    "        # set weights for local and target actor, respectively, critic the same\n",
    "        self.soft_update(self.actor, self.actor_target, 1)\n",
    "        self.soft_update(self.critic, self.critic_target, 1)\n",
    "        \n",
    "        # introduce noise process\n",
    "        self.noise = OUNoise(self.action_size, seed)\n",
    "        \n",
    "    def act(self, states, noise_weight=1.0, add_noise=True):\n",
    "        \"\"\"Return action for given state as per current policy.\"\"\"\n",
    "        \n",
    "        states = torch.FloatTensor(states).to(device)\n",
    "        self.actor.eval()\n",
    "        with torch.no_grad():\n",
    "            actions = self.actor(states).cpu().data.numpy()\n",
    "        self.actor.train()\n",
    "        \n",
    "        if add_noise==True:\n",
    "            self.noise_value = self.noise.sample() * noise_weight\n",
    "            actions += self.noise_value\n",
    "        \n",
    "        actions = np.clip(actions, -1, 1)\n",
    "        \n",
    "        return actions\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset noise parameters.\"\"\"\n",
    "        self.noise.reset()\n",
    "        \n",
    "    def learn(self, agent_id, experiences, gamma, all_actions, all_next_actions):\n",
    "        \"\"\"Update policy and value parameters using given batch of experience tuples.\"\"\"\n",
    "        \n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "        \n",
    "        # get the agent id\n",
    "        agent_id = torch.tensor([agent_id]).to(device)\n",
    "        \n",
    "        # obtain Q expected\n",
    "        Q_expected = self.critic(states, actions)\n",
    "        \n",
    "        next_actions = torch.cat(all_next_actions, dim=1).to(device)\n",
    "        with torch.no_grad():\n",
    "            Q_target_next = self.critic_target(next_states, next_actions)\n",
    "        \n",
    "        # compute Q target\n",
    "        Q_targets = rewards.index_select(1, agent_id) + (gamma * Q_target_next * (1 - dones.index_select(1, agent_id)))\n",
    "        \n",
    "        # compute value loss\n",
    "        value_loss = F.mse_loss(Q_expected, Q_targets.detach())\n",
    "        \n",
    "        # minimize critic loss\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        value_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "        \n",
    "        # detach actions from other agents\n",
    "        actions_pred = [actions if i == self.agent_id else actions.detach() for i, actions in enumerate(all_actions)]\n",
    "        actions_pred = torch.cat(actions_pred, dim=1).to(device)\n",
    "        \n",
    "        # update actor\n",
    "        policy_loss = -self.critic(states, actions_pred).mean()\n",
    "        \n",
    "        # minimize policy loss\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "        \n",
    "        # softly update target networks\n",
    "        self.soft_update(self.actor, self.actor_target, self.tau)\n",
    "        self.soft_update(self.critic, self.critic_target, self.tau)\n",
    "    \n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\"Softly update model parameters.\"\"\"\n",
    "        \n",
    "        for local_param, target_param in zip(local_model.parameters(), target_model.parameters()):\n",
    "            target_param.data.copy_(local_param.data * tau + target_param.data * (1.0 - tau))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MADDPGAgent():\n",
    "    \"\"\"The Agent that will interacts with and learns from the environment.\"\"\"\n",
    "    \n",
    "    def __init__(self, env, num_agents, seed):\n",
    "        \"\"\"Initialize an MADDPG Agent object.\"\"\"\n",
    "        \n",
    "        self.brain = env.brains[env.brain_names[0]] # get the brain from unity environment\n",
    "        \n",
    "        self.env_info = env.reset(train_mode=True)[brain_name]\n",
    "                \n",
    "        self.state_size = env_info.vector_observations.shape[1]\n",
    "        self.action_size = self.brain.vector_action_space_size\n",
    "        \n",
    "        self.num_agents = num_agents\n",
    "        \n",
    "        self.buffer_size = BUFFER_SIZE\n",
    "        self.batch_size = BATCH_SIZE\n",
    "        self.gamma = GAMMA\n",
    "        self.noise_weight = NOISE_START\n",
    "        self.noise_decay = NOISE_DECAY\n",
    "        self.time_stop_noise = TIME_STOP_NOISE\n",
    "        self.noise_on = True\n",
    "        \n",
    "        # create agents, each with their own actor and critic\n",
    "        models = [ActorCriticNetwork(self.num_agents, self.state_size, self.action_size, seed) for _ in range(self.num_agents)]\n",
    "        self.agents = [DDPGAgent(i, models[i], self.state_size, self.action_size, seed) for i in range(self.num_agents)]\n",
    "        \n",
    "        # set buffer\n",
    "        self.buffer = ReplayBuffer(BUFFER_SIZE, BATCH_SIZE, seed)\n",
    "        self.time_step = 0\n",
    "        \n",
    "    def act(self, all_states):\n",
    "        \n",
    "        # pass each agent's state from the environment and calculate its action\n",
    "        all_actions = []\n",
    "        for agent, state in zip(self.agents, all_states):\n",
    "            actions = agent.act(state, noise_weight=self.noise_weight, add_noise=self.noise_on)\n",
    "            self.noise_weight *= self.noise_decay\n",
    "            all_actions.append(actions)\n",
    "        \n",
    "        actions = np.array(all_actions).reshape(1, -1) # reshape 2x2 into 1x4 dimension vector\n",
    "        \n",
    "        return actions\n",
    "        \n",
    "    def step(self, all_states, all_actions, all_rewards, all_next_states, all_dones):\n",
    "        \n",
    "        all_states = all_states.reshape(1, -1) # reshape 2x24 into 1x48 dimension vector\n",
    "        all_next_states = all_next_states.reshape(1, -1) # reshape 2x24 into 1x48 dimension vector\n",
    "        \n",
    "        self.buffer.add(all_states, all_actions, all_rewards, all_next_states, all_dones)\n",
    "        \n",
    "        # if time stop noise are achieved then turning off the noise\n",
    "        if self.time_step > self.time_stop_noise:\n",
    "            self.noise_on = False\n",
    "            \n",
    "        self.time_step += 1     \n",
    "        \n",
    "        # learn every UPDATE_EVERY time steps\n",
    "        if self.time_step % UPDATE_EVERY == 0:\n",
    "            # if enough samples are available in memory, get random subset and learn\n",
    "            if len(self.buffer) > BATCH_SIZE:\n",
    "                \n",
    "                # sample from the replay buffer of each agent\n",
    "                experiences = [self.buffer.sample() for _ in range(self.num_agents)]\n",
    "                self.learn(experiences, self.gamma)\n",
    "                \n",
    "    def learn(self, experiences, gamma):\n",
    "        \n",
    "        # each agent uses its own actor to calculate next actions\n",
    "        all_actions = []\n",
    "        all_next_actions = []\n",
    "\n",
    "        for i, agent in enumerate(self.agents):\n",
    "            \n",
    "            states, _, _, next_states, _ = experiences[i]\n",
    "            agent_id = torch.tensor([i]).to(device)\n",
    "            \n",
    "            # extract agent i's state and get action via actor network\n",
    "            states = states.reshape(-1, 2, 24).index_select(1, agent_id).squeeze(1)\n",
    "            actions = agent.actor(states)\n",
    "            all_actions.append(actions)\n",
    "            \n",
    "            # extract agent i's next state and get action via target action network\n",
    "            next_states = next_states.reshape(-1, 2, 24).index_select(1, agent_id).squeeze(1)\n",
    "            next_actions = agent.actor_target(next_states)\n",
    "            all_next_actions.append(next_actions)\n",
    "            \n",
    "        # each agent learns from its experience sample\n",
    "        for i, agent in enumerate(self.agents):\n",
    "            agent.learn(i, experiences[i], gamma, all_actions, all_next_actions)\n",
    "    \n",
    "    def save(self, actor_path, critic_path):\n",
    "        \"\"\"Save trained agent.\"\"\"\n",
    "        if not os.path.exists('./agents/'): os.makedirs('./agents/')\n",
    "        \n",
    "        # save actor and critic for each agent\n",
    "        for i, agent in enumerate(self.agents):\n",
    "            torch.save(self.agents[i].actor.state_dict(), f'{actor_path}_agent{i}.pth'); torch.save(self.agents[i].critic.state_dict(), f'{critic_path}_agent{i}.pth')\n",
    "            \n",
    "    def load(self, actor_path):\n",
    "        \"\"\"Load trained agent.\"\"\"\n",
    "        for i, agent in enumerate(self.agents):\n",
    "            self.agents[i].actor.load_state_dict(torch.load(f'{actor_path}_agent{i}.pth'))\n",
    "            \n",
    "    def watch(self, num_episodes=10, max_time=1000):\n",
    "        \"\"\"Watch trained agent.\"\"\"\n",
    "        \n",
    "        for i_episode in range(1, num_episodes+1):\n",
    "\n",
    "            all_rewards_A = []\n",
    "            all_rewards_B = []\n",
    "            \n",
    "            env_info = env.reset(train_mode=False)[brain_name]\n",
    "            states = env_info.vector_observations\n",
    "\n",
    "            for time_step in range(max_time):\n",
    "\n",
    "                actions = agent.act(states)\n",
    "                env_info = env.step(actions)[brain_name]\n",
    "                next_states, rewards, dones = env_info.vector_observations, env_info.rewards, env_info.local_done\n",
    "                \n",
    "                agent.step(states, actions, rewards, next_states, dones)\n",
    "                states = next_states\n",
    "                \n",
    "                rewards_B, rewards_A = rewards\n",
    "                all_rewards_A.append(rewards_A)\n",
    "                all_rewards_B.append(rewards_B)\n",
    "                \n",
    "                if any(dones):\n",
    "                    break\n",
    "                    \n",
    "            episode_reward_A = np.max(np.sum(np.array(all_rewards_A), axis=0))\n",
    "            episode_reward_B = np.max(np.sum(np.array(all_rewards_B), axis=0))\n",
    "            \n",
    "            if episode_reward_A > episode_reward_B:\n",
    "                print(f'\\rEpisode: {i_episode}, Reward A: {episode_reward_A:.3f}, Reward B: {episode_reward_B:.3f}, Agent A is win.')\n",
    "            else:\n",
    "                print(f'\\rEpisode: {i_episode}, Reward A: {episode_reward_A:.3f}, Reward B: {episode_reward_B:.3f}, Agent B is win.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "agent = MADDPGAgent(env, num_agents=2, seed=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train The Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_agent(num_episodes=5000, max_time=1000):\n",
    "    \"\"\"Train MADDPG agent.\"\"\"\n",
    "    \n",
    "    all_scores = []\n",
    "    scores_target = 0.5\n",
    "    scores_window = deque(maxlen=PRINT_EVERY)\n",
    "    \n",
    "    for i_episode in range(1, num_episodes+1):\n",
    "        \n",
    "        all_rewards = []\n",
    "        env_info = env.reset(train_mode=True)[brain_name]\n",
    "        states = env_info.vector_observations\n",
    "        \n",
    "        for time_step in range(max_time):\n",
    "            \n",
    "            actions = agent.act(states)\n",
    "            env_info = env.step(actions)[brain_name]\n",
    "            next_states, rewards, dones = env_info.vector_observations, env_info.rewards, env_info.local_done\n",
    "            \n",
    "            agent.step(states, actions, rewards, next_states, dones)\n",
    "            states = next_states\n",
    "            all_rewards.append(rewards)\n",
    "            if any(dones):\n",
    "                break\n",
    "        \n",
    "        # calculate episode reward as maximum of individually collected rewards of agents\n",
    "        episode_reward = np.max(np.sum(np.array(all_rewards), axis=0))\n",
    "        \n",
    "        all_scores.append(episode_reward) # save most recent score to overall score\n",
    "        scores_window.append(episode_reward) # save most recent score to running window of last window scores\n",
    "        \n",
    "        print(f'\\rEpisode: {i_episode}, Average Score: {np.mean(scores_window):.3f}', end='')\n",
    "        \n",
    "        if i_episode % PRINT_EVERY == 0:\n",
    "            print(f'\\rEpisode: {i_episode}, Average Score: {np.mean(scores_window):.3f}')\n",
    "        if np.mean(scores_window) >= scores_target:\n",
    "            print(f'\\nEnvironment solved in {i_episode-100:d} episodes! Average Score: {np.mean(scores_window):.3f}')\n",
    "            break\n",
    "            \n",
    "    agent.save(f'./agents/ACTOR_{brain_name}', f'./agents/CRITIC_{brain_name}')\n",
    "    print('Training completed.')\n",
    "    \n",
    "    return all_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 200, Average Score: 0.007\n",
      "Episode: 400, Average Score: 0.004\n",
      "Episode: 600, Average Score: 0.020\n",
      "Episode: 800, Average Score: 0.014\n",
      "Episode: 1000, Average Score: 0.022\n",
      "Episode: 1200, Average Score: 0.089\n",
      "Episode: 1400, Average Score: 0.074\n",
      "Episode: 1600, Average Score: 0.097\n",
      "Episode: 1800, Average Score: 0.104\n",
      "Episode: 2000, Average Score: 0.103\n",
      "Episode: 2200, Average Score: 0.108\n",
      "Episode: 2400, Average Score: 0.452\n",
      "Episode: 2421, Average Score: 0.503\n",
      "Environment solved in 2321 episodes! Average Score: 0.503\n",
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "all_scores = train_agent(num_episodes=5000, max_time=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate The Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmgAAAFDCAYAAACObPIhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYxp6AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12a488d30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(np.arange(len(all_scores)), all_scores, color='green')\n",
    "plt.xlabel('Num of episodes')\n",
    "plt.ylabel('Score')\n",
    "if not os.path.exists('./images/'): os.makedirs('./images/')\n",
    "plt.savefig('./images/plot_of_maddpg_evaluation.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¬ Watch The Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "agent.load(f'./agents/ACTOR_{brain_name}');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1, Reward A: 0.790, Reward B: 0.700, Agent A is win.\n",
      "Episode: 2, Reward A: 1.390, Reward B: 1.400, Agent B is win.\n",
      "Episode: 3, Reward A: 0.100, Reward B: -0.010, Agent A is win.\n",
      "Episode: 4, Reward A: 1.000, Reward B: 0.990, Agent A is win.\n",
      "Episode: 5, Reward A: 2.600, Reward B: 2.600, Agent B is win.\n",
      "Episode: 6, Reward A: 1.200, Reward B: 1.090, Agent A is win.\n",
      "Episode: 7, Reward A: 0.490, Reward B: 0.600, Agent B is win.\n",
      "Episode: 8, Reward A: 0.200, Reward B: 0.190, Agent A is win.\n",
      "Episode: 9, Reward A: 0.390, Reward B: 0.300, Agent A is win.\n",
      "Episode: 10, Reward A: 2.600, Reward B: 2.500, Agent A is win.\n"
     ]
    }
   ],
   "source": [
    "agent.watch(num_episodes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
