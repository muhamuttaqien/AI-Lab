{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from time import sleep\n",
    "from IPython.display import clear_output\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "from unityagents import UnityEnvironment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from torch.utils.data.sampler import BatchSampler, SubsetRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from buffer import ReplayBuffer\n",
    "from model import ActorNetwork, CriticNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore', UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "if is_cuda: device = torch.device('cuda')\n",
    "else: device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "GAMMA = 0.995\n",
    "ENTROPY_WEIGHT = 0.001\n",
    "EPS_CLIP = 1e-1\n",
    "GRAD_CLIP = 5e-1\n",
    "DEQUE_SIZE = 100\n",
    "\n",
    "GOALIE_LR = 8e-5\n",
    "STRIKER_LR = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TRAINED_AGENT_KEY = 0\n",
    "RANDOM_AGENT_KEY = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 2\n",
      "        Number of External Brains : 2\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: GoalieBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 112\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n",
      "Unity brain name: StrikerBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 112\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 6\n",
      "        Vector Action descriptions: , , , , , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name='./Soccer.app')\n",
    "\n",
    "env_info = env.reset(train_mode=True)\n",
    "\n",
    "goalie_brain_name = env.brain_names[0]\n",
    "goalie_state_size = env_info[goalie_brain_name].vector_observations.shape[1]\n",
    "goalie_action_size = env.brains[goalie_brain_name].vector_action_space_size\n",
    "\n",
    "striker_brain_name = env.brain_names[1]\n",
    "striker_state_size = env_info[striker_brain_name].vector_observations.shape[1]\n",
    "striker_action_size = env.brains[striker_brain_name].vector_action_space_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define [A2C](https://arxiv.org/pdf/1602.01783.pdf) Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PPOAgent():\n",
    "    \"\"\"The Agent that will interacts with and learns from the environment.\"\"\"\n",
    "    \n",
    "    def __init__(self, env, mode='Goalie', seed=90):\n",
    "        \"\"\"Initialize an Agent object.\"\"\"\n",
    "        \n",
    "        self.env_info = env.reset(train_mode=True)\n",
    "                \n",
    "        if mode == 'Goalie': brain_name = env.brain_names[0] # brain name for Goalie\n",
    "        elif mode == 'Striker': brain_name = env.brain_names[1] # brain name for Striker\n",
    "            \n",
    "        self.state_size = self.env_info[brain_name].vector_observations.shape[1]\n",
    "        self.action_size = env.brains[brain_name].vector_action_space_size\n",
    "        \n",
    "        self.batch_size = BATCH_SIZE\n",
    "        self.gamma = GAMMA\n",
    "        self.entropy_weight = ENTROPY_WEIGHT\n",
    "        self.eps_clip = EPS_CLIP       \n",
    "        self.grad_clip = GRAD_CLIP\n",
    "        \n",
    "        if mode == 'Goalie': self.lr = GOALIE_LR\n",
    "        elif mode == 'Striker':  self.lr = STRIKER_LR\n",
    "            \n",
    "        self.actor = ActorNetwork(self.state_size, self.action_size, seed).to(device)\n",
    "        self.critic = CriticNetwork(2 * len(env_info[goalie_brain_name].agents) * self.state_size, seed).to(device)\n",
    "        self.optimizer = torch.optim.Adam(list(self.actor.parameters()) + list(self.critic.parameters()), lr=self.lr)\n",
    "        \n",
    "        self.buffer = ReplayBuffer(self.batch_size, seed)\n",
    "        \n",
    "    def act(self, states):\n",
    "        \n",
    "        states = torch.FloatTensor(states).unsqueeze(0).to(device)\n",
    "        \n",
    "        self.actor.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            actions, action_log_probs, _ = self.actor(states)\n",
    "            \n",
    "        actions = actions.cpu().detach().numpy().item()\n",
    "        action_log_probs = action_log_probs.cpu().detach().numpy().item()\n",
    "        \n",
    "        self.actor.train()\n",
    "        \n",
    "        return actions, action_log_probs\n",
    "    \n",
    "    def memorize(self, actor_state, critic_state, action, log_prob, reward):\n",
    "        \n",
    "        self.buffer.add(actor_state, critic_state, action, log_prob, reward)\n",
    "    \n",
    "    def learn(self):\n",
    "        \n",
    "        actor_states, critic_states, actions, old_log_probs, rewards, num_experiences = self.buffer.get_experiences()\n",
    "        \n",
    "        discount = self.gamma**np.arange(num_experiences)\n",
    "        rewards = rewards.squeeze(1) * discount\n",
    "        next_rewards = rewards[::-1].cumsum(axis=0)[::-1]\n",
    "        \n",
    "        actor_states = torch.FloatTensor(actor_states).to(device)\n",
    "        critic_states = torch.FloatTensor(critic_states).to(device)\n",
    "        actions = torch.LongTensor(actions).squeeze(1).to(device)\n",
    "        old_log_probs = torch.FloatTensor(old_log_probs).squeeze(1).to(device)\n",
    "        rewards = torch.FloatTensor(next_rewards.copy()).to(device)\n",
    "\n",
    "        self.critic.eval()\n",
    "        with torch.no_grad():\n",
    "            state_values = self.critic(critic_states)\n",
    "        \n",
    "        self.critic.train()\n",
    "        \n",
    "        advantages = (rewards - state_values.detach().squeeze())\n",
    "        advantages = advantages.detach()\n",
    "        advantages_normalized = (advantages - advantages.mean()) / (advantages.std() + 1e-10)\n",
    "        advantages_normalized = torch.FloatTensor(advantages_normalized).to(device)\n",
    "        \n",
    "        batches = BatchSampler(SubsetRandomSampler(range(0, num_experiences)), self.batch_size, drop_last=False)\n",
    "        losses = []\n",
    "        \n",
    "        for batch_indices in batches:\n",
    "            \n",
    "            batch_indices = torch.LongTensor(batch_indices).to(device)\n",
    "            \n",
    "            sampled_actor_states = actor_states[batch_indices]\n",
    "            sampled_critic_states = critic_states[batch_indices]\n",
    "            sampled_actions = actions[batch_indices]\n",
    "            sampled_old_log_probs = old_log_probs[batch_indices]\n",
    "            sampled_rewards = rewards[batch_indices]\n",
    "            sampled_advantages_normalized = advantages_normalized[batch_indices]\n",
    "        \n",
    "            _, log_probs, dist_entropies = self.actor(sampled_actor_states, sampled_actions)\n",
    "            state_values = self.critic(sampled_critic_states)\n",
    "            state_values = state_values.squeeze()\n",
    "            \n",
    "            ratios = torch.exp(log_probs - sampled_old_log_probs.detach())\n",
    "            \n",
    "            surrogate1 = ratios * sampled_advantages_normalized\n",
    "            surrogate2 = torch.clamp(ratios, 1-self.eps_clip, 1+self.eps_clip) * sampled_advantages_normalized\n",
    "                \n",
    "            policy_loss = -torch.min(surrogate1, surrogate2).mean() - self.entropy_weight * dist_entropies.mean()\n",
    "            value_loss = 0.5 * (sampled_rewards - state_values).pow(2).mean()\n",
    "            \n",
    "            total_loss = policy_loss + value_loss\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "            nn.utils.clip_grad_norm_(self.actor.parameters(), self.grad_clip)\n",
    "            nn.utils.clip_grad_norm_(self.critic.parameters(), self.grad_clip)\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            losses.append(total_loss.item())\n",
    "            \n",
    "        self.buffer.reset()\n",
    "            \n",
    "        self.eps_clip *= 1.\n",
    "        self.entropy_weight *= 0.995\n",
    "        \n",
    "        return np.average(losses)\n",
    "    \n",
    "    def save(self, actor_path, critic_path):\n",
    "        \n",
    "        if not os.path.exists('./agents/'): os.makedirs('./agents/')\n",
    "        torch.save(self.actor.state_dict(), actor_path); torch.save(self.critic.state_dict(), critic_path);\n",
    "        \n",
    "    def load(self, actor_path, critic_path):\n",
    "        \n",
    "        self.actor.load_state_dict(torch.load(actor_path)); self.critic.load_state_dict(torch.load(critic_path));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "goalie = PPOAgent(env, mode='Goalie', seed=90); striker = PPOAgent(env, mode='Striker', seed=90);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¬ Watch The Smart Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def watch_agent(goalie, striker, num_episodes=10):\n",
    "        \n",
    "    trained_agent_scores_window = deque(maxlen=DEQUE_SIZE)\n",
    "    trained_agent_scores_window_wins = deque(maxlen=DEQUE_SIZE)\n",
    "\n",
    "    random_agent_scores_window = deque(maxlen=DEQUE_SIZE)\n",
    "    random_agent_scores_window_wins = deque(maxlen=DEQUE_SIZE)\n",
    "\n",
    "    draws = deque(maxlen=DEQUE_SIZE)\n",
    "\n",
    "    # training loop\n",
    "    for i_episode in range(1, num_episodes+1):\n",
    "\n",
    "        env_info = env.reset(train_mode=True)\n",
    "\n",
    "        goalie_states = env_info[goalie_brain_name].vector_observations\n",
    "        striker_states = env_info[striker_brain_name].vector_observations\n",
    "\n",
    "        goalie_scores = np.zeros(len(env_info[goalie_brain_name].agents))\n",
    "        striker_scores = np.zeros(len(env_info[striker_brain_name].agents))\n",
    "\n",
    "        while True:\n",
    "\n",
    "            trained_goalie_action, trained_goalie_log_prob = goalie.act(goalie_states[TRAINED_AGENT_KEY])\n",
    "            trained_striker_action, trained_striker_log_prob = striker.act(striker_states[TRAINED_AGENT_KEY])\n",
    "\n",
    "            random_goalie_action = np.asarray([np.random.choice(goalie_action_size)])\n",
    "            random_striker_action = np.asarray([np.random.choice(striker_action_size)])\n",
    "\n",
    "            goalie_actions = np.array((trained_goalie_action, random_goalie_action))\n",
    "            striker_actions = np.array((trained_striker_action, random_striker_action))\n",
    "\n",
    "            actions = dict(zip([goalie_brain_name, striker_brain_name], [goalie_actions, striker_actions]))\n",
    "\n",
    "            env_info = env.step(actions)\n",
    "\n",
    "            # agents get next states\n",
    "            goalie_next_states = env_info[goalie_brain_name].vector_observations\n",
    "            striker_next_states = env_info[striker_brain_name].vector_observations\n",
    "\n",
    "            # agents get rewards\n",
    "            goalie_rewards = env_info[goalie_brain_name].rewards\n",
    "            striker_rewards = env_info[striker_brain_name].rewards\n",
    "\n",
    "            # agents update scores\n",
    "            goalie_scores += goalie_rewards\n",
    "            striker_scores += striker_rewards\n",
    "\n",
    "            # agents roll over states to next states\n",
    "            goalie_states = goalie_next_states\n",
    "            striker_states = striker_next_states\n",
    "\n",
    "            # check if episode finished\n",
    "            done = np.any(env_info[goalie_brain_name].local_done)\n",
    "            if done: break\n",
    "\n",
    "        # agents record scores\n",
    "        trained_agent_score = goalie_scores[TRAINED_AGENT_KEY] + striker_scores[TRAINED_AGENT_KEY]\n",
    "        trained_agent_scores_window.append(trained_agent_score)\n",
    "        trained_agent_scores_window_wins.append(1 if trained_agent_score > 0 else 0)\n",
    "\n",
    "        random_agent_score = goalie_scores[RANDOM_AGENT_KEY] + striker_scores[RANDOM_AGENT_KEY]\n",
    "        random_agent_scores_window.append(random_agent_score)\n",
    "        random_agent_scores_window_wins.append(1 if random_agent_score > 0 else 0)\n",
    "\n",
    "        draws.append(trained_agent_score == random_agent_score)\n",
    "\n",
    "        print(f'\\rEpisode: {i_episode}')\n",
    "        print(f'Red Wins: {np.count_nonzero(trained_agent_scores_window_wins)}, Score: {trained_agent_score:.4f}, Average Score: {np.sum(trained_agent_scores_window):.4f}')\n",
    "        print(f'Blue Wins: {np.count_nonzero(random_agent_scores_window_wins)}, Score: {random_agent_score:.4f}, Average Score: {np.sum(random_agent_scores_window):.4f}')\n",
    "        print(f'Draws: {np.count_nonzero(draws)}\\n')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# goalie.load(f'./agents/Actor_{goalie_brain_name}_episode100.pth', f'./agents/Critic_{goalie_brain_name}_episode100.pth');\n",
    "# striker.load(f'./agents/Actor_{striker_brain_name}_episode100.pth', f'./agents/Critic_{striker_brain_name}_episode100.pth');\n",
    "\n",
    "goalie.load(f'./agents/Actor_{goalie_brain_name}.pth', f'./agents/Critic_{goalie_brain_name}.pth');\n",
    "striker.load(f'./agents/Actor_{striker_brain_name}.pth', f'./agents/Critic_{striker_brain_name}.pth');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1\n",
      "Red Wins: 1, Score: 1.1000, Average Score: 1.1000\n",
      "Blue Wins: 0, Score: -1.1000, Average Score: -1.1000\n",
      "Draws: 0\n",
      "\n",
      "Episode: 2\n",
      "Red Wins: 1, Score: -1.1000, Average Score: 0.0000\n",
      "Blue Wins: 1, Score: 1.1000, Average Score: -0.0000\n",
      "Draws: 0\n",
      "\n",
      "Episode: 3\n",
      "Red Wins: 2, Score: 1.1000, Average Score: 1.1000\n",
      "Blue Wins: 1, Score: -1.1000, Average Score: -1.1000\n",
      "Draws: 0\n",
      "\n",
      "Episode: 4\n",
      "Red Wins: 2, Score: -1.1000, Average Score: 0.0000\n",
      "Blue Wins: 2, Score: 1.1000, Average Score: -0.0000\n",
      "Draws: 0\n",
      "\n",
      "Episode: 5\n",
      "Red Wins: 3, Score: 1.1000, Average Score: 1.1000\n",
      "Blue Wins: 2, Score: -1.1000, Average Score: -1.1000\n",
      "Draws: 0\n",
      "\n",
      "Episode: 6\n",
      "Red Wins: 3, Score: -1.1000, Average Score: 0.0000\n",
      "Blue Wins: 3, Score: 1.1000, Average Score: -0.0000\n",
      "Draws: 0\n",
      "\n",
      "Episode: 7\n",
      "Red Wins: 4, Score: 1.1000, Average Score: 1.1000\n",
      "Blue Wins: 3, Score: -1.1000, Average Score: -1.1000\n",
      "Draws: 0\n",
      "\n",
      "Episode: 8\n",
      "Red Wins: 5, Score: 1.1000, Average Score: 2.2000\n",
      "Blue Wins: 3, Score: -1.1000, Average Score: -2.2000\n",
      "Draws: 0\n",
      "\n",
      "Episode: 9\n",
      "Red Wins: 6, Score: 1.1000, Average Score: 3.3000\n",
      "Blue Wins: 3, Score: -1.1000, Average Score: -3.3000\n",
      "Draws: 0\n",
      "\n",
      "Episode: 10\n",
      "Red Wins: 7, Score: 1.1000, Average Score: 4.4000\n",
      "Blue Wins: 3, Score: -1.1000, Average Score: -4.4000\n",
      "Draws: 0\n",
      "\n",
      "Episode: 11\n",
      "Red Wins: 8, Score: 1.1000, Average Score: 5.5000\n",
      "Blue Wins: 3, Score: -1.1000, Average Score: -5.5000\n",
      "Draws: 0\n",
      "\n",
      "Episode: 12\n",
      "Red Wins: 8, Score: -1.1000, Average Score: 4.4000\n",
      "Blue Wins: 4, Score: 1.1000, Average Score: -4.4000\n",
      "Draws: 0\n",
      "\n",
      "Episode: 13\n",
      "Red Wins: 9, Score: 1.1000, Average Score: 5.5000\n",
      "Blue Wins: 4, Score: -1.1000, Average Score: -5.5000\n",
      "Draws: 0\n",
      "\n",
      "Episode: 14\n",
      "Red Wins: 9, Score: 0.0000, Average Score: 5.5000\n",
      "Blue Wins: 4, Score: 0.0000, Average Score: -5.5000\n",
      "Draws: 1\n",
      "\n",
      "Episode: 15\n",
      "Red Wins: 10, Score: 1.1000, Average Score: 6.6000\n",
      "Blue Wins: 4, Score: -1.1000, Average Score: -6.6000\n",
      "Draws: 1\n",
      "\n",
      "Episode: 16\n",
      "Red Wins: 11, Score: 1.1000, Average Score: 7.7000\n",
      "Blue Wins: 4, Score: -1.1000, Average Score: -7.7000\n",
      "Draws: 1\n",
      "\n",
      "Episode: 17\n",
      "Red Wins: 11, Score: 0.0000, Average Score: 7.7000\n",
      "Blue Wins: 4, Score: 0.0000, Average Score: -7.7000\n",
      "Draws: 2\n",
      "\n",
      "Episode: 18\n",
      "Red Wins: 12, Score: 1.1000, Average Score: 8.8000\n",
      "Blue Wins: 4, Score: -1.1000, Average Score: -8.8000\n",
      "Draws: 2\n",
      "\n",
      "Episode: 19\n",
      "Red Wins: 13, Score: 1.1000, Average Score: 9.9000\n",
      "Blue Wins: 4, Score: -1.1000, Average Score: -9.9000\n",
      "Draws: 2\n",
      "\n",
      "Episode: 20\n",
      "Red Wins: 14, Score: 1.1000, Average Score: 11.0000\n",
      "Blue Wins: 4, Score: -1.1000, Average Score: -11.0000\n",
      "Draws: 2\n",
      "\n",
      "Episode: 21\n",
      "Red Wins: 15, Score: 1.1000, Average Score: 12.1000\n",
      "Blue Wins: 4, Score: -1.1000, Average Score: -12.1000\n",
      "Draws: 2\n",
      "\n",
      "Episode: 22\n",
      "Red Wins: 15, Score: 0.0000, Average Score: 12.1000\n",
      "Blue Wins: 4, Score: 0.0000, Average Score: -12.1000\n",
      "Draws: 3\n",
      "\n",
      "Episode: 23\n",
      "Red Wins: 16, Score: 1.1000, Average Score: 13.2000\n",
      "Blue Wins: 4, Score: -1.1000, Average Score: -13.2000\n",
      "Draws: 3\n",
      "\n",
      "Episode: 24\n",
      "Red Wins: 17, Score: 1.1000, Average Score: 14.3000\n",
      "Blue Wins: 4, Score: -1.1000, Average Score: -14.3000\n",
      "Draws: 3\n",
      "\n",
      "Episode: 25\n",
      "Red Wins: 18, Score: 1.1000, Average Score: 15.4000\n",
      "Blue Wins: 4, Score: -1.1000, Average Score: -15.4000\n",
      "Draws: 3\n",
      "\n",
      "Episode: 26\n",
      "Red Wins: 18, Score: -1.1000, Average Score: 14.3000\n",
      "Blue Wins: 5, Score: 1.1000, Average Score: -14.3000\n",
      "Draws: 3\n",
      "\n",
      "Episode: 27\n",
      "Red Wins: 18, Score: 0.0000, Average Score: 14.3000\n",
      "Blue Wins: 5, Score: 0.0000, Average Score: -14.3000\n",
      "Draws: 4\n",
      "\n",
      "Episode: 28\n",
      "Red Wins: 19, Score: 1.1000, Average Score: 15.4000\n",
      "Blue Wins: 5, Score: -1.1000, Average Score: -15.4000\n",
      "Draws: 4\n",
      "\n",
      "Episode: 29\n",
      "Red Wins: 20, Score: 1.1000, Average Score: 16.5000\n",
      "Blue Wins: 5, Score: -1.1000, Average Score: -16.5000\n",
      "Draws: 4\n",
      "\n",
      "Episode: 30\n",
      "Red Wins: 20, Score: -1.1000, Average Score: 15.4000\n",
      "Blue Wins: 6, Score: 1.1000, Average Score: -15.4000\n",
      "Draws: 4\n",
      "\n",
      "Episode: 31\n",
      "Red Wins: 21, Score: 1.1000, Average Score: 16.5000\n",
      "Blue Wins: 6, Score: -1.1000, Average Score: -16.5000\n",
      "Draws: 4\n",
      "\n",
      "Episode: 32\n",
      "Red Wins: 22, Score: 1.1000, Average Score: 17.6000\n",
      "Blue Wins: 6, Score: -1.1000, Average Score: -17.6000\n",
      "Draws: 4\n",
      "\n",
      "Episode: 33\n",
      "Red Wins: 22, Score: 0.0000, Average Score: 17.6000\n",
      "Blue Wins: 6, Score: 0.0000, Average Score: -17.6000\n",
      "Draws: 5\n",
      "\n",
      "Episode: 34\n",
      "Red Wins: 23, Score: 1.1000, Average Score: 18.7000\n",
      "Blue Wins: 6, Score: -1.1000, Average Score: -18.7000\n",
      "Draws: 5\n",
      "\n",
      "Episode: 35\n",
      "Red Wins: 24, Score: 1.1000, Average Score: 19.8000\n",
      "Blue Wins: 6, Score: -1.1000, Average Score: -19.8000\n",
      "Draws: 5\n",
      "\n",
      "Episode: 36\n",
      "Red Wins: 25, Score: 1.1000, Average Score: 20.9000\n",
      "Blue Wins: 6, Score: -1.1000, Average Score: -20.9000\n",
      "Draws: 5\n",
      "\n",
      "Episode: 37\n",
      "Red Wins: 26, Score: 1.1000, Average Score: 22.0000\n",
      "Blue Wins: 6, Score: -1.1000, Average Score: -22.0000\n",
      "Draws: 5\n",
      "\n",
      "Episode: 38\n",
      "Red Wins: 26, Score: 0.0000, Average Score: 22.0000\n",
      "Blue Wins: 6, Score: 0.0000, Average Score: -22.0000\n",
      "Draws: 6\n",
      "\n",
      "Episode: 39\n",
      "Red Wins: 27, Score: 1.1000, Average Score: 23.1000\n",
      "Blue Wins: 6, Score: -1.1000, Average Score: -23.1000\n",
      "Draws: 6\n",
      "\n",
      "Episode: 40\n",
      "Red Wins: 28, Score: 1.1000, Average Score: 24.2000\n",
      "Blue Wins: 6, Score: -1.1000, Average Score: -24.2000\n",
      "Draws: 6\n",
      "\n",
      "Episode: 41\n",
      "Red Wins: 28, Score: 0.0000, Average Score: 24.2000\n",
      "Blue Wins: 6, Score: 0.0000, Average Score: -24.2000\n",
      "Draws: 7\n",
      "\n",
      "Episode: 42\n",
      "Red Wins: 28, Score: 0.0000, Average Score: 24.2000\n",
      "Blue Wins: 6, Score: 0.0000, Average Score: -24.2000\n",
      "Draws: 8\n",
      "\n",
      "Episode: 43\n",
      "Red Wins: 29, Score: 1.1000, Average Score: 25.3000\n",
      "Blue Wins: 6, Score: -1.1000, Average Score: -25.3000\n",
      "Draws: 8\n",
      "\n",
      "Episode: 44\n",
      "Red Wins: 30, Score: 1.1000, Average Score: 26.4000\n",
      "Blue Wins: 6, Score: -1.1000, Average Score: -26.4000\n",
      "Draws: 8\n",
      "\n",
      "Episode: 45\n",
      "Red Wins: 31, Score: 1.1000, Average Score: 27.5000\n",
      "Blue Wins: 6, Score: -1.1000, Average Score: -27.5000\n",
      "Draws: 8\n",
      "\n",
      "Episode: 46\n",
      "Red Wins: 32, Score: 1.1000, Average Score: 28.6000\n",
      "Blue Wins: 6, Score: -1.1000, Average Score: -28.6000\n",
      "Draws: 8\n",
      "\n",
      "Episode: 47\n",
      "Red Wins: 32, Score: -1.1000, Average Score: 27.5000\n",
      "Blue Wins: 7, Score: 1.1000, Average Score: -27.5000\n",
      "Draws: 8\n",
      "\n",
      "Episode: 48\n",
      "Red Wins: 33, Score: 1.1000, Average Score: 28.6000\n",
      "Blue Wins: 7, Score: -1.1000, Average Score: -28.6000\n",
      "Draws: 8\n",
      "\n",
      "Episode: 49\n",
      "Red Wins: 34, Score: 1.1000, Average Score: 29.7000\n",
      "Blue Wins: 7, Score: -1.1000, Average Score: -29.7000\n",
      "Draws: 8\n",
      "\n",
      "Episode: 50\n",
      "Red Wins: 34, Score: 0.0000, Average Score: 29.7000\n",
      "Blue Wins: 7, Score: 0.0000, Average Score: -29.7000\n",
      "Draws: 9\n",
      "\n",
      "Episode: 51\n",
      "Red Wins: 34, Score: -1.1000, Average Score: 28.6000\n",
      "Blue Wins: 8, Score: 1.1000, Average Score: -28.6000\n",
      "Draws: 9\n",
      "\n",
      "Episode: 52\n",
      "Red Wins: 35, Score: 1.1000, Average Score: 29.7000\n",
      "Blue Wins: 8, Score: -1.1000, Average Score: -29.7000\n",
      "Draws: 9\n",
      "\n",
      "Episode: 53\n",
      "Red Wins: 35, Score: -1.1000, Average Score: 28.6000\n",
      "Blue Wins: 9, Score: 1.1000, Average Score: -28.6000\n",
      "Draws: 9\n",
      "\n",
      "Episode: 54\n",
      "Red Wins: 36, Score: 1.1000, Average Score: 29.7000\n",
      "Blue Wins: 9, Score: -1.1000, Average Score: -29.7000\n",
      "Draws: 9\n",
      "\n",
      "Episode: 55\n",
      "Red Wins: 37, Score: 1.1000, Average Score: 30.8000\n",
      "Blue Wins: 9, Score: -1.1000, Average Score: -30.8000\n",
      "Draws: 9\n",
      "\n",
      "Episode: 56\n",
      "Red Wins: 37, Score: 0.0000, Average Score: 30.8000\n",
      "Blue Wins: 9, Score: 0.0000, Average Score: -30.8000\n",
      "Draws: 10\n",
      "\n",
      "Episode: 57\n",
      "Red Wins: 38, Score: 1.1000, Average Score: 31.9000\n",
      "Blue Wins: 9, Score: -1.1000, Average Score: -31.9000\n",
      "Draws: 10\n",
      "\n",
      "Episode: 58\n",
      "Red Wins: 39, Score: 1.1000, Average Score: 33.0000\n",
      "Blue Wins: 9, Score: -1.1000, Average Score: -33.0000\n",
      "Draws: 10\n",
      "\n",
      "Episode: 59\n",
      "Red Wins: 40, Score: 1.1000, Average Score: 34.1000\n",
      "Blue Wins: 9, Score: -1.1000, Average Score: -34.1000\n",
      "Draws: 10\n",
      "\n",
      "Episode: 60\n",
      "Red Wins: 40, Score: -1.1000, Average Score: 33.0000\n",
      "Blue Wins: 10, Score: 1.1000, Average Score: -33.0000\n",
      "Draws: 10\n",
      "\n",
      "Episode: 61\n",
      "Red Wins: 41, Score: 1.1000, Average Score: 34.1000\n",
      "Blue Wins: 10, Score: -1.1000, Average Score: -34.1000\n",
      "Draws: 10\n",
      "\n",
      "Episode: 62\n",
      "Red Wins: 42, Score: 1.1000, Average Score: 35.2000\n",
      "Blue Wins: 10, Score: -1.1000, Average Score: -35.2000\n",
      "Draws: 10\n",
      "\n",
      "Episode: 63\n",
      "Red Wins: 42, Score: -1.1000, Average Score: 34.1000\n",
      "Blue Wins: 11, Score: 1.1000, Average Score: -34.1000\n",
      "Draws: 10\n",
      "\n",
      "Episode: 64\n",
      "Red Wins: 42, Score: -1.1000, Average Score: 33.0000\n",
      "Blue Wins: 12, Score: 1.1000, Average Score: -33.0000\n",
      "Draws: 10\n",
      "\n",
      "Episode: 65\n",
      "Red Wins: 43, Score: 1.1000, Average Score: 34.1000\n",
      "Blue Wins: 12, Score: -1.1000, Average Score: -34.1000\n",
      "Draws: 10\n",
      "\n",
      "Episode: 66\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Red Wins: 44, Score: 1.1000, Average Score: 35.2000\n",
      "Blue Wins: 12, Score: -1.1000, Average Score: -35.2000\n",
      "Draws: 10\n",
      "\n",
      "Episode: 67\n",
      "Red Wins: 45, Score: 1.1000, Average Score: 36.3000\n",
      "Blue Wins: 12, Score: -1.1000, Average Score: -36.3000\n",
      "Draws: 10\n",
      "\n",
      "Episode: 68\n",
      "Red Wins: 46, Score: 1.1000, Average Score: 37.4000\n",
      "Blue Wins: 12, Score: -1.1000, Average Score: -37.4000\n",
      "Draws: 10\n",
      "\n",
      "Episode: 69\n",
      "Red Wins: 47, Score: 1.1000, Average Score: 38.5000\n",
      "Blue Wins: 12, Score: -1.1000, Average Score: -38.5000\n",
      "Draws: 10\n",
      "\n",
      "Episode: 70\n",
      "Red Wins: 47, Score: 0.0000, Average Score: 38.5000\n",
      "Blue Wins: 12, Score: 0.0000, Average Score: -38.5000\n",
      "Draws: 11\n",
      "\n",
      "Episode: 71\n",
      "Red Wins: 48, Score: 1.1000, Average Score: 39.6000\n",
      "Blue Wins: 12, Score: -1.1000, Average Score: -39.6000\n",
      "Draws: 11\n",
      "\n",
      "Episode: 72\n",
      "Red Wins: 49, Score: 1.1000, Average Score: 40.7000\n",
      "Blue Wins: 12, Score: -1.1000, Average Score: -40.7000\n",
      "Draws: 11\n",
      "\n",
      "Episode: 73\n",
      "Red Wins: 49, Score: -1.1000, Average Score: 39.6000\n",
      "Blue Wins: 13, Score: 1.1000, Average Score: -39.6000\n",
      "Draws: 11\n",
      "\n",
      "Episode: 74\n",
      "Red Wins: 49, Score: 0.0000, Average Score: 39.6000\n",
      "Blue Wins: 13, Score: 0.0000, Average Score: -39.6000\n",
      "Draws: 12\n",
      "\n",
      "Episode: 75\n",
      "Red Wins: 49, Score: 0.0000, Average Score: 39.6000\n",
      "Blue Wins: 13, Score: 0.0000, Average Score: -39.6000\n",
      "Draws: 13\n",
      "\n",
      "Episode: 76\n",
      "Red Wins: 50, Score: 1.1000, Average Score: 40.7000\n",
      "Blue Wins: 13, Score: -1.1000, Average Score: -40.7000\n",
      "Draws: 13\n",
      "\n",
      "Episode: 77\n",
      "Red Wins: 50, Score: 0.0000, Average Score: 40.7000\n",
      "Blue Wins: 13, Score: 0.0000, Average Score: -40.7000\n",
      "Draws: 14\n",
      "\n",
      "Episode: 78\n",
      "Red Wins: 51, Score: 1.1000, Average Score: 41.8000\n",
      "Blue Wins: 13, Score: -1.1000, Average Score: -41.8000\n",
      "Draws: 14\n",
      "\n",
      "Episode: 79\n",
      "Red Wins: 51, Score: 0.0000, Average Score: 41.8000\n",
      "Blue Wins: 13, Score: 0.0000, Average Score: -41.8000\n",
      "Draws: 15\n",
      "\n",
      "Episode: 80\n",
      "Red Wins: 51, Score: 0.0000, Average Score: 41.8000\n",
      "Blue Wins: 13, Score: 0.0000, Average Score: -41.8000\n",
      "Draws: 16\n",
      "\n",
      "Episode: 81\n",
      "Red Wins: 51, Score: 0.0000, Average Score: 41.8000\n",
      "Blue Wins: 13, Score: 0.0000, Average Score: -41.8000\n",
      "Draws: 17\n",
      "\n",
      "Episode: 82\n",
      "Red Wins: 52, Score: 1.1000, Average Score: 42.9000\n",
      "Blue Wins: 13, Score: -1.1000, Average Score: -42.9000\n",
      "Draws: 17\n",
      "\n",
      "Episode: 83\n",
      "Red Wins: 53, Score: 1.1000, Average Score: 44.0000\n",
      "Blue Wins: 13, Score: -1.1000, Average Score: -44.0000\n",
      "Draws: 17\n",
      "\n",
      "Episode: 84\n",
      "Red Wins: 54, Score: 1.1000, Average Score: 45.1000\n",
      "Blue Wins: 13, Score: -1.1000, Average Score: -45.1000\n",
      "Draws: 17\n",
      "\n",
      "Episode: 85\n",
      "Red Wins: 54, Score: 0.0000, Average Score: 45.1000\n",
      "Blue Wins: 13, Score: 0.0000, Average Score: -45.1000\n",
      "Draws: 18\n",
      "\n",
      "Episode: 86\n",
      "Red Wins: 55, Score: 1.1000, Average Score: 46.2000\n",
      "Blue Wins: 13, Score: -1.1000, Average Score: -46.2000\n",
      "Draws: 18\n",
      "\n",
      "Episode: 87\n",
      "Red Wins: 56, Score: 1.1000, Average Score: 47.3000\n",
      "Blue Wins: 13, Score: -1.1000, Average Score: -47.3000\n",
      "Draws: 18\n",
      "\n",
      "Episode: 88\n",
      "Red Wins: 56, Score: 0.0000, Average Score: 47.3000\n",
      "Blue Wins: 13, Score: 0.0000, Average Score: -47.3000\n",
      "Draws: 19\n",
      "\n",
      "Episode: 89\n",
      "Red Wins: 57, Score: 1.1000, Average Score: 48.4000\n",
      "Blue Wins: 13, Score: -1.1000, Average Score: -48.4000\n",
      "Draws: 19\n",
      "\n",
      "Episode: 90\n",
      "Red Wins: 58, Score: 1.1000, Average Score: 49.5000\n",
      "Blue Wins: 13, Score: -1.1000, Average Score: -49.5000\n",
      "Draws: 19\n",
      "\n",
      "Episode: 91\n",
      "Red Wins: 59, Score: 1.1000, Average Score: 50.6000\n",
      "Blue Wins: 13, Score: -1.1000, Average Score: -50.6000\n",
      "Draws: 19\n",
      "\n",
      "Episode: 92\n",
      "Red Wins: 60, Score: 1.1000, Average Score: 51.7000\n",
      "Blue Wins: 13, Score: -1.1000, Average Score: -51.7000\n",
      "Draws: 19\n",
      "\n",
      "Episode: 93\n",
      "Red Wins: 61, Score: 1.1000, Average Score: 52.8000\n",
      "Blue Wins: 13, Score: -1.1000, Average Score: -52.8000\n",
      "Draws: 19\n",
      "\n",
      "Episode: 94\n",
      "Red Wins: 62, Score: 1.1000, Average Score: 53.9000\n",
      "Blue Wins: 13, Score: -1.1000, Average Score: -53.9000\n",
      "Draws: 19\n",
      "\n",
      "Episode: 95\n",
      "Red Wins: 63, Score: 1.1000, Average Score: 55.0000\n",
      "Blue Wins: 13, Score: -1.1000, Average Score: -55.0000\n",
      "Draws: 19\n",
      "\n",
      "Episode: 96\n",
      "Red Wins: 64, Score: 1.1000, Average Score: 56.1000\n",
      "Blue Wins: 13, Score: -1.1000, Average Score: -56.1000\n",
      "Draws: 19\n",
      "\n",
      "Episode: 97\n",
      "Red Wins: 65, Score: 1.1000, Average Score: 57.2000\n",
      "Blue Wins: 13, Score: -1.1000, Average Score: -57.2000\n",
      "Draws: 19\n",
      "\n",
      "Episode: 98\n",
      "Red Wins: 65, Score: 0.0000, Average Score: 57.2000\n",
      "Blue Wins: 13, Score: 0.0000, Average Score: -57.2000\n",
      "Draws: 20\n",
      "\n",
      "Episode: 99\n",
      "Red Wins: 66, Score: 1.1000, Average Score: 58.3000\n",
      "Blue Wins: 13, Score: -1.1000, Average Score: -58.3000\n",
      "Draws: 20\n",
      "\n",
      "Episode: 100\n",
      "Red Wins: 67, Score: 1.1000, Average Score: 59.4000\n",
      "Blue Wins: 13, Score: -1.1000, Average Score: -59.4000\n",
      "Draws: 20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "watch_agent(goalie, striker, num_episodes=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
