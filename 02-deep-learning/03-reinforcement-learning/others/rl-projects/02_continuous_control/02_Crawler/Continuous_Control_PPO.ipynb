{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from time import sleep\n",
    "from IPython.display import clear_output\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "from unityagents import UnityEnvironment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from buffer import ReplayBuffer\n",
    "from model import PolicyNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore', UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "if is_cuda: device = torch.device('cuda')\n",
    "else: device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1024\n",
    "AGENT_BATCH_SIZE = 32\n",
    "GAMMA = 0.99\n",
    "TAU = 0.99\n",
    "PRINT_EVERY = 10\n",
    "\n",
    "LR = 1e-4\n",
    "EPSILON = 1e-5\n",
    "WEIGHT_DECAY = 1e-4\n",
    "\n",
    "EPS_CLIP = 0.2\n",
    "GRAD_CLIP = 1\n",
    "NOISE_REDUCE = 0.999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = UnityEnvironment(file_name='./Crawler.app')\n",
    "\n",
    "brain_name = env.brain_names[0] # get the brain from unity environment\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Environment Info')\n",
    "env_info = env.reset(train_mode=False)[brain_name] # reset environment to a new, random state\n",
    "state = env_info.vector_observations\n",
    "action_size = brain.vector_action_space_size\n",
    "\n",
    "print('Number of agents: {}'.format(len(env_info.agents)))\n",
    "print('State space: {}'.format(state.shape[1]))\n",
    "print('Action space: {}'.format(action_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define [PPO](https://arxiv.org/pdf/1707.06347.pdf) Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PPOAgent():\n",
    "    \"\"\"The Agent that will interacts with and learns from the environment.\"\"\"\n",
    "    \n",
    "    def __init__(self, env, seed):\n",
    "        \"\"\"Initialize an Agent object.\"\"\"\n",
    "        \n",
    "        self.brain = env.brains[env.brain_names[0]] # get the brain from unity environment\n",
    "        \n",
    "        self.env_info = env.reset(train_mode=False)[brain_name]\n",
    "        \n",
    "        self.num_agents = len(env_info.agents)\n",
    "        \n",
    "        self.state_size = env_info.vector_observations.shape[1]\n",
    "        self.action_size = self.brain.vector_action_space_size\n",
    "        \n",
    "        self.gamma = GAMMA\n",
    "        self.tau = TAU\n",
    "        self.lr = LR\n",
    "        self.epsilon = EPSILON\n",
    "        self.weight_decay = WEIGHT_DECAY\n",
    "        \n",
    "        self.eps_clip = EPS_CLIP\n",
    "        self.grad_clip = GRAD_CLIP\n",
    "        self.noice_reduce = NOISE_REDUCE\n",
    "        \n",
    "        self.policy = PolicyNetwork(self.state_size, self.action_size, self.action_std, seed).to(device)\n",
    "        self.policy_optimizer = torch.optim.Adam(self.policy.parameters(), lr=self.lr, eps=self.epsilon, weight_decay=self.weight_decay)\n",
    "        \n",
    "        self.trajectory = []\n",
    "        self.std_scale = 1.\n",
    "        \n",
    "        # set buffer\n",
    "        self.buffer = ReplayBuffer(self.batch_size, self.num_agents, random_seed)\n",
    "        \n",
    "    def compute_action(self, action_means, scale=1.):\n",
    "        \n",
    "        dist = torch.distributions.Normal(action_means, \n",
    "                                          F.hardtanh(self.policy.std, min_val=0.06*scale, max_val=0.6*scale))\n",
    "        actions = dist.sample()\n",
    "        \n",
    "        return actions.detach()\n",
    "    \n",
    "    def act(self, states):\n",
    "        \n",
    "        states = torch.FloatTensor(states).to(device)\n",
    "        \n",
    "        self.policy.eval()\n",
    "        with torch.no_grad():\n",
    "            action_means = self.policy.act(states)\n",
    "            \n",
    "        self.policy.train()\n",
    "        \n",
    "        actions = self.compute_action(action_means)\n",
    "        actions = actions.cpu().data.numpy()\n",
    "        \n",
    "        return actions\n",
    "        \n",
    "    def evaluate_policy(self, states, actions):\n",
    "        \n",
    "        action_means = self.policy.act(states)\n",
    "        \n",
    "        dist = torch.distributions.Normal(action_means, \n",
    "                                          F.hardtanh(self.policy.std, min_val=0.06*scale, max_val=0.6*scale))\n",
    "        \n",
    "        actions = dist.sample()\n",
    "        action_log_probs = dist.log_prob(actions)\n",
    "        action_log_probs = torch.sum(action_log_probs, dim=1, keepdim=True)\n",
    "        \n",
    "        dist_entropies = dist.entropy().mean()\n",
    "        state_values = self.policy.evaluate(states)\n",
    "        \n",
    "        return action_log_probs, state_values, dist_entropies\n",
    "        \n",
    "    def memorize_step(self, trajectory):\n",
    "        \n",
    "        self.trajectory.append(trajectory)\n",
    "        \n",
    "    def memorize_trajectory(self, states):\n",
    "        \n",
    "        pending_value = self.policy(states)[-1]\n",
    "        self.trajectory.append([states, pending_value, None, None, None, None])\n",
    "        \n",
    "        processed_trajectory = [None] * (len(self.trajectory) - 1)\n",
    "        advantages = torch.Tensor(np.zeros((self.num_agents, 1))).to(device)\n",
    "        returns = pending_values.detach()\n",
    "\n",
    "        for i in reversed(range(len(self.trajectory) - 1)):\n",
    "            \n",
    "            states, values, actions, log_probs, rewards, dones = self.trajectory[i]\n",
    "            \n",
    "            states = torch.Tensor(states).to(device)\n",
    "            actions = torch.Tensor(actions).to(device)\n",
    "            rewards = torch.Tensor(rewards).unsqueeze(1).to(device)\n",
    "            dones = torch.Tensor(dones).unsqueeze(1).to(device)\n",
    "            \n",
    "            next_values = self.trajectory[i + 1][1]\n",
    "            returns = rewards + self.gamma * dones * returns\n",
    "            TD_error = rewards + self.gamma * dones * next_values.detach() - values.detach()\n",
    "            advantages = advantages * self.tau * self.gamma * dones + TD_error\n",
    "            \n",
    "            processed_trajectory[i] = [states, actions, log_probs, returns, advantages]\n",
    "            \n",
    "        self.buffer.add(processed_trajectory)\n",
    "        self.trajectory = []\n",
    "        \n",
    "    def learn(self, states):\n",
    "        \n",
    "        if len(self.memory) * self.num_agents > BATCH_SIZE * AGENT_BATCH_SIZE:\n",
    "            \n",
    "            for states, actions, old_log_probs, returns, advantages in self.memory.sample():\n",
    "                \n",
    "                log_probs, state_values, dist_entropies = self.evaluate_policy(states, actions)\n",
    "                \n",
    "                ratios = torch.exp(log_probs - old_log_probs.detach())\n",
    "                \n",
    "                surrogate1 = ratios * advantages\n",
    "                surrogate2 = torch.clamp(ratios, 1-self.eps_clip, 1+self.eps_clip) * advantages\n",
    "                \n",
    "                policy_loss = -torch.min(surrogate1, surrogate2).mean(0) - 0.01 * dist_entropies.mean()\n",
    "                value_loss = 0.5 * (returns - state_values).pow(2).mean()\n",
    "                \n",
    "                total_loss = policy_loss + value_loss\n",
    "                \n",
    "                self.policy_optimizer.zero_grad()\n",
    "                total_loss.backward()\n",
    "                nn.utils.clip_grad_norm_(self.policy.parameters(), self.grad_clip)\n",
    "                self.policy_optimizer.step()\n",
    "            \n",
    "            self.buffer.reset()\n",
    "            \n",
    "        self.std_scale = self.std_scale * self.noise_reduce\n",
    "        \n",
    "    def save(self, policy_path):\n",
    "        \n",
    "        if not os.path.exists('./agents/'): os.makedirs('./agents/')\n",
    "        torch.save(self.policy.state_dict(), policy_path)\n",
    "        \n",
    "    def watch(self, num_episodes=10, max_time=2000):\n",
    "        \n",
    "        for i_episode in range(1, num_episodes+1):\n",
    "\n",
    "            env_info = env.reset(train_mode=True)[brain_name]\n",
    "            states = env_info.vector_observations\n",
    "\n",
    "            agent_scores = np.zeros(len(env_info.agent_scores))\n",
    "\n",
    "            for time_step in range(max_time):\n",
    "\n",
    "                actions = agent.act(states)\n",
    "                \n",
    "                env_info = env.step(actions)[brain_name]\n",
    "                next_states, rewards, dones = env_info.vector_observations, env_info.rewards, env_info.local_done\n",
    "                dones = np.array([1 if time_step else 0 for time_step in dones])\n",
    "\n",
    "                states = next_states\n",
    "                agent_scores += rewards\n",
    "\n",
    "                if all(dones):\n",
    "                    agent.learn(next_states)\n",
    "                    break\n",
    "\n",
    "            scores_window.append(np.mean(agent_scores))\n",
    "\n",
    "            print(f'\\rEpisode: {i_episode}, Average Score: {np.mean(scores_window):.3f}', end='')\n",
    "\n",
    "        env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "agent = PPOAgent(env, seed=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train The Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_agent(num_episodes=2000, max_time=2000):\n",
    "    \n",
    "    all_scores = []\n",
    "    scores_target = 1000.0\n",
    "    scores_window = deque(maxlen=PRINT_EVERY)\n",
    "    \n",
    "    # training loop\n",
    "    for i_episode in range(1, num_episodes+1):\n",
    "\n",
    "        env_info = env.reset(train_mode=True)[brain_name]\n",
    "        states = env_info.vector_observations\n",
    "        \n",
    "        agent_scores = np.zeros(len(env_info.agent_scores))\n",
    "        \n",
    "        for time_step in range(max_time):\n",
    "            \n",
    "            actions = agent.act(states)\n",
    "            log_probs, state_values, _ = agent.evaluate_policy(states, actions)\n",
    "            \n",
    "            env_info = env.step(actions)[brain_name]\n",
    "            next_states, rewards, dones = env_info.vector_observations, env_info.rewards, env_info.local_done\n",
    "            dones = np.array([1 if time_step else 0 for time_step in dones])\n",
    "            \n",
    "            agent.save_step([states, state_values.detach(), actions, log_probs.detach(), rewards, 1-dones])\n",
    "            states = next_states\n",
    "            agent_scores += rewards\n",
    "            \n",
    "            if all(dones):\n",
    "                agent.learn(next_states)\n",
    "                break\n",
    "        \n",
    "        all_scores.append(np.mean(agent_scores))\n",
    "        scores_window.append(np.mean(agent_scores))\n",
    "        \n",
    "        print(f'\\rEpisode: {i_episode}, Average Score: {np.mean(scores_window):.3f}', end='')\n",
    "        \n",
    "        if i_episode % PRINT_EVERY == 0:\n",
    "            print(f'\\rEpisode: {i_episode}, Average Score: {np.mean(scores_window):.3f}')\n",
    "        if np.mean(scores_window) >= scores_target:\n",
    "            print(f'\\nEnvironment solved in {i_episode-100:d} episodes! Average Score: {np.mean(scores_window):.3f}')\n",
    "            break\n",
    "            \n",
    "    agent.save(f'./agents/PPO_{brain_name}.pth')\n",
    "    print('Training completed.')\n",
    "     \n",
    "    return all_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores = train_agent(num_episodes=2000, max_time=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate The Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(np.arange(len(scores)), scores, color='green')\n",
    "plt.xlabel('Num of episodes')\n",
    "plt.ylabel('Score')\n",
    "if not os.path.exists('./images/'): os.makedirs('./images/')\n",
    "plt.savefig('./images/plot_of_ppo_evaluation.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¬ Watch The Smart Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "agent.policy.load_state_dict(torch.load(f'./agents/PPO_{brain_name}.pth'));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "agent.watch(num_episodes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
