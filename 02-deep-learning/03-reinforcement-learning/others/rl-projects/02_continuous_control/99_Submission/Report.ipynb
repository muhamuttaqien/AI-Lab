{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Udacity - Continuous Control Project Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This report explains about the result of Continuous Control Project as the 2nd project of Deep Reinforcement Learning [Udacity](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) Course. Written by [muhamuttaqien](https://github.com/muhamuttaqien)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/robotic_arm_reacher.gif'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Learning Algorithm & The Choosen Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Continuous Control project, I tried to solve the environment using one of the most state-of-the-art Reinforcement Learning algorithm, called **DDPG** (stands for **Deep Deterministic Policy Gradient**). DDPG is a different kind of actor-critic method and it could be seen as approximate DQN instead of an actual actor-critic since the algorithm introduces Target network to help learning process. Actor-critic agent is an agent that uses function approximation to learn a policy and a value function. In DDPG, we use two deep neural networks where we can call one the actor and the other one the critic and the actor is used to approximate the optimal policy deterministically, that means we want to always output the best believed action for any given state. Keep in mind that this is unlike a stochastic policies in which we want the policy to learn a probability distribution over the actions! Instead, we want the beleived best action every single time acquired by the actor network and it is a deterministic policy. Other interesting aspects of DDPG are they use of a replay buffer and soft updates to update the target networks.\n",
    "\n",
    "1. Experience Replay\n",
    "   * Basically this feature is exactly the same as applied for DQN algorithm where the agent interacts with the environment and at each time step, the agent obtain a state, action, reward and next state tuple, just learn from it and then directly discard it then moves on to the next new tuple in the following time step. But this seems will little wasteful. The main idea is the agent could actually learn more from those experience tuples if they are stored somewhere, moreover, some states are pretty rare to come by and some actions can be pretty costly. So it would be nice if we can recall such experiences for learning process. This is exactly what a experience replay allows us to do where we store each experience tuple in the buffer as the agent keep interacting with the environment and then randomly sample a small batch of tuples from it in order to learn. This random sampling also prevent action values from oscillating or diverging catastrophically due to high correlation problem caused by learning those experience tuples in sequential order. I implemented this feature by defining dedicated class named `ReplayBuffer.`\n",
    "   \n",
    "2. Soft Updates\n",
    "    * This update is unlike the ordinary update we use before. The soft updates are a bit different, in DQN, we have two copies of our network weights, the regular and the target network and in the Atari paper in which DQN was introduced, the target network is updated every 10,000 time steps, we simply copy the weights of our regular network into our target network that is the target network is fixed for 10,000 time steps and then he gets a big update. In DDPG, we have two copies of our network weights for each network, a regular for the actor and a regular for the critic, and a target for the actor and a target for the critic but in DDPG the target networks are updated using a soft updates strategy where the update strategy consists of slowly blending our regular network weights with our target network weights so every time step me make our target network be 99.99 percent of our target networks and only 0.01 percent of our regular network weights and we are slowly mix in our regular network weights into our target network weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters chosen for optimally training the DDPG network was that `1e6` for buffer size. We set that number as maximum memory capacity for the agent to memorize all experiences obtained during learning. When the capacity reached the number of batch size, which is `512`, then the buffer randomly sampled the experiences for the agent to learn from the past. The learning step was performed for every 4 step done by the agent in order to gradually update the parameters (weights) of actor network and critic network by learning rate `1e-4` and `3e-4`, respectively. We set gamma value 0.99 since the agent was supposed to care about future rewards. Lastly, since we use separate network for estimating Q target, we set `1e-3` as tau value so we can gradually copy the parameters of our trained Actor network to update the weight of Critic network and made the learning algorithm more stable. These parameters was defined on `Continuous_Control_DDPG.ipynb` file. Tricky improvement I did for improving the agent performence are: 1) Implementing gradient clipping and 2) Extending network capacity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Architecture for Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model architecture implemented here did't use convolutional neural network that takes in the difference between the current and previous screen patches. Instead, I used a fully connected layer that takes state vectors containing information the agent observed as input for Actor Network and for Critic Network to outputs Q value for each available action can be performed on the environment. The Actor is basically learning the argmax of Q(s a), which is the best action, and the Critic learn to evaluate the optimal action value function by using the Actor's best believed action. We use the Actor which is an approximate maximizer to calculate a new target value for training the action value function much in the way DQN does.\n",
    "\n",
    "Both Actor & Critic networks defined on `model.py` file consists of a number of fully connected layers, with 256 neuron units on the hidden layer of Actor Network and 256 followed by 128 neuron units for Critic Network. Both use rectifier activation function called ReLU to introduce non-linearity for each hidden layers. The function is a simple calculation that returns the value provided as input directly, or the value 0.0 if the input is 0.0 or less. Also, I set the seed on the network to allow reproducible randomness or stochasticity behaviour to the neural network. When the DDPG agent was initialized, it automatically set Adam as the optimizer that performs stochastic gradient descent procedure to update network weights during training, and calculating `policy loss` and `value loss` to update the weights of the network so it gradually performed better for the next episodes. All work was super easily done by [PyTorch](http://pytorch.org/) Deep Learning framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Plot of Rewards & Number of Episodes Needed to Solve The Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I set the total of episodes `5000` and max time `2000` for each episode as plotted on the figure below and the result showed that the total scores moderately increased overtime as the more episodes taken by the agent to observe the environment. The agent learned from experiences for every 4 episodes taken and the agent considerably performed well when it achieved 30 average rewards over 100 consecutive episodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/plot_of_ddpg_evaluation.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result, my agent finally solved the task to reach the target with average score 30 in around 446 episodes, which is very good. We can see the statistics of obtained scores progress on `Continuous_Control_DDPG` jupyter notebook, and also the demonstration of how the trained agent smartly behaves to the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Future Ideas for Improving The Agent's Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the date of submission, I plan some further improvement ideas that focus on utilizing DDPG as one of powerful algorithms on Reinforcement Learning world especially to encounter continuous action space. I'm eager to extend the capacity of Actor & Policy network by adding more layers to them. Moreoever, I'm also interested in working with **A2C** (stands for **Advantage Actor Critic**) algorithm since it looks promising to improve the agent's performance and gain higher total scores on the environment. I'm excited about!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Github Link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My workplace repository can be easily accessed from [muhamuttaqien](https://github.com/muhamuttaqien/AI-Lab/tree/master/02-deep-learning/03-reinforcement-learning/others/rl-projects/02_continuous_control/99_Submission) Github profile."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
