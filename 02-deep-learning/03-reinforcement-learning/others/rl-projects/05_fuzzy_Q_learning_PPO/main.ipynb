{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning for Unit Commitment Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from ppo.main import Agent as PPO_Agent\n",
    "from ppo.buffer import ReplayBuffer\n",
    "from environment.main import Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = Environment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "buffer = ReplayBuffer(capacity=240)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "agent = PPO_Agent(env.state_size, env.action_size, 'agent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train The Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_generator = 10\n",
    "num_episodes = 5000\n",
    "max_time_step = 24 # hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "total_rewards = []\n",
    "\n",
    "for i_episode in range(1, num_episodes+1):\n",
    "    \n",
    "    env.reset()\n",
    "    state = np.zeros(env.state_size)\n",
    "    \n",
    "    for time_step in range(max_time_step):\n",
    "        \n",
    "        hour = 'Hour-' + str(time_step)\n",
    "        actions = []\n",
    "        \n",
    "        for i_gen in range(num_generator):\n",
    "            \n",
    "            gen = 'gen-' + str(i_gen)\n",
    "            \n",
    "            action = agent.act(state)\n",
    "            action = np.argmax(action)\n",
    "            \n",
    "            env.net.net.res_on_off_schedule.loc[hour, gen] = action\n",
    "            next_state, reward = env.step(gen, hour, action)\n",
    "            \n",
    "            if i_gen == 9:\n",
    "                gen_end = True\n",
    "            else:\n",
    "                gen_end = False\n",
    "            \n",
    "            if time_step == 23:\n",
    "                done = True\n",
    "            else:\n",
    "                done = False\n",
    "                \n",
    "            buffer.add(state, action, reward, next_state, done)\n",
    "            \n",
    "            if gen_end:\n",
    "                schedule = env.net.net.res_use_schedule.loc[hour, :].values\n",
    "                on_off = env.net.net.res_on_off_schedule.loc[hour, :].values\n",
    "                \n",
    "                for index, (sample_state, sample_action, _, sample_next_state, sample_done) in enumerate(buffer.memory):\n",
    "                    \n",
    "                    if schedule[index] == 1 and on_off[index] == 1:\n",
    "                        agent.memorize(sample_state, sample_action, reward, sample_next_state, sample_done)\n",
    "                        \n",
    "                    elif schedule[index] == 0 and on_off[index] == 1:\n",
    "                        agent.memorize(sample_state, sample_action, -reward*0.8, sample_next_state, sample_done)\n",
    "                    \n",
    "                    else:\n",
    "                        agent.memorize(sample_state, sample_action, reward*0.01, sample_next_state, sample_done)\n",
    "                \n",
    "                buffer.memory = deque(maxlen=96)\n",
    "            \n",
    "            state = next_state\n",
    "\n",
    "    total_rewards.append(reward)\n",
    "    print(f'Episode {i_episode}, Total Reward: {reward:.4f}')\n",
    "    \n",
    "print('Training done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate The Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(total_rewards, color='green')\n",
    "plt.xlabel('Number of Episodes')\n",
    "plt.ylabel('Total Rewards')\n",
    "plt.savefig('./images/plot_PPO_total_rewards.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('--- UNIT COMMITMENT REPORT')\n",
    "\n",
    "print('\\nUC Hourly Schedule:', env.net.net.res_on_off_schedule)\n",
    "print('\\nSimulation Result:', env.net.net.res_generation)\n",
    "print('\\nTotal Generation (MW):', env.net.net.res_generation.sum(axis=1))\n",
    "print('\\nFuel Cost:', env.net.net.res_cost)\n",
    "print('\\nPenalty:', env.net.net.res_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
