{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gym\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "ENV_NAME = 'CartPole-v0'\n",
    "env = gym.make(ENV_NAME).unwrapped; env.seed(90);\n",
    "\n",
    "nb_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment Display:\n",
      "State space Box(4,)\n",
      "Action space Discrete(2)\n"
     ]
    }
   ],
   "source": [
    "print('Environment Display:')\n",
    "env.reset() # reset environment to a new, random state\n",
    "env.render()\n",
    "\n",
    "print('State space {}'.format(env.observation_space))\n",
    "print('Action space {}'.format(env.action_space))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build DQN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                80        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 34        \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 114\n",
      "Trainable params: 114\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('linear'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MEMORY_LIMIT = 50000\n",
    "WINDOW_LENGTH = 1\n",
    "MODEL_UPDATE = 1e-2\n",
    "NB_WARMUP = 10\n",
    "NB_STEPS = 5000\n",
    "LR = 1e-3\n",
    "LOSS = 'mae'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Memory, Policy & Init DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "memory = SequentialMemory(limit=MEMORY_LIMIT, window_length=WINDOW_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "policy = EpsGreedyQPolicy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, policy=policy, \n",
    "               nb_steps_warmup=NB_WARMUP, target_model_update=MODEL_UPDATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dqn.compile(Adam(lr=LR), metrics=[LOSS])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 5000 steps ...\n",
      "    9/5000: episode: 1, duration: 0.214s, episode steps: 9, steps per second: 42, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.142 [-1.802, 2.874], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   18/5000: episode: 2, duration: 0.599s, episode steps: 9, steps per second: 15, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.119 [-1.755, 2.741], loss: 0.543079, mean_absolute_error: 0.839125, mean_q: 0.464855\n",
      "   30/5000: episode: 3, duration: 0.066s, episode steps: 12, steps per second: 181, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.083 [0.000, 1.000], mean observation: 0.103 [-1.975, 3.030], loss: 0.495114, mean_absolute_error: 0.831914, mean_q: 0.538446\n",
      "   39/5000: episode: 4, duration: 0.050s, episode steps: 9, steps per second: 179, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.138 [-1.726, 2.755], loss: 0.430001, mean_absolute_error: 0.828605, mean_q: 0.657879\n",
      "   48/5000: episode: 5, duration: 0.047s, episode steps: 9, steps per second: 192, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.119 [-1.779, 2.753], loss: 0.393591, mean_absolute_error: 0.810299, mean_q: 0.727109\n",
      "   58/5000: episode: 6, duration: 0.053s, episode steps: 10, steps per second: 188, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.125 [-1.936, 2.989], loss: 0.356613, mean_absolute_error: 0.805010, mean_q: 0.837429\n",
      "   67/5000: episode: 7, duration: 0.053s, episode steps: 9, steps per second: 170, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.122 [-1.808, 2.745], loss: 0.322552, mean_absolute_error: 0.798948, mean_q: 0.964296\n",
      "   76/5000: episode: 8, duration: 0.061s, episode steps: 9, steps per second: 149, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.160 [-1.773, 2.872], loss: 0.296834, mean_absolute_error: 0.772707, mean_q: 1.046453\n",
      "   85/5000: episode: 9, duration: 0.070s, episode steps: 9, steps per second: 129, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.152 [-1.800, 2.843], loss: 0.297943, mean_absolute_error: 0.730016, mean_q: 1.075840\n",
      "   98/5000: episode: 10, duration: 0.103s, episode steps: 13, steps per second: 126, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.154 [0.000, 1.000], mean observation: 0.111 [-1.735, 2.768], loss: 0.289162, mean_absolute_error: 0.716529, mean_q: 1.239444\n",
      "  107/5000: episode: 11, duration: 0.068s, episode steps: 9, steps per second: 132, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.149 [-1.785, 2.852], loss: 0.283884, mean_absolute_error: 0.687162, mean_q: 1.405209\n",
      "  115/5000: episode: 12, duration: 0.059s, episode steps: 8, steps per second: 136, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.129 [-1.591, 2.543], loss: 0.267248, mean_absolute_error: 0.626072, mean_q: 1.492219\n",
      "  125/5000: episode: 13, duration: 0.075s, episode steps: 10, steps per second: 134, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.120 [-1.986, 3.049], loss: 0.295063, mean_absolute_error: 0.595356, mean_q: 1.572688\n",
      "  135/5000: episode: 14, duration: 0.084s, episode steps: 10, steps per second: 119, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.145 [-1.164, 2.002], loss: 0.245631, mean_absolute_error: 0.515386, mean_q: 1.595219\n",
      "  143/5000: episode: 15, duration: 0.061s, episode steps: 8, steps per second: 132, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.125 [0.000, 1.000], mean observation: 0.175 [-1.324, 2.242], loss: 0.251184, mean_absolute_error: 0.476387, mean_q: 1.660919\n",
      "  153/5000: episode: 16, duration: 0.076s, episode steps: 10, steps per second: 132, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.118 [-1.563, 2.468], loss: 0.307953, mean_absolute_error: 0.482313, mean_q: 1.718277\n",
      "  162/5000: episode: 17, duration: 0.068s, episode steps: 9, steps per second: 132, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.130 [-1.803, 2.816], loss: 0.250152, mean_absolute_error: 0.402972, mean_q: 1.794431\n",
      "  172/5000: episode: 18, duration: 0.067s, episode steps: 10, steps per second: 149, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.140 [-1.588, 2.631], loss: 0.278614, mean_absolute_error: 0.410806, mean_q: 1.865045\n",
      "  184/5000: episode: 19, duration: 0.074s, episode steps: 12, steps per second: 162, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.083 [0.000, 1.000], mean observation: 0.097 [-1.952, 3.001], loss: 0.295104, mean_absolute_error: 0.474903, mean_q: 1.910461\n",
      "  193/5000: episode: 20, duration: 0.072s, episode steps: 9, steps per second: 125, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.148 [-1.742, 2.839], loss: 0.278356, mean_absolute_error: 0.533779, mean_q: 1.942964\n",
      "  203/5000: episode: 21, duration: 0.081s, episode steps: 10, steps per second: 124, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.142 [-1.971, 3.033], loss: 0.221379, mean_absolute_error: 0.553020, mean_q: 1.983054\n",
      "  212/5000: episode: 22, duration: 0.066s, episode steps: 9, steps per second: 136, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.175 [-1.729, 2.891], loss: 0.226363, mean_absolute_error: 0.608096, mean_q: 2.092158\n",
      "  223/5000: episode: 23, duration: 0.060s, episode steps: 11, steps per second: 182, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.141 [-2.144, 3.282], loss: 0.292999, mean_absolute_error: 0.687116, mean_q: 2.136094\n",
      "  235/5000: episode: 24, duration: 0.075s, episode steps: 12, steps per second: 161, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.083 [0.000, 1.000], mean observation: 0.093 [-1.997, 3.004], loss: 0.221579, mean_absolute_error: 0.731590, mean_q: 2.145602\n",
      "  245/5000: episode: 25, duration: 0.072s, episode steps: 10, steps per second: 138, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.148 [-1.938, 3.041], loss: 0.272429, mean_absolute_error: 0.810158, mean_q: 2.233374\n",
      "  255/5000: episode: 26, duration: 0.068s, episode steps: 10, steps per second: 146, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.151 [-1.982, 3.135], loss: 0.181050, mean_absolute_error: 0.812409, mean_q: 2.266511\n",
      "  264/5000: episode: 27, duration: 0.064s, episode steps: 9, steps per second: 141, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.141 [-1.760, 2.744], loss: 0.204744, mean_absolute_error: 0.856211, mean_q: 2.385258\n",
      "  274/5000: episode: 28, duration: 0.070s, episode steps: 10, steps per second: 143, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.160 [-1.916, 3.066], loss: 0.251707, mean_absolute_error: 0.916839, mean_q: 2.451312\n",
      "  284/5000: episode: 29, duration: 0.093s, episode steps: 10, steps per second: 107, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.169 [-1.905, 3.071], loss: 0.212355, mean_absolute_error: 0.936477, mean_q: 2.461589\n",
      "  293/5000: episode: 30, duration: 0.084s, episode steps: 9, steps per second: 108, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.162 [-1.741, 2.806], loss: 0.182698, mean_absolute_error: 0.956160, mean_q: 2.534238\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  303/5000: episode: 31, duration: 0.056s, episode steps: 10, steps per second: 180, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.147 [-1.935, 3.077], loss: 0.223558, mean_absolute_error: 1.007768, mean_q: 2.633713\n",
      "  314/5000: episode: 32, duration: 0.068s, episode steps: 11, steps per second: 161, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.122 [-1.736, 2.736], loss: 0.208995, mean_absolute_error: 1.026046, mean_q: 2.659080\n",
      "  326/5000: episode: 33, duration: 0.093s, episode steps: 12, steps per second: 128, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.083 [0.000, 1.000], mean observation: 0.084 [-2.004, 3.043], loss: 0.301327, mean_absolute_error: 1.114365, mean_q: 2.696966\n",
      "  336/5000: episode: 34, duration: 0.081s, episode steps: 10, steps per second: 124, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.142 [-1.978, 3.062], loss: 0.272284, mean_absolute_error: 1.140407, mean_q: 2.691997\n",
      "  346/5000: episode: 35, duration: 0.074s, episode steps: 10, steps per second: 135, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.137 [-1.518, 2.498], loss: 0.259148, mean_absolute_error: 1.155174, mean_q: 2.816250\n",
      "  360/5000: episode: 36, duration: 0.099s, episode steps: 14, steps per second: 141, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.143 [0.000, 1.000], mean observation: 0.083 [-1.982, 2.973], loss: 0.246578, mean_absolute_error: 1.203881, mean_q: 2.916020\n",
      "  369/5000: episode: 37, duration: 0.049s, episode steps: 9, steps per second: 184, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.136 [-1.768, 2.807], loss: 0.229168, mean_absolute_error: 1.235970, mean_q: 2.967946\n",
      "  379/5000: episode: 38, duration: 0.052s, episode steps: 10, steps per second: 193, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.154 [-1.985, 3.116], loss: 0.213581, mean_absolute_error: 1.275877, mean_q: 3.057317\n",
      "  391/5000: episode: 39, duration: 0.062s, episode steps: 12, steps per second: 192, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.083 [0.000, 1.000], mean observation: 0.126 [-1.991, 3.100], loss: 0.237496, mean_absolute_error: 1.336487, mean_q: 3.176914\n",
      "  403/5000: episode: 40, duration: 0.092s, episode steps: 12, steps per second: 130, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.083 [0.000, 1.000], mean observation: 0.123 [-1.967, 3.110], loss: 0.195226, mean_absolute_error: 1.375872, mean_q: 3.212534\n",
      "  413/5000: episode: 41, duration: 0.059s, episode steps: 10, steps per second: 169, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.147 [-1.990, 3.083], loss: 0.233376, mean_absolute_error: 1.441225, mean_q: 3.304279\n",
      "  423/5000: episode: 42, duration: 0.064s, episode steps: 10, steps per second: 157, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.150 [-1.988, 3.085], loss: 0.214843, mean_absolute_error: 1.480099, mean_q: 3.283895\n",
      "  434/5000: episode: 43, duration: 0.064s, episode steps: 11, steps per second: 171, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.141 [-2.103, 3.317], loss: 0.213754, mean_absolute_error: 1.520444, mean_q: 3.374433\n",
      "  445/5000: episode: 44, duration: 0.073s, episode steps: 11, steps per second: 150, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.150 [-2.110, 3.301], loss: 0.227925, mean_absolute_error: 1.560179, mean_q: 3.412006\n",
      "  454/5000: episode: 45, duration: 0.065s, episode steps: 9, steps per second: 139, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.132 [-1.798, 2.839], loss: 0.226891, mean_absolute_error: 1.592986, mean_q: 3.404905\n",
      "  463/5000: episode: 46, duration: 0.065s, episode steps: 9, steps per second: 139, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.178 [-1.716, 2.854], loss: 0.197793, mean_absolute_error: 1.610613, mean_q: 3.493993\n",
      "  473/5000: episode: 47, duration: 0.072s, episode steps: 10, steps per second: 139, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.139 [-1.985, 3.070], loss: 0.217850, mean_absolute_error: 1.671889, mean_q: 3.594573\n",
      "  482/5000: episode: 48, duration: 0.056s, episode steps: 9, steps per second: 161, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.163 [-1.525, 2.513], loss: 0.212947, mean_absolute_error: 1.686368, mean_q: 3.550814\n",
      "  492/5000: episode: 49, duration: 0.056s, episode steps: 10, steps per second: 178, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.145 [-1.769, 2.719], loss: 0.183495, mean_absolute_error: 1.690670, mean_q: 3.453197\n",
      "  503/5000: episode: 50, duration: 0.085s, episode steps: 11, steps per second: 129, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.128 [-1.527, 2.484], loss: 0.162607, mean_absolute_error: 1.713659, mean_q: 3.562577\n",
      "  515/5000: episode: 51, duration: 0.083s, episode steps: 12, steps per second: 144, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.167 [0.000, 1.000], mean observation: 0.128 [-1.520, 2.516], loss: 0.146279, mean_absolute_error: 1.735999, mean_q: 3.723681\n",
      "  525/5000: episode: 52, duration: 0.062s, episode steps: 10, steps per second: 161, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.159 [-1.326, 2.209], loss: 0.160241, mean_absolute_error: 1.765075, mean_q: 3.723468\n",
      "  533/5000: episode: 53, duration: 0.050s, episode steps: 8, steps per second: 159, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.149 [-1.613, 2.553], loss: 0.127964, mean_absolute_error: 1.775348, mean_q: 3.717625\n",
      "  542/5000: episode: 54, duration: 0.050s, episode steps: 9, steps per second: 179, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.135 [-1.346, 2.222], loss: 0.156232, mean_absolute_error: 1.804528, mean_q: 3.821091\n",
      "  553/5000: episode: 55, duration: 0.067s, episode steps: 11, steps per second: 165, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.131 [-1.321, 2.223], loss: 0.163758, mean_absolute_error: 1.831784, mean_q: 3.817397\n",
      "  561/5000: episode: 56, duration: 0.041s, episode steps: 8, steps per second: 195, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.125 [0.000, 1.000], mean observation: 0.160 [-1.352, 2.229], loss: 0.172926, mean_absolute_error: 1.856509, mean_q: 3.849759\n",
      "  569/5000: episode: 57, duration: 0.041s, episode steps: 8, steps per second: 196, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.125 [0.000, 1.000], mean observation: 0.164 [-1.366, 2.257], loss: 0.149579, mean_absolute_error: 1.851164, mean_q: 3.852418\n",
      "  579/5000: episode: 58, duration: 0.060s, episode steps: 10, steps per second: 167, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.153 [-1.334, 2.202], loss: 0.195951, mean_absolute_error: 1.892166, mean_q: 3.874548\n",
      "  589/5000: episode: 59, duration: 0.061s, episode steps: 10, steps per second: 163, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.106 [-1.390, 2.137], loss: 0.148452, mean_absolute_error: 1.911368, mean_q: 3.980752\n",
      "  600/5000: episode: 60, duration: 0.063s, episode steps: 11, steps per second: 175, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.115 [-1.544, 2.362], loss: 0.128237, mean_absolute_error: 1.938686, mean_q: 4.147145\n",
      "  611/5000: episode: 61, duration: 0.057s, episode steps: 11, steps per second: 193, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.130 [-1.571, 2.413], loss: 0.152092, mean_absolute_error: 1.971046, mean_q: 4.183059\n",
      "  620/5000: episode: 62, duration: 0.055s, episode steps: 9, steps per second: 164, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.138 [-1.523, 2.443], loss: 0.132210, mean_absolute_error: 1.955086, mean_q: 4.193109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  629/5000: episode: 63, duration: 0.048s, episode steps: 9, steps per second: 187, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.129 [-1.563, 2.450], loss: 0.176096, mean_absolute_error: 1.969663, mean_q: 4.181559\n",
      "  638/5000: episode: 64, duration: 0.051s, episode steps: 9, steps per second: 177, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.147 [-1.565, 2.505], loss: 0.228695, mean_absolute_error: 1.999659, mean_q: 4.207668\n",
      "  649/5000: episode: 65, duration: 0.058s, episode steps: 11, steps per second: 189, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.134 [-1.567, 2.481], loss: 0.160468, mean_absolute_error: 1.976828, mean_q: 4.212317\n",
      "  660/5000: episode: 66, duration: 0.058s, episode steps: 11, steps per second: 191, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.138 [-1.759, 2.820], loss: 0.157127, mean_absolute_error: 1.996097, mean_q: 4.332809\n",
      "  669/5000: episode: 67, duration: 0.051s, episode steps: 9, steps per second: 177, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.137 [-1.801, 2.840], loss: 0.121546, mean_absolute_error: 2.005657, mean_q: 4.535327\n",
      "  679/5000: episode: 68, duration: 0.052s, episode steps: 10, steps per second: 192, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.136 [-1.964, 3.002], loss: 0.144811, mean_absolute_error: 2.017319, mean_q: 4.442657\n",
      "  688/5000: episode: 69, duration: 0.050s, episode steps: 9, steps per second: 180, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.149 [-0.951, 1.751], loss: 0.141564, mean_absolute_error: 2.026456, mean_q: 4.429071\n",
      "  699/5000: episode: 70, duration: 0.058s, episode steps: 11, steps per second: 191, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.119 [-1.753, 2.791], loss: 0.133923, mean_absolute_error: 2.057753, mean_q: 4.487689\n",
      "  708/5000: episode: 71, duration: 0.048s, episode steps: 9, steps per second: 188, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.149 [-1.772, 2.836], loss: 0.196513, mean_absolute_error: 2.094047, mean_q: 4.402085\n",
      "  718/5000: episode: 72, duration: 0.051s, episode steps: 10, steps per second: 194, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.127 [-1.982, 2.986], loss: 0.146895, mean_absolute_error: 2.078341, mean_q: 4.418487\n",
      "  728/5000: episode: 73, duration: 0.052s, episode steps: 10, steps per second: 193, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.143 [-1.925, 3.001], loss: 0.134485, mean_absolute_error: 2.124521, mean_q: 4.602888\n",
      "  737/5000: episode: 74, duration: 0.047s, episode steps: 9, steps per second: 193, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.146 [-1.740, 2.787], loss: 0.138506, mean_absolute_error: 2.109304, mean_q: 4.434554\n",
      "  748/5000: episode: 75, duration: 0.057s, episode steps: 11, steps per second: 193, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.136 [-1.604, 2.524], loss: 0.096485, mean_absolute_error: 2.186697, mean_q: 4.693478\n",
      "  758/5000: episode: 76, duration: 0.052s, episode steps: 10, steps per second: 193, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.137 [-1.727, 2.675], loss: 0.122312, mean_absolute_error: 2.244887, mean_q: 4.740092\n",
      "  767/5000: episode: 77, duration: 0.048s, episode steps: 9, steps per second: 188, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.125 [-1.540, 2.414], loss: 0.163798, mean_absolute_error: 2.256312, mean_q: 4.703557\n",
      "  777/5000: episode: 78, duration: 0.058s, episode steps: 10, steps per second: 174, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.148 [-1.741, 2.737], loss: 0.127373, mean_absolute_error: 2.235728, mean_q: 4.655514\n",
      "  785/5000: episode: 79, duration: 0.056s, episode steps: 8, steps per second: 142, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.146 [-1.535, 2.499], loss: 0.172452, mean_absolute_error: 2.284137, mean_q: 4.736511\n",
      "  795/5000: episode: 80, duration: 0.055s, episode steps: 10, steps per second: 181, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.157 [-1.725, 2.714], loss: 0.104310, mean_absolute_error: 2.271977, mean_q: 4.782615\n",
      "  805/5000: episode: 81, duration: 0.052s, episode steps: 10, steps per second: 194, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.142 [-1.374, 2.271], loss: 0.145923, mean_absolute_error: 2.291434, mean_q: 4.739673\n",
      "  816/5000: episode: 82, duration: 0.064s, episode steps: 11, steps per second: 172, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.132 [-1.327, 2.284], loss: 0.116957, mean_absolute_error: 2.319588, mean_q: 4.798467\n",
      "  828/5000: episode: 83, duration: 0.065s, episode steps: 12, steps per second: 184, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.124 [-1.354, 2.177], loss: 0.119890, mean_absolute_error: 2.403431, mean_q: 4.998575\n",
      "  836/5000: episode: 84, duration: 0.045s, episode steps: 8, steps per second: 179, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.125 [0.000, 1.000], mean observation: 0.120 [-1.388, 2.180], loss: 0.111995, mean_absolute_error: 2.467310, mean_q: 5.127705\n",
      "  846/5000: episode: 85, duration: 0.051s, episode steps: 10, steps per second: 196, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.101 [-1.611, 2.425], loss: 0.090474, mean_absolute_error: 2.460855, mean_q: 5.096519\n",
      "  854/5000: episode: 86, duration: 0.046s, episode steps: 8, steps per second: 173, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.125 [0.000, 1.000], mean observation: 0.137 [-1.345, 2.168], loss: 0.126333, mean_absolute_error: 2.457485, mean_q: 5.042516\n",
      "  862/5000: episode: 87, duration: 0.042s, episode steps: 8, steps per second: 192, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.125 [0.000, 1.000], mean observation: 0.147 [-1.410, 2.219], loss: 0.102995, mean_absolute_error: 2.483489, mean_q: 5.079181\n",
      "  873/5000: episode: 88, duration: 0.062s, episode steps: 11, steps per second: 178, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.112 [-1.402, 2.167], loss: 0.120588, mean_absolute_error: 2.464031, mean_q: 4.943607\n",
      "  885/5000: episode: 89, duration: 0.064s, episode steps: 12, steps per second: 187, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.115 [-1.158, 1.963], loss: 0.085620, mean_absolute_error: 2.529306, mean_q: 5.161853\n",
      "  893/5000: episode: 90, duration: 0.042s, episode steps: 8, steps per second: 188, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.125 [0.000, 1.000], mean observation: 0.154 [-1.365, 2.236], loss: 0.131218, mean_absolute_error: 2.487504, mean_q: 4.946424\n",
      "  902/5000: episode: 91, duration: 0.047s, episode steps: 9, steps per second: 192, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.125 [-1.397, 2.260], loss: 0.104384, mean_absolute_error: 2.546344, mean_q: 5.164208\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  910/5000: episode: 92, duration: 0.047s, episode steps: 8, steps per second: 170, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.125 [0.000, 1.000], mean observation: 0.139 [-1.406, 2.258], loss: 0.108146, mean_absolute_error: 2.476599, mean_q: 4.998950\n",
      "  920/5000: episode: 93, duration: 0.051s, episode steps: 10, steps per second: 195, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.094 [-1.801, 2.643], loss: 0.099124, mean_absolute_error: 2.521772, mean_q: 5.110631\n",
      "  930/5000: episode: 94, duration: 0.051s, episode steps: 10, steps per second: 195, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.143 [-1.724, 2.717], loss: 0.126879, mean_absolute_error: 2.494609, mean_q: 5.034311\n",
      "  941/5000: episode: 95, duration: 0.056s, episode steps: 11, steps per second: 197, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.098 [-1.776, 2.697], loss: 0.094848, mean_absolute_error: 2.512638, mean_q: 5.113647\n",
      "  951/5000: episode: 96, duration: 0.062s, episode steps: 10, steps per second: 162, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.137 [-1.599, 2.653], loss: 0.083280, mean_absolute_error: 2.524383, mean_q: 5.163140\n",
      "  959/5000: episode: 97, duration: 0.049s, episode steps: 8, steps per second: 163, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.150 [-1.594, 2.545], loss: 0.075481, mean_absolute_error: 2.509600, mean_q: 5.148583\n",
      "  969/5000: episode: 98, duration: 0.052s, episode steps: 10, steps per second: 191, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.148 [-1.933, 3.109], loss: 0.093581, mean_absolute_error: 2.581784, mean_q: 5.269719\n",
      "  979/5000: episode: 99, duration: 0.056s, episode steps: 10, steps per second: 177, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.119 [-1.909, 2.945], loss: 0.084831, mean_absolute_error: 2.546619, mean_q: 5.174434\n",
      "  992/5000: episode: 100, duration: 0.072s, episode steps: 13, steps per second: 181, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.114 [-1.141, 2.000], loss: 0.083377, mean_absolute_error: 2.578160, mean_q: 5.272958\n",
      " 1001/5000: episode: 101, duration: 0.047s, episode steps: 9, steps per second: 190, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.144 [-1.594, 2.489], loss: 0.098845, mean_absolute_error: 2.589178, mean_q: 5.168963\n",
      " 1010/5000: episode: 102, duration: 0.051s, episode steps: 9, steps per second: 175, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.154 [-1.605, 2.538], loss: 0.077220, mean_absolute_error: 2.613055, mean_q: 5.187446\n",
      " 1020/5000: episode: 103, duration: 0.051s, episode steps: 10, steps per second: 195, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.118 [-1.610, 2.460], loss: 0.067156, mean_absolute_error: 2.641780, mean_q: 5.320935\n",
      " 1029/5000: episode: 104, duration: 0.047s, episode steps: 9, steps per second: 191, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.164 [-1.551, 2.496], loss: 0.069845, mean_absolute_error: 2.723617, mean_q: 5.481342\n",
      " 1038/5000: episode: 105, duration: 0.048s, episode steps: 9, steps per second: 188, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.137 [-1.386, 2.186], loss: 0.107664, mean_absolute_error: 2.580448, mean_q: 5.019163\n",
      " 1048/5000: episode: 106, duration: 0.053s, episode steps: 10, steps per second: 190, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.137 [-1.588, 2.427], loss: 0.064183, mean_absolute_error: 2.720732, mean_q: 5.359781\n",
      " 1059/5000: episode: 107, duration: 0.061s, episode steps: 11, steps per second: 179, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.105 [-1.360, 2.040], loss: 0.059404, mean_absolute_error: 2.721459, mean_q: 5.383105\n",
      " 1098/5000: episode: 108, duration: 0.233s, episode steps: 39, steps per second: 167, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.718 [0.000, 1.000], mean observation: 0.139 [-3.524, 3.255], loss: 0.068581, mean_absolute_error: 2.750750, mean_q: 5.314120\n",
      " 1107/5000: episode: 109, duration: 0.052s, episode steps: 9, steps per second: 174, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.164 [-2.822, 1.739], loss: 0.062691, mean_absolute_error: 2.852572, mean_q: 5.553840\n",
      " 1117/5000: episode: 110, duration: 0.054s, episode steps: 10, steps per second: 185, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.140 [-3.104, 1.986], loss: 0.076917, mean_absolute_error: 2.905334, mean_q: 5.649962\n",
      " 1127/5000: episode: 111, duration: 0.066s, episode steps: 10, steps per second: 151, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.156 [-3.096, 1.909], loss: 0.464961, mean_absolute_error: 3.020707, mean_q: 5.850066\n",
      " 1136/5000: episode: 112, duration: 0.095s, episode steps: 9, steps per second: 95, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.152 [-2.880, 1.799], loss: 0.076166, mean_absolute_error: 2.952701, mean_q: 5.759494\n",
      " 1147/5000: episode: 113, duration: 0.112s, episode steps: 11, steps per second: 98, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.110 [-2.787, 1.802], loss: 1.106757, mean_absolute_error: 3.071179, mean_q: 5.844299\n",
      " 1156/5000: episode: 114, duration: 0.092s, episode steps: 9, steps per second: 98, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.159 [-2.897, 1.789], loss: 0.817028, mean_absolute_error: 3.088238, mean_q: 5.793862\n",
      " 1165/5000: episode: 115, duration: 0.055s, episode steps: 9, steps per second: 163, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.130 [-2.766, 1.746], loss: 1.362226, mean_absolute_error: 3.199932, mean_q: 5.945408\n",
      " 1188/5000: episode: 116, duration: 0.114s, episode steps: 23, steps per second: 202, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.565 [0.000, 1.000], mean observation: 0.109 [-0.787, 1.201], loss: 0.662991, mean_absolute_error: 3.164613, mean_q: 5.973285\n",
      " 1200/5000: episode: 117, duration: 0.078s, episode steps: 12, steps per second: 154, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.095 [-0.816, 1.250], loss: 1.153381, mean_absolute_error: 3.252424, mean_q: 6.134435\n",
      " 1215/5000: episode: 118, duration: 0.079s, episode steps: 15, steps per second: 191, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.078 [-0.809, 1.189], loss: 0.744657, mean_absolute_error: 3.197859, mean_q: 6.067351\n",
      " 1231/5000: episode: 119, duration: 0.090s, episode steps: 16, steps per second: 177, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.108 [-0.752, 1.187], loss: 0.904133, mean_absolute_error: 3.204019, mean_q: 6.104744\n",
      " 1243/5000: episode: 120, duration: 0.075s, episode steps: 12, steps per second: 160, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.081 [-1.217, 1.779], loss: 0.937696, mean_absolute_error: 3.298391, mean_q: 6.281450\n",
      " 1257/5000: episode: 121, duration: 0.079s, episode steps: 14, steps per second: 177, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.115 [-0.737, 1.247], loss: 0.820144, mean_absolute_error: 3.241150, mean_q: 6.147134\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1272/5000: episode: 122, duration: 0.131s, episode steps: 15, steps per second: 114, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.090 [-0.784, 1.194], loss: 1.051814, mean_absolute_error: 3.343407, mean_q: 6.300366\n",
      " 1291/5000: episode: 123, duration: 0.182s, episode steps: 19, steps per second: 104, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.065 [-1.020, 1.423], loss: 0.286842, mean_absolute_error: 3.255180, mean_q: 6.244413\n",
      " 1346/5000: episode: 124, duration: 0.343s, episode steps: 55, steps per second: 160, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.582 [0.000, 1.000], mean observation: 0.211 [-0.592, 1.636], loss: 0.736958, mean_absolute_error: 3.418966, mean_q: 6.463250\n",
      " 1429/5000: episode: 125, duration: 0.432s, episode steps: 83, steps per second: 192, episode reward: 83.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.518 [0.000, 1.000], mean observation: 0.188 [-0.777, 1.123], loss: 0.780721, mean_absolute_error: 3.570560, mean_q: 6.733683\n",
      " 1580/5000: episode: 126, duration: 0.804s, episode steps: 151, steps per second: 188, episode reward: 151.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.523 [0.000, 1.000], mean observation: -0.029 [-1.123, 1.692], loss: 1.044362, mean_absolute_error: 3.984323, mean_q: 7.488257\n",
      " 1613/5000: episode: 127, duration: 0.203s, episode steps: 33, steps per second: 162, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.099 [-0.669, 0.404], loss: 1.346556, mean_absolute_error: 4.199249, mean_q: 7.894768\n",
      " 1628/5000: episode: 128, duration: 0.096s, episode steps: 15, steps per second: 157, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: -0.076 [-1.298, 0.814], loss: 0.812177, mean_absolute_error: 4.197218, mean_q: 7.939625\n",
      " 1642/5000: episode: 129, duration: 0.073s, episode steps: 14, steps per second: 191, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.123 [-1.461, 0.761], loss: 1.731166, mean_absolute_error: 4.404699, mean_q: 8.221953\n",
      " 1654/5000: episode: 130, duration: 0.069s, episode steps: 12, steps per second: 173, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.102 [-1.547, 1.019], loss: 0.825786, mean_absolute_error: 4.468685, mean_q: 8.489436\n",
      " 1666/5000: episode: 131, duration: 0.077s, episode steps: 12, steps per second: 156, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.107 [-1.975, 1.193], loss: 1.769936, mean_absolute_error: 4.604612, mean_q: 8.684336\n",
      " 1677/5000: episode: 132, duration: 0.062s, episode steps: 11, steps per second: 177, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.101 [-1.695, 0.989], loss: 1.192118, mean_absolute_error: 4.583821, mean_q: 8.720977\n",
      " 1689/5000: episode: 133, duration: 0.075s, episode steps: 12, steps per second: 160, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.090 [-1.638, 1.029], loss: 1.317560, mean_absolute_error: 4.595590, mean_q: 8.655654\n",
      " 1705/5000: episode: 134, duration: 0.169s, episode steps: 16, steps per second: 95, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.625 [0.000, 1.000], mean observation: -0.104 [-1.472, 0.750], loss: 1.498523, mean_absolute_error: 4.614742, mean_q: 8.622772\n",
      " 1718/5000: episode: 135, duration: 0.109s, episode steps: 13, steps per second: 120, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.088 [-1.489, 1.011], loss: 1.213106, mean_absolute_error: 4.621367, mean_q: 8.692511\n",
      " 1730/5000: episode: 136, duration: 0.078s, episode steps: 12, steps per second: 155, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.121 [-1.262, 0.744], loss: 2.944502, mean_absolute_error: 4.998112, mean_q: 9.268250\n",
      " 1742/5000: episode: 137, duration: 0.063s, episode steps: 12, steps per second: 191, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.093 [-1.468, 0.820], loss: 1.788171, mean_absolute_error: 4.791852, mean_q: 8.965809\n",
      " 1755/5000: episode: 138, duration: 0.068s, episode steps: 13, steps per second: 192, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.078 [-1.437, 1.011], loss: 0.706445, mean_absolute_error: 4.850283, mean_q: 9.213284\n",
      " 1768/5000: episode: 139, duration: 0.078s, episode steps: 13, steps per second: 167, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.099 [-1.434, 0.940], loss: 2.627300, mean_absolute_error: 4.840168, mean_q: 8.970138\n",
      " 1778/5000: episode: 140, duration: 0.058s, episode steps: 10, steps per second: 171, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: -0.131 [-1.684, 0.961], loss: 1.045233, mean_absolute_error: 4.975729, mean_q: 9.373955\n",
      " 1789/5000: episode: 141, duration: 0.064s, episode steps: 11, steps per second: 171, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.115 [-2.217, 1.408], loss: 2.376067, mean_absolute_error: 4.948269, mean_q: 9.171884\n",
      " 1801/5000: episode: 142, duration: 0.071s, episode steps: 12, steps per second: 170, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.833 [0.000, 1.000], mean observation: -0.108 [-2.500, 1.548], loss: 0.759683, mean_absolute_error: 4.883775, mean_q: 9.271816\n",
      " 1811/5000: episode: 143, duration: 0.057s, episode steps: 10, steps per second: 176, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.130 [-2.175, 1.377], loss: 0.737949, mean_absolute_error: 5.140148, mean_q: 9.885534\n",
      " 1824/5000: episode: 144, duration: 0.097s, episode steps: 13, steps per second: 134, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.125 [-2.038, 1.158], loss: 2.913822, mean_absolute_error: 5.150754, mean_q: 9.656602\n",
      " 1835/5000: episode: 145, duration: 0.065s, episode steps: 11, steps per second: 170, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.116 [-1.890, 1.145], loss: 2.287286, mean_absolute_error: 5.304415, mean_q: 9.940898\n",
      " 1847/5000: episode: 146, duration: 0.065s, episode steps: 12, steps per second: 186, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.095 [-1.609, 0.972], loss: 1.758117, mean_absolute_error: 5.126241, mean_q: 9.691338\n",
      " 1861/5000: episode: 147, duration: 0.094s, episode steps: 14, steps per second: 148, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.643 [0.000, 1.000], mean observation: -0.114 [-1.500, 0.783], loss: 3.290627, mean_absolute_error: 5.351511, mean_q: 9.815463\n",
      " 1872/5000: episode: 148, duration: 0.062s, episode steps: 11, steps per second: 177, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.105 [-1.549, 0.973], loss: 5.414283, mean_absolute_error: 5.679571, mean_q: 10.148945\n",
      " 1884/5000: episode: 149, duration: 0.064s, episode steps: 12, steps per second: 188, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.118 [-1.675, 0.939], loss: 3.718774, mean_absolute_error: 5.546199, mean_q: 9.884206\n",
      " 1894/5000: episode: 150, duration: 0.078s, episode steps: 10, steps per second: 128, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: -0.114 [-1.481, 0.802], loss: 2.008266, mean_absolute_error: 5.381606, mean_q: 9.783691\n",
      " 1905/5000: episode: 151, duration: 0.099s, episode steps: 11, steps per second: 111, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.117 [-1.474, 0.800], loss: 2.400267, mean_absolute_error: 5.673530, mean_q: 10.458533\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1915/5000: episode: 152, duration: 0.108s, episode steps: 10, steps per second: 93, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: -0.108 [-1.487, 0.818], loss: 2.257081, mean_absolute_error: 5.527555, mean_q: 10.185092\n",
      " 1931/5000: episode: 153, duration: 0.144s, episode steps: 16, steps per second: 111, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.562 [0.000, 1.000], mean observation: -0.104 [-1.187, 0.578], loss: 2.523775, mean_absolute_error: 5.388201, mean_q: 9.974365\n",
      " 1942/5000: episode: 154, duration: 0.068s, episode steps: 11, steps per second: 162, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.101 [-1.934, 1.210], loss: 2.064177, mean_absolute_error: 5.347342, mean_q: 10.000385\n",
      " 1952/5000: episode: 155, duration: 0.055s, episode steps: 10, steps per second: 183, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: -0.130 [-1.681, 0.965], loss: 3.660538, mean_absolute_error: 5.716477, mean_q: 10.498257\n",
      " 1963/5000: episode: 156, duration: 0.083s, episode steps: 11, steps per second: 133, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.117 [-2.225, 1.419], loss: 3.660262, mean_absolute_error: 5.623632, mean_q: 10.218376\n",
      " 1972/5000: episode: 157, duration: 0.055s, episode steps: 9, steps per second: 165, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.140 [-2.280, 1.347], loss: 3.834933, mean_absolute_error: 5.620903, mean_q: 10.191436\n",
      " 1983/5000: episode: 158, duration: 0.059s, episode steps: 11, steps per second: 187, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.105 [-2.493, 1.613], loss: 4.711518, mean_absolute_error: 5.793178, mean_q: 10.390059\n",
      " 1992/5000: episode: 159, duration: 0.050s, episode steps: 9, steps per second: 178, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.136 [-2.796, 1.758], loss: 5.296587, mean_absolute_error: 5.924847, mean_q: 10.592981\n",
      " 2001/5000: episode: 160, duration: 0.050s, episode steps: 9, steps per second: 180, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.128 [-2.817, 1.801], loss: 3.122898, mean_absolute_error: 5.883091, mean_q: 10.686154\n",
      " 2012/5000: episode: 161, duration: 0.066s, episode steps: 11, steps per second: 167, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.112 [-3.294, 2.189], loss: 1.694070, mean_absolute_error: 5.741121, mean_q: 10.690366\n",
      " 2021/5000: episode: 162, duration: 0.056s, episode steps: 9, steps per second: 161, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.140 [-2.810, 1.745], loss: 3.205544, mean_absolute_error: 5.885025, mean_q: 10.880938\n",
      " 2030/5000: episode: 163, duration: 0.074s, episode steps: 9, steps per second: 121, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.141 [-2.799, 1.757], loss: 3.663885, mean_absolute_error: 5.785940, mean_q: 10.686992\n",
      " 2041/5000: episode: 164, duration: 0.096s, episode steps: 11, steps per second: 115, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.143 [-2.839, 1.751], loss: 3.475250, mean_absolute_error: 6.015889, mean_q: 11.049664\n",
      " 2049/5000: episode: 165, duration: 0.079s, episode steps: 8, steps per second: 102, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.875 [0.000, 1.000], mean observation: -0.150 [-2.234, 1.382], loss: 3.393769, mean_absolute_error: 5.771743, mean_q: 10.544567\n",
      " 2060/5000: episode: 166, duration: 0.101s, episode steps: 11, steps per second: 109, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.095 [-2.339, 1.585], loss: 2.737830, mean_absolute_error: 5.896663, mean_q: 10.900261\n",
      " 2068/5000: episode: 167, duration: 0.060s, episode steps: 8, steps per second: 134, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.139 [-2.558, 1.570], loss: 3.218501, mean_absolute_error: 5.857098, mean_q: 10.797810\n",
      " 2077/5000: episode: 168, duration: 0.067s, episode steps: 9, steps per second: 135, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.165 [-2.544, 1.565], loss: 4.121970, mean_absolute_error: 6.150490, mean_q: 11.226727\n",
      " 2086/5000: episode: 169, duration: 0.078s, episode steps: 9, steps per second: 115, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.108 [-2.751, 1.803], loss: 2.506470, mean_absolute_error: 5.912603, mean_q: 10.970203\n",
      " 2097/5000: episode: 170, duration: 0.090s, episode steps: 11, steps per second: 123, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.131 [-2.457, 1.610], loss: 3.607719, mean_absolute_error: 6.097246, mean_q: 11.218973\n",
      " 2106/5000: episode: 171, duration: 0.069s, episode steps: 9, steps per second: 130, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.159 [-2.863, 1.748], loss: 1.815243, mean_absolute_error: 5.937077, mean_q: 11.139016\n",
      " 2114/5000: episode: 172, duration: 0.054s, episode steps: 8, steps per second: 149, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.137 [-2.548, 1.605], loss: 6.020367, mean_absolute_error: 6.246337, mean_q: 11.231942\n",
      " 2124/5000: episode: 173, duration: 0.059s, episode steps: 10, steps per second: 171, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.140 [-3.065, 1.990], loss: 4.168172, mean_absolute_error: 6.160766, mean_q: 11.267058\n",
      " 2132/5000: episode: 174, duration: 0.048s, episode steps: 8, steps per second: 166, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.152 [-2.571, 1.547], loss: 4.284981, mean_absolute_error: 6.304629, mean_q: 11.368682\n",
      " 2141/5000: episode: 175, duration: 0.058s, episode steps: 9, steps per second: 154, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.164 [-2.849, 1.762], loss: 3.851418, mean_absolute_error: 6.140070, mean_q: 11.143690\n",
      " 2151/5000: episode: 176, duration: 0.058s, episode steps: 10, steps per second: 172, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.106 [-2.945, 1.962], loss: 3.508524, mean_absolute_error: 6.079120, mean_q: 11.132229\n",
      " 2161/5000: episode: 177, duration: 0.058s, episode steps: 10, steps per second: 172, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.125 [-3.005, 1.953], loss: 4.273313, mean_absolute_error: 6.178452, mean_q: 11.315485\n",
      " 2172/5000: episode: 178, duration: 0.064s, episode steps: 11, steps per second: 173, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.136 [-2.826, 1.773], loss: 1.867748, mean_absolute_error: 6.097574, mean_q: 11.472568\n",
      " 2180/5000: episode: 179, duration: 0.051s, episode steps: 8, steps per second: 157, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.153 [-2.543, 1.574], loss: 4.643894, mean_absolute_error: 6.146201, mean_q: 11.304470\n",
      " 2191/5000: episode: 180, duration: 0.065s, episode steps: 11, steps per second: 171, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.097 [-2.744, 1.769], loss: 3.452320, mean_absolute_error: 6.179045, mean_q: 11.450361\n",
      " 2201/5000: episode: 181, duration: 0.078s, episode steps: 10, steps per second: 128, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.147 [-3.022, 1.934], loss: 3.713748, mean_absolute_error: 6.245076, mean_q: 11.512854\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2211/5000: episode: 182, duration: 0.074s, episode steps: 10, steps per second: 136, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.134 [-2.972, 1.913], loss: 2.114795, mean_absolute_error: 6.184721, mean_q: 11.606050\n",
      " 2220/5000: episode: 183, duration: 0.065s, episode steps: 9, steps per second: 138, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.129 [-2.734, 1.742], loss: 4.876651, mean_absolute_error: 6.431284, mean_q: 11.797325\n",
      " 2228/5000: episode: 184, duration: 0.049s, episode steps: 8, steps per second: 163, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.162 [-2.587, 1.548], loss: 4.274918, mean_absolute_error: 6.449813, mean_q: 11.891754\n",
      " 2237/5000: episode: 185, duration: 0.057s, episode steps: 9, steps per second: 159, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.157 [-2.790, 1.736], loss: 5.982335, mean_absolute_error: 6.490141, mean_q: 11.605022\n",
      " 2247/5000: episode: 186, duration: 0.060s, episode steps: 10, steps per second: 166, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.160 [-3.090, 1.953], loss: 4.590726, mean_absolute_error: 6.381393, mean_q: 11.630186\n",
      " 2257/5000: episode: 187, duration: 0.068s, episode steps: 10, steps per second: 147, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.125 [-2.497, 1.583], loss: 4.219822, mean_absolute_error: 6.207450, mean_q: 11.235224\n",
      " 2266/5000: episode: 188, duration: 0.053s, episode steps: 9, steps per second: 170, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.133 [-2.778, 1.744], loss: 4.227184, mean_absolute_error: 6.425076, mean_q: 11.697578\n",
      " 2274/5000: episode: 189, duration: 0.049s, episode steps: 8, steps per second: 164, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.149 [-2.533, 1.554], loss: 7.035689, mean_absolute_error: 6.615934, mean_q: 11.660383\n",
      " 2283/5000: episode: 190, duration: 0.055s, episode steps: 9, steps per second: 163, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.133 [-2.801, 1.787], loss: 4.197577, mean_absolute_error: 6.393470, mean_q: 11.614109\n",
      " 2291/5000: episode: 191, duration: 0.050s, episode steps: 8, steps per second: 160, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.164 [-2.543, 1.549], loss: 4.272777, mean_absolute_error: 6.453607, mean_q: 11.645269\n",
      " 2300/5000: episode: 192, duration: 0.064s, episode steps: 9, steps per second: 140, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.132 [-2.307, 1.389], loss: 3.108228, mean_absolute_error: 6.340757, mean_q: 11.661799\n",
      " 2311/5000: episode: 193, duration: 0.094s, episode steps: 11, steps per second: 117, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.116 [-2.741, 1.781], loss: 3.866088, mean_absolute_error: 6.524083, mean_q: 12.019650\n",
      " 2322/5000: episode: 194, duration: 0.105s, episode steps: 11, steps per second: 105, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.137 [-2.699, 1.726], loss: 5.030124, mean_absolute_error: 6.697088, mean_q: 12.308152\n",
      " 2331/5000: episode: 195, duration: 0.074s, episode steps: 9, steps per second: 122, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.164 [-2.324, 1.384], loss: 6.315935, mean_absolute_error: 6.862430, mean_q: 12.243998\n",
      " 2342/5000: episode: 196, duration: 0.089s, episode steps: 11, steps per second: 124, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.111 [-1.851, 1.181], loss: 4.078074, mean_absolute_error: 6.526199, mean_q: 11.782690\n",
      " 2354/5000: episode: 197, duration: 0.097s, episode steps: 12, steps per second: 123, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.113 [-1.684, 1.004], loss: 2.765568, mean_absolute_error: 6.354407, mean_q: 11.555345\n",
      " 2367/5000: episode: 198, duration: 0.114s, episode steps: 13, steps per second: 114, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.110 [-1.510, 0.959], loss: 3.195403, mean_absolute_error: 6.361314, mean_q: 11.629917\n",
      " 2377/5000: episode: 199, duration: 0.086s, episode steps: 10, steps per second: 116, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: -0.141 [-1.529, 0.755], loss: 4.238416, mean_absolute_error: 6.408525, mean_q: 11.566494\n",
      " 2390/5000: episode: 200, duration: 0.116s, episode steps: 13, steps per second: 112, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.100 [-1.413, 0.820], loss: 3.589622, mean_absolute_error: 6.488650, mean_q: 11.777981\n",
      " 2404/5000: episode: 201, duration: 0.104s, episode steps: 14, steps per second: 135, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.643 [0.000, 1.000], mean observation: -0.098 [-1.678, 0.964], loss: 3.795267, mean_absolute_error: 6.362418, mean_q: 11.593379\n",
      " 2416/5000: episode: 202, duration: 0.088s, episode steps: 12, steps per second: 137, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.097 [-2.159, 1.335], loss: 4.904179, mean_absolute_error: 6.604916, mean_q: 11.827962\n",
      " 2428/5000: episode: 203, duration: 0.106s, episode steps: 12, steps per second: 114, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.109 [-1.667, 0.998], loss: 4.659689, mean_absolute_error: 6.539970, mean_q: 11.655479\n",
      " 2440/5000: episode: 204, duration: 0.097s, episode steps: 12, steps per second: 124, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.129 [-2.065, 1.152], loss: 3.006580, mean_absolute_error: 6.453156, mean_q: 11.821198\n",
      " 2452/5000: episode: 205, duration: 0.092s, episode steps: 12, steps per second: 130, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.107 [-2.144, 1.369], loss: 4.382764, mean_absolute_error: 6.471971, mean_q: 11.724881\n",
      " 2462/5000: episode: 206, duration: 0.079s, episode steps: 10, steps per second: 126, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.129 [-2.521, 1.588], loss: 3.283802, mean_absolute_error: 6.528674, mean_q: 12.034021\n",
      " 2471/5000: episode: 207, duration: 0.070s, episode steps: 9, steps per second: 129, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.151 [-2.274, 1.340], loss: 3.670299, mean_absolute_error: 6.636505, mean_q: 12.132298\n",
      " 2481/5000: episode: 208, duration: 0.098s, episode steps: 10, steps per second: 102, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.145 [-2.233, 1.350], loss: 3.285237, mean_absolute_error: 6.633675, mean_q: 12.313083\n",
      " 2492/5000: episode: 209, duration: 0.116s, episode steps: 11, steps per second: 95, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.151 [-2.251, 1.340], loss: 3.916428, mean_absolute_error: 6.552534, mean_q: 12.020801\n",
      " 2501/5000: episode: 210, duration: 0.073s, episode steps: 9, steps per second: 123, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.151 [-1.967, 1.184], loss: 4.743933, mean_absolute_error: 6.724566, mean_q: 12.135708\n",
      " 2514/5000: episode: 211, duration: 0.077s, episode steps: 13, steps per second: 169, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.769 [0.000, 1.000], mean observation: -0.097 [-2.326, 1.388], loss: 4.081462, mean_absolute_error: 6.500220, mean_q: 11.764969\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2525/5000: episode: 212, duration: 0.088s, episode steps: 11, steps per second: 126, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.115 [-2.154, 1.338], loss: 3.598913, mean_absolute_error: 6.624563, mean_q: 12.062018\n",
      " 2534/5000: episode: 213, duration: 0.084s, episode steps: 9, steps per second: 107, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.148 [-2.236, 1.364], loss: 3.587668, mean_absolute_error: 6.576097, mean_q: 11.975159\n",
      " 2545/5000: episode: 214, duration: 0.075s, episode steps: 11, steps per second: 147, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.134 [-2.248, 1.357], loss: 2.874215, mean_absolute_error: 6.541881, mean_q: 12.021461\n",
      " 2556/5000: episode: 215, duration: 0.062s, episode steps: 11, steps per second: 177, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.136 [-1.985, 1.193], loss: 3.765182, mean_absolute_error: 6.633307, mean_q: 12.147107\n",
      " 2566/5000: episode: 216, duration: 0.056s, episode steps: 10, steps per second: 180, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.133 [-1.963, 1.136], loss: 5.192221, mean_absolute_error: 6.747353, mean_q: 12.146761\n",
      " 2576/5000: episode: 217, duration: 0.053s, episode steps: 10, steps per second: 188, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.121 [-1.971, 1.190], loss: 3.386750, mean_absolute_error: 6.381106, mean_q: 11.635524\n",
      " 2588/5000: episode: 218, duration: 0.079s, episode steps: 12, steps per second: 151, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.091 [-1.868, 1.223], loss: 4.085304, mean_absolute_error: 6.722897, mean_q: 12.218880\n",
      " 2599/5000: episode: 219, duration: 0.103s, episode steps: 11, steps per second: 107, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.131 [-1.909, 1.136], loss: 3.032486, mean_absolute_error: 6.547544, mean_q: 12.035275\n",
      " 2612/5000: episode: 220, duration: 0.106s, episode steps: 13, steps per second: 123, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.093 [-1.663, 1.028], loss: 2.363704, mean_absolute_error: 6.526273, mean_q: 12.148350\n",
      " 2625/5000: episode: 221, duration: 0.078s, episode steps: 13, steps per second: 167, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.091 [-1.671, 0.977], loss: 3.502956, mean_absolute_error: 6.499303, mean_q: 11.970678\n",
      " 2639/5000: episode: 222, duration: 0.086s, episode steps: 14, steps per second: 163, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.079 [-1.321, 0.836], loss: 4.471428, mean_absolute_error: 6.717425, mean_q: 12.143059\n",
      " 2654/5000: episode: 223, duration: 0.088s, episode steps: 15, steps per second: 171, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.084 [-1.374, 0.796], loss: 4.679657, mean_absolute_error: 6.631737, mean_q: 11.814402\n",
      " 2667/5000: episode: 224, duration: 0.075s, episode steps: 13, steps per second: 174, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.105 [-1.648, 1.012], loss: 2.782213, mean_absolute_error: 6.418673, mean_q: 11.738561\n",
      " 2679/5000: episode: 225, duration: 0.092s, episode steps: 12, steps per second: 131, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.112 [-1.471, 0.814], loss: 3.986200, mean_absolute_error: 6.462481, mean_q: 11.643109\n",
      " 2690/5000: episode: 226, duration: 0.086s, episode steps: 11, steps per second: 128, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.120 [-1.831, 1.014], loss: 3.240324, mean_absolute_error: 6.570967, mean_q: 12.028051\n",
      " 2700/5000: episode: 227, duration: 0.080s, episode steps: 10, steps per second: 125, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: -0.131 [-1.684, 0.951], loss: 4.455791, mean_absolute_error: 6.696135, mean_q: 12.087240\n",
      " 2710/5000: episode: 228, duration: 0.086s, episode steps: 10, steps per second: 116, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: -0.136 [-1.504, 0.758], loss: 3.721176, mean_absolute_error: 6.633311, mean_q: 12.029528\n",
      " 2721/5000: episode: 229, duration: 0.085s, episode steps: 11, steps per second: 129, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.096 [-1.714, 1.017], loss: 3.285443, mean_absolute_error: 6.638079, mean_q: 12.138489\n",
      " 2730/5000: episode: 230, duration: 0.071s, episode steps: 9, steps per second: 126, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.132 [-1.725, 0.993], loss: 4.432199, mean_absolute_error: 6.701035, mean_q: 12.095945\n",
      " 2743/5000: episode: 231, duration: 0.099s, episode steps: 13, steps per second: 132, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.111 [-1.893, 1.138], loss: 3.958916, mean_absolute_error: 6.645710, mean_q: 12.036950\n",
      " 2756/5000: episode: 232, duration: 0.095s, episode steps: 13, steps per second: 137, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.094 [-1.679, 1.009], loss: 3.948625, mean_absolute_error: 6.580094, mean_q: 11.914824\n",
      " 2766/5000: episode: 233, duration: 0.062s, episode steps: 10, steps per second: 161, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: -0.121 [-1.624, 0.942], loss: 2.311200, mean_absolute_error: 6.448900, mean_q: 11.979321\n",
      " 2779/5000: episode: 234, duration: 0.070s, episode steps: 13, steps per second: 185, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.099 [-1.788, 0.985], loss: 2.841759, mean_absolute_error: 6.622324, mean_q: 12.211271\n",
      " 2792/5000: episode: 235, duration: 0.077s, episode steps: 13, steps per second: 169, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.082 [-1.720, 1.013], loss: 3.928352, mean_absolute_error: 6.668577, mean_q: 12.211380\n",
      " 2804/5000: episode: 236, duration: 0.061s, episode steps: 12, steps per second: 195, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.124 [-1.522, 0.814], loss: 2.476702, mean_absolute_error: 6.437366, mean_q: 11.944969\n",
      " 2815/5000: episode: 237, duration: 0.057s, episode steps: 11, steps per second: 194, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.129 [-1.422, 0.771], loss: 3.312952, mean_absolute_error: 6.647061, mean_q: 12.142227\n",
      " 2831/5000: episode: 238, duration: 0.086s, episode steps: 16, steps per second: 185, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.562 [0.000, 1.000], mean observation: -0.073 [-1.201, 0.620], loss: 2.692931, mean_absolute_error: 6.548765, mean_q: 12.064059\n",
      " 2850/5000: episode: 239, duration: 0.133s, episode steps: 19, steps per second: 143, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.579 [0.000, 1.000], mean observation: -0.061 [-1.202, 0.593], loss: 3.138408, mean_absolute_error: 6.502874, mean_q: 11.947060\n",
      " 2864/5000: episode: 240, duration: 0.091s, episode steps: 14, steps per second: 153, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.119 [-1.102, 0.417], loss: 2.985841, mean_absolute_error: 6.473761, mean_q: 11.831871\n",
      " 2883/5000: episode: 241, duration: 0.137s, episode steps: 19, steps per second: 139, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.081 [-1.046, 0.424], loss: 2.864349, mean_absolute_error: 6.528690, mean_q: 12.012763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2899/5000: episode: 242, duration: 0.118s, episode steps: 16, steps per second: 136, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.562 [0.000, 1.000], mean observation: -0.092 [-1.086, 0.446], loss: 3.687540, mean_absolute_error: 6.603423, mean_q: 11.977713\n",
      " 2917/5000: episode: 243, duration: 0.094s, episode steps: 18, steps per second: 191, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: -0.065 [-1.202, 0.635], loss: 3.221349, mean_absolute_error: 6.517032, mean_q: 11.877217\n",
      " 2943/5000: episode: 244, duration: 0.142s, episode steps: 26, steps per second: 183, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: -0.065 [-1.046, 0.560], loss: 3.671613, mean_absolute_error: 6.567753, mean_q: 11.895388\n",
      " 2960/5000: episode: 245, duration: 0.091s, episode steps: 17, steps per second: 187, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: -0.122 [-1.209, 0.751], loss: 2.811395, mean_absolute_error: 6.519621, mean_q: 11.928540\n",
      " 2988/5000: episode: 246, duration: 0.141s, episode steps: 28, steps per second: 198, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.536 [0.000, 1.000], mean observation: -0.055 [-1.128, 0.446], loss: 3.356708, mean_absolute_error: 6.472095, mean_q: 11.786077\n",
      " 3009/5000: episode: 247, duration: 0.126s, episode steps: 21, steps per second: 167, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.055 [-1.258, 0.638], loss: 2.287861, mean_absolute_error: 6.508817, mean_q: 12.067143\n",
      " 3025/5000: episode: 248, duration: 0.097s, episode steps: 16, steps per second: 165, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.562 [0.000, 1.000], mean observation: -0.106 [-1.073, 0.366], loss: 3.279590, mean_absolute_error: 6.564952, mean_q: 12.039150\n",
      " 3039/5000: episode: 249, duration: 0.090s, episode steps: 14, steps per second: 155, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.116 [-1.086, 0.617], loss: 2.353850, mean_absolute_error: 6.470603, mean_q: 11.997444\n",
      " 3058/5000: episode: 250, duration: 0.100s, episode steps: 19, steps per second: 190, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.080 [-1.008, 0.593], loss: 2.883067, mean_absolute_error: 6.511765, mean_q: 11.980311\n",
      " 3077/5000: episode: 251, duration: 0.105s, episode steps: 19, steps per second: 182, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.067 [-1.024, 0.450], loss: 3.531502, mean_absolute_error: 6.568380, mean_q: 11.956156\n",
      " 3108/5000: episode: 252, duration: 0.156s, episode steps: 31, steps per second: 199, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: -0.035 [-1.043, 0.412], loss: 3.382913, mean_absolute_error: 6.544209, mean_q: 11.903405\n",
      " 3138/5000: episode: 253, duration: 0.150s, episode steps: 30, steps per second: 200, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.065 [-0.851, 0.375], loss: 2.626971, mean_absolute_error: 6.454147, mean_q: 11.934304\n",
      " 3165/5000: episode: 254, duration: 0.132s, episode steps: 27, steps per second: 205, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: -0.066 [-1.031, 0.550], loss: 2.592462, mean_absolute_error: 6.531339, mean_q: 12.085938\n",
      " 3183/5000: episode: 255, duration: 0.094s, episode steps: 18, steps per second: 192, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: -0.110 [-1.305, 0.574], loss: 2.293262, mean_absolute_error: 6.526255, mean_q: 12.171772\n",
      " 3204/5000: episode: 256, duration: 0.108s, episode steps: 21, steps per second: 195, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: -0.107 [-0.961, 0.364], loss: 3.260826, mean_absolute_error: 6.609937, mean_q: 12.087214\n",
      " 3279/5000: episode: 257, duration: 0.374s, episode steps: 75, steps per second: 201, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.507 [0.000, 1.000], mean observation: 0.073 [-0.878, 0.589], loss: 2.385535, mean_absolute_error: 6.522839, mean_q: 12.107714\n",
      " 3309/5000: episode: 258, duration: 0.175s, episode steps: 30, steps per second: 171, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.134 [-0.662, 0.176], loss: 2.280657, mean_absolute_error: 6.606721, mean_q: 12.338265\n",
      " 3404/5000: episode: 259, duration: 0.517s, episode steps: 95, steps per second: 184, episode reward: 95.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.075 [-0.828, 0.469], loss: 2.320404, mean_absolute_error: 6.679338, mean_q: 12.478571\n",
      " 3494/5000: episode: 260, duration: 0.475s, episode steps: 90, steps per second: 189, episode reward: 90.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.113 [-0.452, 1.043], loss: 2.417112, mean_absolute_error: 6.835607, mean_q: 12.814944\n",
      " 3538/5000: episode: 261, duration: 0.218s, episode steps: 44, steps per second: 202, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: -0.113 [-0.774, 0.246], loss: 2.740054, mean_absolute_error: 6.984859, mean_q: 12.966206\n",
      " 3607/5000: episode: 262, duration: 0.337s, episode steps: 69, steps per second: 205, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.087 [-0.776, 0.283], loss: 2.529642, mean_absolute_error: 7.071312, mean_q: 13.286716\n",
      " 3643/5000: episode: 263, duration: 0.177s, episode steps: 36, steps per second: 203, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.133 [-0.725, 0.213], loss: 2.743458, mean_absolute_error: 7.139218, mean_q: 13.380753\n",
      " 3682/5000: episode: 264, duration: 0.199s, episode steps: 39, steps per second: 196, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.487 [0.000, 1.000], mean observation: -0.047 [-0.946, 0.265], loss: 3.152612, mean_absolute_error: 7.226187, mean_q: 13.458289\n",
      " 3709/5000: episode: 265, duration: 0.147s, episode steps: 27, steps per second: 184, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.103 [-0.789, 0.370], loss: 3.508181, mean_absolute_error: 7.158151, mean_q: 13.194569\n",
      " 3742/5000: episode: 266, duration: 0.179s, episode steps: 33, steps per second: 184, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.108 [-0.805, 0.203], loss: 2.421988, mean_absolute_error: 7.071038, mean_q: 13.254139\n",
      " 3780/5000: episode: 267, duration: 0.189s, episode steps: 38, steps per second: 201, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.072 [-0.811, 0.411], loss: 2.644791, mean_absolute_error: 7.252306, mean_q: 13.606168\n",
      " 3806/5000: episode: 268, duration: 0.133s, episode steps: 26, steps per second: 196, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.119 [-0.877, 0.184], loss: 2.366748, mean_absolute_error: 7.239980, mean_q: 13.659220\n",
      " 3840/5000: episode: 269, duration: 0.173s, episode steps: 34, steps per second: 197, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.108 [-0.813, 0.165], loss: 2.369267, mean_absolute_error: 7.314596, mean_q: 13.774647\n",
      " 3867/5000: episode: 270, duration: 0.135s, episode steps: 27, steps per second: 200, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.125 [-0.839, 0.384], loss: 2.801316, mean_absolute_error: 7.377723, mean_q: 13.846703\n",
      " 3892/5000: episode: 271, duration: 0.131s, episode steps: 25, steps per second: 190, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.123 [-0.793, 0.176], loss: 3.352790, mean_absolute_error: 7.467466, mean_q: 13.879291\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3934/5000: episode: 272, duration: 0.424s, episode steps: 42, steps per second: 99, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.093 [-0.734, 0.393], loss: 2.656617, mean_absolute_error: 7.399725, mean_q: 13.867351\n",
      " 3967/5000: episode: 273, duration: 0.243s, episode steps: 33, steps per second: 136, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.131 [-0.807, 0.172], loss: 3.119757, mean_absolute_error: 7.512783, mean_q: 14.037930\n",
      " 4020/5000: episode: 274, duration: 0.350s, episode steps: 53, steps per second: 152, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: -0.031 [-1.061, 0.390], loss: 3.568189, mean_absolute_error: 7.499327, mean_q: 13.872531\n",
      " 4036/5000: episode: 275, duration: 0.110s, episode steps: 16, steps per second: 145, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.115 [-0.862, 0.359], loss: 2.722244, mean_absolute_error: 7.406682, mean_q: 13.762993\n",
      " 4122/5000: episode: 276, duration: 0.682s, episode steps: 86, steps per second: 126, episode reward: 86.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: -0.167 [-1.050, 0.352], loss: 2.474828, mean_absolute_error: 7.574805, mean_q: 14.276675\n",
      " 4185/5000: episode: 277, duration: 0.354s, episode steps: 63, steps per second: 178, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.097 [-0.238, 0.716], loss: 2.451457, mean_absolute_error: 7.663356, mean_q: 14.421515\n",
      " 4240/5000: episode: 278, duration: 0.261s, episode steps: 55, steps per second: 211, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.473 [0.000, 1.000], mean observation: -0.121 [-0.776, 0.268], loss: 2.265978, mean_absolute_error: 7.786916, mean_q: 14.742762\n",
      " 4290/5000: episode: 279, duration: 0.267s, episode steps: 50, steps per second: 187, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.540 [0.000, 1.000], mean observation: 0.133 [-0.352, 0.738], loss: 2.655581, mean_absolute_error: 7.734885, mean_q: 14.563667\n",
      " 4344/5000: episode: 280, duration: 0.270s, episode steps: 54, steps per second: 200, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.537 [0.000, 1.000], mean observation: 0.141 [-0.241, 0.862], loss: 2.598590, mean_absolute_error: 7.896990, mean_q: 14.926395\n",
      " 4391/5000: episode: 281, duration: 0.265s, episode steps: 47, steps per second: 178, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.079 [-0.965, 0.425], loss: 2.247560, mean_absolute_error: 7.847884, mean_q: 14.873913\n",
      " 4461/5000: episode: 282, duration: 0.464s, episode steps: 70, steps per second: 151, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.133 [-0.341, 0.864], loss: 2.841875, mean_absolute_error: 7.938850, mean_q: 14.943401\n",
      " 4502/5000: episode: 283, duration: 0.229s, episode steps: 41, steps per second: 179, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: -0.086 [-0.841, 0.270], loss: 3.943485, mean_absolute_error: 8.038993, mean_q: 14.990407\n",
      " 4535/5000: episode: 284, duration: 0.176s, episode steps: 33, steps per second: 187, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.130 [-0.699, 0.436], loss: 2.167590, mean_absolute_error: 8.008014, mean_q: 15.210993\n",
      " 4578/5000: episode: 285, duration: 0.220s, episode steps: 43, steps per second: 196, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.535 [0.000, 1.000], mean observation: 0.121 [-0.324, 0.736], loss: 2.858130, mean_absolute_error: 8.091520, mean_q: 15.245408\n",
      " 4618/5000: episode: 286, duration: 0.189s, episode steps: 40, steps per second: 211, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.089 [-0.721, 0.427], loss: 2.790149, mean_absolute_error: 8.092901, mean_q: 15.274287\n",
      " 4654/5000: episode: 287, duration: 0.192s, episode steps: 36, steps per second: 188, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.118 [-0.737, 0.208], loss: 2.271882, mean_absolute_error: 8.197687, mean_q: 15.605164\n",
      " 4702/5000: episode: 288, duration: 0.251s, episode steps: 48, steps per second: 191, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: -0.116 [-0.766, 0.212], loss: 2.923867, mean_absolute_error: 8.275345, mean_q: 15.655459\n",
      " 4731/5000: episode: 289, duration: 0.174s, episode steps: 29, steps per second: 167, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.139 [-0.716, 0.164], loss: 3.395619, mean_absolute_error: 8.345422, mean_q: 15.710187\n",
      " 4772/5000: episode: 290, duration: 0.325s, episode steps: 41, steps per second: 126, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.144 [-0.736, 0.398], loss: 3.140575, mean_absolute_error: 8.303047, mean_q: 15.650656\n",
      " 4833/5000: episode: 291, duration: 0.322s, episode steps: 61, steps per second: 189, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: 0.088 [-0.348, 1.225], loss: 2.859546, mean_absolute_error: 8.382624, mean_q: 15.923442\n",
      " 4923/5000: episode: 292, duration: 0.551s, episode steps: 90, steps per second: 163, episode reward: 90.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: 0.114 [-0.502, 0.938], loss: 2.779344, mean_absolute_error: 8.542645, mean_q: 16.227007\n",
      " 4955/5000: episode: 293, duration: 0.162s, episode steps: 32, steps per second: 197, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.148 [-0.758, 0.155], loss: 2.791404, mean_absolute_error: 8.606033, mean_q: 16.325285\n",
      "done, took 31.755 seconds\n"
     ]
    }
   ],
   "source": [
    "history = dqn.fit(env, nb_steps=NB_STEPS, visualize=True, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEKCAYAAAAIO8L1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJzt3Xl4VOX1wPHvyWSBsAeCouyKCG6A\nVFFaoForauuOWrVa9+2nrda61Srdq9altS4oKIu44QKoKKKACIqA7FtYZU0gAUJIQrbJ+f1x7x0m\nyWQyJJlscz7Pkyczd7b3ZiZz7jnnvfeKqmKMMcaUF1ffAzDGGNMwWYAwxhgTkgUIY4wxIVmAMMYY\nE5IFCGOMMSFZgDDGGBOSBQhjjDEhWYAwxhgTkgUIY4wxIcXX9wBqokOHDtq9e/f6HoYxxjQq33//\nfZaqplZ1v0YdILp3786iRYvqexjGGNOoiMiWSO5nJSZjjDEhWYAwxhgTkgUIY4wxIVmAMMYYE5IF\nCGOMMSFZgDDGGBOSBQhjjDEhWYAwMe2DNR+wO293fQ/DmAbJAoSJWfnF+Vz+7uVMWDahvodiTINk\nAcLErGJ/MYpSXFpc30MxpkGyAGFiVqmWAqCq9TwSYxomCxAmZvnVDxwKFMaYsixAmJjlL7UAYUw4\nFiBMzPIyCMVKTMaEYgHCxCzLIIwJzwKEiVnWgzAmPAsQJmbZLCZjwrMAYWKWlZiMCc8ChIlZ1qQ2\nJjwLECZmWQZhTHhRCxAi8pqI7BaRlSFuu19EVEQ6uNdFRP4rIhtEZLmIDIjWuIzxWJPamPCimUGM\nBYaXXygiXYBzgK1Bi88Derk/twIvRXFcxgCHMghrUhsTWtQChKrOAfaGuOlZ4AEoU/i9CBivjvlA\nWxHpFK2xGQOHMgfLIIwJrU57ECJyIbBDVZeVu+loYFvQ9e3uslDPcauILBKRRZmZmVEaqYkFVmIy\nJrw6CxAikgz8EXgs1M0hloXM+1X1FVUdqKoDU1NTa3OIJsYESkw2i8mYkOLr8LWOAXoAy0QEoDOw\nWEROw8kYugTdtzOwsw7HZmKQZRDGhFdnGYSqrlDVjqraXVW74wSFAaqaAUwFrnNnMw0C9qtqel2N\nzcQmm+ZqTHjRnOb6FvAt0FtEtovITWHuPg3YBGwAXgXujNa4jPEEdpSzWUzGhBS1EpOq/qqK27sH\nXVbgrmiNxZhQbBaTMeHZntQmZlmT2pjwLECYmGVNamPCswBhYpY1qY0JzwKEiVnWpDYmPAsQJmYF\nMggsgzAmFAsQJmZZD8KY8CxAmJhlpxw1JjwLECZmWZPamPAsQJiYZSUmY8KzAGFilu0oZ0x4FiBM\nzLIMwpjwLECYmGWnHDUmPAsQJmbZwfqMCc8ChIlZVmIyJjwLECZmWZPamPAsQJiYZRmEMeFZgDAx\ny3aUMyY8CxAmZtnRXI0JL5rnpH5NRHaLyMqgZU+JyFoRWS4iH4pI26DbHhaRDSKSJiLnRmtcxnhs\nFpMx4UUzgxgLDC+3bAZwoqqeDKwDHgYQkb7AVcAJ7mNeFBFfFMdmjJWYjKlC1AKEqs4B9pZb9rmq\nlrhX5wOd3csXAW+raqGqbgY2AKdFa2zGQFCJyWYxGRNSffYgbgQ+dS8fDWwLum27u8yYqLEMwpjw\n6iVAiMgfgRJgorcoxN1CbtaJyK0iskhEFmVmZkZriCYG2DRXY8Kr8wAhItcDvwCu0UPTR7YDXYLu\n1hnYGerxqvqKqg5U1YGpqanRHaxp0uxYTMaEV6cBQkSGAw8CF6pqftBNU4GrRCRJRHoAvYAFdTk2\nE3ssgzAmvPhoPbGIvAUMAzqIyHbgcZxZS0nADBEBmK+qt6vqKhF5F1iNU3q6S9X97zUmSgKnHLUm\ntTEhRS1AqOqvQiweE+b+fwf+Hq3xGFOeNamNCc/2pDYxy0pMxoRnAcLELGtSGxOeBQgTsyyDMCY8\nCxAmZlkPwpjwLECYmFWKzWIyJhwLECZmWQZhTHgWIEzMsh6EMeFZgDAxy2YxGROeBQgTsyyDMCY8\nCxAmZgUyCGtSGxPSYQUIEWnjnv3NmEbPTjlqTHhVBggR+VJEWotIO2AF8KaIPBX9oRkTXVZiMia8\nSDKIFFXNAS4FxqlqP+Dc6A7LmOizJrUx4UUSIOJFJBUYAXwU5fEYU2csgzAmvEgCxN+Br4CtqrpA\nRHoCm6M7LGOiz3aUMya8Ks8HoapvA28HXd8EXBTNQRlTF7wMwmYxGRNapQFCRJ6Fyv9zVPW+qIzI\nmDpis5iMCS9ciWklsApoBZwBbHN/Tq/iccY0ClZiMia8SjMIVR0DICLXAENUtdi9/gLwWd0Mz5jo\nCZSYbBaTMSFFkgkcDbQIup7sLgtLRF4Tkd0isjJoWYqIzBCR9e7vdu5yEZH/isgGEVkuIgMOd0WM\nOVyWQRgTXiQB4ilgqYiMFpHRwGLgiQgeNxYYXm7ZQ8CXqtoL+NK9DnAe0Mv9uRV4KYLnN6ZGrElt\nTHhhA4SICPAJMBj41P35iaq+VtUTq+ocYG+5xRcB49zL44CLg5aPV8d8oK2IdIp4LYypBssgjAkv\n7DRXVVUR+VhVTwXer4XXO0JV093nTheRju7yo3Ea4J7t7rL0WnhNY0KyHeWMCS+SEtOCOugJSIhl\nIfN+EblVRBaJyKLMzMwoD8s0ZV5gsCa1MaFFEiB+jBMk0kRksYgsEZHF1Xy9XV7pyP29212+HegS\ndL/OwM5QT6Cqr6jqQFUdmJqaWs1hGGMlJmOqUuWe1BzqE9SGqcD1wL/c31OClv+fiLyNs5/Ffq8U\nZUy0WInJmPAiOdTGRnCmqALNIn1iEXkLGAZ0EJHtwOM4geFdEbkJ2IpzAECAacD5wAYgH7gh8lUw\npnrshEHGhFdlgBCRC4Bncco+e4CjgPXA8eEep6q/quSms0PcV4G7qhqLMbXJMghjwov0aK6DgTRV\n7YKzb8PsaA7KmLpgPQhzOGLxcxJJgChR1UwgTkREVWcAtqezafRsFpOJ1JjFY/D9xceu3F31PZQ6\nFUmA2C8iLYC5wHgReRqIvVBqmhwrMZlIvb/G2Q1s/vb59TySuhVJgLgYKAB+h1Na2gH8MopjMqZO\nWJPaROrYlGMB2LB3Qz2PpG5FMs31EuBrVd0MjInyeIypM5ZBmEh1ae3sprVx38Z6HkndiiRAHA/c\nLCJHAQuAr3ECxsrwDzOmYbMmtYmUc1i62MsgqiwxqeojqjoEOAmYDzwMLIv2wIyJNjsfhImUtzFh\nAaIcEXlIRD4GvsDJJh4Cukd5XMZEnZ1y1ETK25jYnL05ECzqy/6C/XU2hkia1FcDHXH2dn4TmKSq\n28I/xJiGz5rUJlLBX8g7Duyo13Ec+/yxjFlSN+3gSEpMJwM/B5bjzF5aKSKzozwuY6LO2yoEKzOZ\n8II/KwUlBfU2jkJ/IVn5Wew8EPJYprUukkNtHA/8BBgKnAbsAr6J8riMiSpVLVNaKtVSfOKrxxGZ\nhiw4gygpLam3cRSWFALUWYkpkllM/wHmAK8AN6lqYXSHZEz0ecEhIS6B4tJiJ0BgAcKEFpxB1GeA\nKPIX1ekYIjma67kikgh0teBgmgrvHz7B5wQI60OYcBpMBuF3MwhtIE1q92iuK4AZ7vV+IvJhtAdm\nTDR5GUR8XHyZ68aEUlsZxIyNM3h31bvVfnxdl5gimcX0F5yT+GQDqOpS4NhoDsqYaPP+wRLiEgBr\nUpvwgr+Qi/3F1X6en7/xc65878pqP76uS0yRBIhiVc0ut8z+m0yj5m0RWgZhItFQehB1XWKKpEm9\nRkSuwDncdw/gtzh7VBvTaHlbhBYgTCQaTA+iAZaY/g84FecQ3x8AhThHdjWm0QpuUoPtLGfCaygZ\nREOcxZQHPOj+ACAinXHOHW1Mo+T9g3k9CMsgTDgNJoNoSLOYRORHInKxiHRwr58gIuOpYYlJRO4V\nkVUislJE3hKRZiLSQ0S+E5H1IvKOO7XWmKjwUvXmCc0BCxAmvOAv5OLS6jepPdWdFBEoMdV3gBCR\nfwITgWuAz0Tkj8AsnCO5HlfdFxSRo4F7gIGqeiLgA64CngCeVdVewD7gpuq+hjFV8bbEmsU3A2wW\nkwmvtktM1Q0yDanEdBFwiqoeFJEUYKd7Pa2WXre5iBQDyUA6cBbOgQEBxgEjgZdq4bWMqcDbEvMC\nhGUQJpzaLjEVlBSQ6Dv8IkmgxNQAmtQFqnoQQFX3AmtrIzio6g7g38BWnMCwH/geyFZV7y+/HTg6\n1ONF5FYRWSQiizIzM2s6HBOjvC2x5vFWYjJV86ufOHG+LmsrQFSH97ltCNNce4rIB+5lAboHXUdV\nL63OC4pIO5zspAfOzneTgPNC3DVkzq+qr+AcF4qBAwdaXcBUi7cl5vUgbBaTCcdf6ifJl8TBkoO1\nEiAOFh+s1uO8zLchlJguK3f9f7X0mj8DNqtqJoAbdM4E2opIvJtFdMYpaRkTFVZiMofDr36S4p0A\nUZM9qT3VzSDqusRUaYBQ1S+j9JpbgUEikgwcBM4GFuE0wC8H3gauB6ZE6fWNOZRBuCUma1KbcLwM\nAmKrxBTJjnK1SlW/A94DFuMcBDAOp2T0IHCfiGwA2gN1c8okE5MC01ytB2Ei4GUQUDsB4mBJ4y8x\nRY2qPg48Xm7xJpwTEhkTdeWnuVqAMOH4S/2Bz0p9ZhANZhaTiIx1f/9fnYzEmDpUfkc5a1KbULLy\ns5i5eaaTQbglptrYUa66TeqGVGI6zd2p7RYRaSUirYN/6mR0xkSJZRAmEq9+/yrnvnEuRf6iWi0x\nVTuDaEAlptHAbKArsApnqqtH3eXGNEo2i8lEIr84n5LSEgpLChtEk7rBlJhU9Rn3sBfjVbWrqnYJ\n+rHgYBo1m8VkIuEFg0J/IQm+BASp1yZ1Q9pRDgBVvUVETgR+7C6ao6qrozssY6IrsCe1HazPhOF9\nEReWFOJr5iM+Lr5GASIhzjkHek0ziAZzRjkRuQt4F6ek1BWYJCJ3RntgxkSTlZhMJIIzCF+cjwRf\nQo12lPOOv1Tj/SDqe0e5ILcBp6lqLoCI/AP4BngxmgMzJpoK/YXEx8XjEx9gs5hMaN4XcUFJAT6p\neQaR6EskrzivxofaaAizmDwCBIfMYso2rI1pdLymo3cANssgTCiBDKLEySBqGiC8z1v5DCIzL5MJ\nyyZU+fgGV2ICJgDzReRREXkUJ3sYF91hGRNdhf5CkuIPBQhrUptQypSYgjKI3KLcaj2ft+VfPkBM\nXDGR6yZfx76D+8I+vq5LTFUGCFV9ErgV5xSjB4HbVfXf0R6YMdHkZRAiTjJsGYQJJfgL3csgFu5c\nSNt/tWXj3o2H/Xze56z8LKa8oryQy8ur6xJTRIfaUNWFwMIoj8WYOlM+g7AAYULxMogifxFxEkd8\nXDwb923Er352HtjJMSnHHNbzBfc0gnnXvQyhMg2xxGRMk1PoL9uDsCa1CSV4S90nPhLiEgLlpaq+\nzMM9X/lMwbvuZQiVaXAlJmOaosKSQhJ9iQhWYjKVC95SL9+krlaAqCKD8DKEyjTEWUzGNDlWYjKR\nCN5S95rUnuoECO9z1lhKTJX2IERkH6FP+ymAqmpK1EZlTJQV+YvKlphsFpMJIVQG4anOUV0DJabi\nxlFiCtek7lAnIzCmHhSWOBmEzWIy4ZQJEDXMIII/Y42lxBTulKNlRiAiKUCzoEV2zmjTaBX6C2md\n1NpKTCasCk1qX0Lg+uEGiOCt/vIBwssoqnpO7/YGM4tJRC4QkXXAduA79/fMaA/MmGjyMgibxWTC\nCVdiqkkGUX4WUyCDqKLE1GAO9x3k78BgIE1VuwDn4pwnotpEpK2IvCcia0VkjYicISIpIjJDRNa7\nv9vV5DWMCceb5mqzmEw4tdmkDs5G9uTvKXNbJCUmVW1QZ5TzlKhqJhAnIqKqM4ABNXzd/wCfqerx\nwCnAGuAh4Ev3HBRfuteNiYoKGYQ1qU0ItZlBeMGmRUILsvKzyhwV1ssowj2n1xT3ia/hlJiA/SLS\nApgLjBeRp4Fqb265pysdAowBUNUiVc0GLuLQMZ7GARdX9zWMqUogg7AmtQkjXJP6cA/77W31H936\naBQlMz8zcFskJSbvtuSEZKBuPrORBIiLgQLgdzilpR3AL2rwmj2BTOB1EVkiIqPdAHSEqqYDuL87\n1uA1jAnLjuZqIlGmSR3n7EntqW4PonPrzgBk5GYEboukxORlGa2SWjljq4M+RCQB4mFV9atqsaqO\nUdVngPtq8JrxOCWql1S1P5DHYZSTRORWEVkkIosyMzOrfoAxIRT6nT2prUltwqnNaa7eF/rRrY4G\nygaISGYx5RfnA9AqsVWFsUVLJAFieIhlF9TgNbcD21X1O/f6ezgBY5eIdAJwf+8O9WBVfUVVB6rq\nwNTU1BoMw8SyIn+Rsx+ENalNGGWa1DXtQbjZyFGtjgIqySDClJi8IBLIIOqgUV1pgBCR20RkCdBb\nRBYH/awHqn1OalXNALaJSG930dnu800FrneXXQ9Mqe5rGBNOqZZSUlpiJSZTpTrLILw9qcOUmMpn\nEHVRYgq3J/W7OLOJ/knZEtABVQ25dX8Y7gYmikgisAm4ASdYvSsiNwFbgRE1fA1jQvK20mwWk6lK\n+R5EbWQQyQnJtG3WlvQD6YCzceI9V0QlpqS6KzGF25N6H7APGCEiJwI/dm/6mkrKP5FS1aXAwBA3\nnV2T5zUmEt5Wms1iMlWJxqE2fHE+jmx5JBl5TgYRXFYKW2LymtSJDaDE5BGRu3Cyia7uz7sicme0\nB2ZMtITKICxAmFDK7wcRPIvpcA/W55WE4iTOCRBuiSl4r+rGVGLy3Aacpqq5ACLyD5zzUr8YzYEZ\nEy3BGYTNYjLhRGNPap84GcTCHc5JOoOPy9TQSkyRzGISIDhUFrvLjGmUvH9CO2GQqUo09qT2xfno\n1qYbW/dvJSM3g883fh64T9j9IIrrvsQU7nwQ8apaAkwA5ovI++5Nl3Boj2djGh1vD9gEX4I1qU1Y\n5Y/mWis9CPHRu31vikuLue3j25iaNjVwn3A9iPIZRH2XmBYAA1T1SRGZBfwEJ3O4XVUXRn1kxkRJ\ncAZhPQgTTm1lEDdNuYnN2ZsBpwdxXPvjAPh0/adl7tfQdpQLFyACZSQ3IFhQME2C11xMiEuwWUwm\nrPI9iOqeD2JxxmK2ZG9xnifOR+8Ozm5g5RvdkRxqo0ViC2ds9VliAlJFpNJDariH3DCm0fFKTHao\nDVOV2sogDhYfDHzB+8RHh+QOpDRPYe/BvWXuV1WJqXl888BMqvo+FpMPaAm0quTHmEbJ+8dO8CVY\nk9qEFfZorocxzTW/OD8wW8kX5wMIlJk8yQnJVZaYkhOSA4+v7xJTuqr+JeojMKaOBZeYrAdhwqmt\nPamD93XwPnO92/dmwY4Fgc9e22Ztq5zF1DyhOT7xVRhbtITLIGwqq2mSQpaYbBaTCaG29qT2Gsze\n8wDcO+heXv3lq4HlbZLahM8gSpwMwhtDfZeY7LAXpkkqU2KyJrWphKqW+VxU93wQqlo2QLglolOO\nPIUb+98YWN46qXXIHoT3OvVRYqo0QKjq3spuM6YxsxKTiUT5Ek5wBpHkS4o4QJQvG3kZRHltmrWp\ncN/tOdtp9c9WzN8+3ykxxTecEpMxTZLNYjKRKL+FHtyDaJ3UOuIAEZw9wKEeRPnroUpMm/dtpshf\nxMa9GwMZRF2WmCI5FpMxTYrNYjKRKP8FHJxBtE5qzb6CfRE9j3eIjMDzxJXNIBbcvIA3lr9BfnF+\nhRJTdkE24ASZ/OJ8UpqnNIwSkzFNVagSkzWpTXnhMohWSa0CmWhVymcQ5UtMpx51Ks8Of5ak+KQK\nJSYvCOUX53OwpGHNYjKmSQpVYrIMwpQXrgfRKrFVtUtM5TMIT6IvscJzls8ggktMlkEYEwU2i8lE\nIlQG4R1qw+tBRJJ5Bu8DARV7EJ4kX1KVJabk+EOzmOp7mqsxTZJXYrImtQknVA+i/5H9ObPLmfTp\n0AdFIyrzVFVi8iTFJ+FXP/5SPztydgBlA0RD21HOmCYpkEHYNFcTRvkMIk7i6NW+F/NunEdqi1Qg\nsn0hqmpSexJ9iQC8tfItOj/bmVmbZwUCRF5xXr2UmOptFpOI+IBFwA5V/YWI9ADeBlKAxcCvVfXw\nDrhuTASCzwdhs5hMZUKVmDzel3mRv4jkhORKn+PRmY+yYveKss9TWQbhSwJg+a7lALy/5v1AgNh7\ncC+KltlRrqmXmH4LrAm6/gTwrKr2AvYBN9XLqEyTV1xajE98xEmczWIylQrVpPZ4e1RXlUGMWzau\nzAmBoPIMIineCRDexsqK3SsCAWLPwT0AsbGjnIh0Bi4ARrvXBTgLeM+9yzjg4voYm2n6ivxFgWaj\nNalNZSLJIKqa6ppXlFdhWWVNai8T+SH7BwAWpy8mKz8LgN15uwHnXBCxMIvpOeABwPuvbA9ku6c4\nBdgOHF0fAzNNX7G/OLAFaD0IU5lQTWpPcIkpnNyi3ArLKisxHdHiCABWZ64OPNYrT+08sBNwjvja\npEtMIvILYLeqfh+8OMRdQ+b8InKriCwSkUWZmZlRGaNp2CYun8gn6z6p9uOLS4sD/+A2i8lUxttC\n9z4joTKIOz65gz35e0I+vthfHPKcEZWVmI5seSQA6/euD5xW1OOdWKhts7ZNvsQ0GLhQRH7AaUqf\nhZNRtBURr2neGdgZ6sGq+oqqDlTVgampqXUxXtPAPPnNk7yw8IVqP75Micma1KYSXoBoFt8MCJ1B\nTN84nVHfjwJgTeaaMvsx5BVXLC+Vf55gXoAoKS1hQKcBtE5qXeE+bZu1bdolJlV9WFU7q2p34Cpg\npqpeA8wCLnfvdj0wpa7HZhqHwpLCwzoWf3nFpRVLTNakNuV5W+iBABEigwBo16wdO3J20PfFvjww\n44HA8lDlJai8B9GxRcfAbaktUunZrmeF+zT5ElMYDwL3icgGnJ7EmHoej2mgCv2FYc+8VZVif8US\nk2UQpjxvC92bflpmFpPv0HkhcotyWbdnHQALdy4MLA/VoIbKS0y+OB+pyU5VpH3z9nRv273C67Zr\n1q5OS0z1ejRXVZ0NzHYvbwJOq8/xmMahphmEzWIykfC20ENlEPsOHjqSa3ZBNhv3bQScLMBzuCUm\ncMpMu/J20b55e5rHN3fGERQI2jRrE9jxrkmWmIypqUJ/zUtM1qQ2VQnXgxjcdTAtEloAToBIy0oD\nKNM3qKzEVFkGAYf6EB2SO3DHj+6geXxzLutzGQAtE1sSHxcfsyUmYyJSWFIY8tSMkbJpriYS4XoQ\nXdt0JfeRXI5pdwzZhdmk7XECxIGiA4H7VFZiqqwHAYcCRPvk9hzX/jjy/5jPqZ1OBZz+A9DkZzEZ\nUyNF/qLaKzHZLCZTiXAZhKdts7ZkF2QHehDZBdnc+9m9fL3l62qXmMDpQXi8Hei8ABETx2IypjpK\ntZTi0uKaNalDlZhsFpMpp0KACFEaatusLZl5mYEexI6cHcz+YTYlpSWcetSpgfu1SmwVyC4iLTF5\nygcI7zNrJSZjyvEyhxr1IIJKTCKCICF3aDKxrUKTupIMYuXulYFgsmnfJgC2H9geKDENPGogfVL7\nBB4TLoPod2Q/msc3p0e7HoFlLRJbBF4LnM9snMRZicmY8rzeQ016EMElJnC20MofktmYSDKIds3a\nBU4IdPIRJwe+tLfnbA80qWddP4uXLngp8JhwPYhh3YeR83BOmdlQ5TMIcMpMNovJmHK80lJtzWIC\nZwutsnqxiR2fbfiMCcsmBK5XaFJXkkF4+h3ZL3B5R84O8orzEIQWCS0C+1JA+AABh3oMnkCASDr0\nWo8PfZxzep5T5TrVlPUgTKPiZQ61VWICaJFgAcLAFZOu4EDRATq26Mi5x557aEc59zDclfUgwAke\nJ3U8KbA8IzeD7IJsWiS2QEQCGWucxAX2vYlUqAzikZ88cljPUV2WQZhGxcsgvFMzVkeoElP500Ka\n2NOlTRcA7vnsHuBQD6JTy060SGgRyCSCeV/aXdp0KdNYVpQNezcE9pXwNkjC9R8qEypA1BULEKZR\nCe49VDeLCFliqmTOuokd3udp3Z51bNu/LZBB3NDvBtbctSZsgOjetnuFL/B1e9bRMrElQJkM4nBZ\ngDAmQsHTW6s71bXIX2Qlphj2+cbP2Z6zvcLyfQf3cWaXMwECU1UBmic0D2QX5YUKEF6/YeO+jYEZ\nSN4GSbgprpU5rv1xjBw6kgt7X3jYj60pCxCmUQnOGqqdQfgtg4hVqspFb1/Ev7/5d4Xl2QXZDOk6\nhHbN2jH7h9mBJnX5pnGwQIBocyhAnHzEyYHba6PEFCdxPD7scVJb1P3pDSxAmEYluMRU3amuwYf7\nButBxJK84jwKSgrYun9rheV+9ZPSPIUh3YYwd9vcQAYR7kvdm47aq32vQIA4oeMJgQPtlS8xVSeD\nqE8WIEyjElxWqm4GUb5JbSWmpk9VeXDGg3y95WsAdhzYUeb27IJswMkIerbryY6cHYEmdbgMolf7\nXnx+7eeM6DsiECA6tezE8GOHA4dmQJU/9ldjYdNcTaNSJoOoZg+iQokpwUpMTd2uvF08+c2TrN+7\nHqBCDyI4QKQmp5JXnEdOYQ5Q9Vb/Occ4+yO0iWvD7afezsXHX8yu3F18uPZD5myZAwRlENUoMdUn\nCxCmUalpBqGqVmKKQRm5GQCBo65m5GZQUloSyA6CA4RXNkrPTQfCZxDBRISXfuHsMe2Vpy45/hLA\nyRx84mt0JSYLEKZRqek0V+8ft0yJKbEFhf5C/KX+RvcPbCLjBYgNezcAzkEfM3Iz6Ny6M1A2QHiH\nzvACRHW2+uPj4sl5KIfmCc0DyxJ8CY0ug2hcBTET88pMc62kSb09ZzuvLXkt5G3eQfnKl5ig8jOA\nmcbPCxDBGxU7cg71IUJmEAcOL4Mor1VSqzKPTYhLaHQbIBYgTKMSSQYxftl4bpp6E/sL9le4rdjv\nBIgy+0G4c9WtD9F0eQEiWHAfonwPAg5lELXVWE7wJTS6JnWdj1ZEuojILBFZIyKrROS37vIUEZkh\nIuvd3+3qemym4YtkRzkvMOw9uLfCbV5QCc4gvD1VrQ/RdEUaINo0a1Mmg/CJ77CPnVSZhDgrMUWi\nBPi9qvYBBgF3iUhf4CHgS1XD2S6zAAAdHklEQVTtBXzpXjemjEh2lPNmn+wr2FfhNq/EVH6aK1iJ\nqSnzsgGA1ORUknxJFQJEckIyib5EWia2JMmXRHFpca2WhBJ9iVZiqoqqpqvqYvfyAWANcDRwETDO\nvds44OK6Hptp+CLZUS6nyAkQoTIIKzHFpuAMon1ye7q37c6m7E2BZdkF2WVOyONlEd3bdq+1MViT\n+jCJSHegP/AdcISqpoMTRICOlTzmVhFZJCKLMjMz62qopoGIZJprIIM4WDGDCFVi8jIIKzE1XRm5\nGYGGcUrzFHp36B04j7SqsufgnjIHw0tpngLAT7v/tNbGkBBnPYiIiUhL4H3gd6qaE+njVPUVVR2o\nqgNTU+v+2CSmfkXSpD5Q6Jz7N1QGUVBSAITuQViJqWlYuXsl7Z9sz1c/fEXqU6nM2zqPjNwM+nRw\nTvvZrlk7jks5jvV71uMv9XP5pMuZvHYy7Zu3DzyHt6f1sO7Dam1cCT6bxRQREUnACQ4TVfUDd/Eu\nEenk3t4J2F0fYzMNWyRNai+DCBUg1matBeCYlGMCy6zE1LRMTZvK3oN7eX7B82TlZ/HG8jfIKcwJ\nHETPyyAK/YVs3b+Vr374isFdBvPMuc8EniMrPwuAod2G1tq4En2JVmKqijhTAsYAa1T1maCbpgLX\nu5evB6bU9dhMw1dYUhg4nHJ1mtRLM5YSHxdP39S+gWXWpG5aZv0wC4DpG6cDMHbZWOBQNtCuWTt6\nt+8NwLfbv2XPwT1ccvwlDDxqYOA53hvxHjf3v5lOrTrV2rhsP4jIDAZ+DZwlIkvdn/OBfwHniMh6\n4Bz3ujFlFPoLaZXUyrlcWZM6TAaxdNdS+nToU+bkLzbNteko8hcxb+s8AHKLcgGnrNghuQPXnnwt\n7Zq1o0e7HhzX/jgAPl73MQC9O/Qu8zyX9b2MVy98tVbHZvtBREBV56qqqOrJqtrP/ZmmqntU9WxV\n7eX+rvjfbWJeob+Q1kmtgcgyiFmbZ3H66NMDwWJJ+hL6d+pf5v7hSkxPzH2C30//fa2N/6YpNzF2\n6dhaez5T1qKdizhYcjCQFXrB/9qTrqVZfDPW3LWGO390Jx1bdKRNUhs+WONUuL2AEU1JvqRq75Vd\nXxpXODMxr7CkkOSEZOIkLmQPoshfFFi+OnM1Z40/iwU7FjBv6zx25e4iPTedfkf0K/OYJF8ScRIX\nssQ0btk4Xlr0UrXPPRHMX+pnwvIJTFo9qcbP1ZRNWTuFH7J/qNZjl+9aDsAvjvsFAJf3vZy//vSv\nPDD4AQCOaHkEib5ERISf9vhp4LPSo22Pmg+8Co/85BEeG/JY1F+nNlmAaCCK/EWB48+byhX5i0jy\nJZHoSwyZQXgzmOBQQxqc3sNjs5x/ziHdhpR5jIjQK6UXb654M7BHLTglp7Q9aRwsOcjCnQsPa5yh\nspH03HSKS4tJy0o7rOdqiIr9xVH5vL6/+n0ufudi7vjkjmo9Pi0rjeSEZM7qcRYAfTv05dEhj4bs\nJfzmlN8ELgfvOBktZ/U4i3OPPTfqr1ObLEA0EMPGDuO+6ffV9zAavEJ/IUnxSST5kkJu1QeO4e/O\nFjkh9QSOTTmW0UtG88riV3hw8IOcetSpFR439uKxbMvZxqMzHw0sW7FrBaVaCjjnKI7Uuj3raP9k\nez5Z90mZ5Zv3bXZ+Z2+u9smOGoKs/CyOf+F47v707lp93o17N3Lj1BtJ9CXy+cbPyxxML1Lr9q6j\nV0ovTup4EuCc3a0y5/c6H3BO8GNCswARxiNfPsKszbOi/jp5RXnM3z6fb7d/G/XXaugOFh/kxik3\nsmnfppC3F5YUkuhLrDSD8ALEES2PAJyZK/2P7M/W/Vtpk9SGx4c+HvJ5B3UexK9O/BVvLH+Dg8XO\n4Z6XZCwBnNNKfrbhM1SVzzZ8xs/G/4ybp96MqgYen5GbwbUfXEtWfhZjFo+h0F/IR+s+KvMaXtmk\nVEvZuHfjYfxV6l+Rv4jbP76dOVvmcMtHt7Bp3yZeX/p6yAMiBluWsYzbP749sAd7KMX+Yq794FqG\njRtGnMQx7epplGopoxePDnn/j9I+4qxxZ/G7z37H/O3z+dn4n3Hdh9eRW5RLWlYavTv0ZlDnQcy8\nbmYgCISS4Etg9Z2r+e7m7yL7I8SgmA4QqsrMzTPL/KN7sguy+efcf/LK4lfCPkdWfhZLM5aWWVZY\nUsjCHVWXJJZmLCWvKI8Vu1egKGl70lBVNuzdwIRlE1ixa0VE61GqpXy77duQ61GXlu9aHnKrLyM3\no8LfqDKzfpjF60tfZ9Kq0HX6Qr8zzTUpPilsgPDmsQ/uMph+Rzo9h6tOvKrM8fnL+02/37C/cD+P\nz36cHTk7WJqxlDZJbXjgzAeYt20e902/jxGTRjB/+3zGLBlD2p60wHs9adUkJq6YyCvfv8KE5RMA\nJ+tYsGNBYJzBdXXvxDWNxYMzHmTU96P425y/MWXtFIYfO5yCkgIenfkoE5ZNYMKyCazavarC4yYs\nn8Co70cFppyWt3DHQqakTWHiiol0bt2Z9694n7N7ns0lx1/CP+b+g+93fl/hMaO+H8WsH2bxn+/+\nw8jZI5m3bR4TV0zk5qk3szl7M8elHBfoMVQ1a6hPah+6tOlSvT9KDIjpAPHphk85e/zZTFs/rcJt\nyzKWAc6sl3DumnYXP3n9J4EvAVXl5o9u5rTRpwWm24XyxaYvGDBqAPd8ek/gNXIKc9iVt4vL3r2M\n6yZfx7lvnBs4wU04f5vzN8587UzGLRtX5X2jZcWuFZw++nTunX5vmeW5Rbl0eroT/Uf1jyiAeaWc\npbtCB5TCEqfElOhLDNmkPlDk9CCuOvEqwMkghnQbQnxcPLcMuCXsaw/rPoxeKb146punOPO1M3l/\nzfsM6jyIe8+4lwt6XcBz3z1Hi4QWfHy1MzVy1uZZ3DT1Jk4bfRrPzn8WgD9/9WfSc9MZ2m0oaXvS\nOH306YycPRJwAoQ3A8s7zENj8OGaD3nuu+doHt+cGZtmoCgP//hh+h3Zj/8t/B/XTb6O6yZfx/CJ\nwyv0Jbws7PWlr1d43rlb53La6NO45oNr6NiiI3N+MyfQO3j1l69yRIsjGDFpRJm+kPec3l7R0zdO\n57xjz2Pk0JG8s+odSrW0wpRVU30xHSC+3PSl83vzlxVu87Z41+1ZV+ketnvy9zB57WRyi3IDGcOH\naz/kjeVvAFR60pqdB3ZyzQfXoCjvrHqHudvmBm57e+XbLN+1nHOPOZf03HQ+3/g5f/nqLyT9LYm+\nL/StsNU8c/PMwBeQ90+YW5TLiS+eGJjCB84X68/G/4yEvyZw1yd3BZaPnD2ShL8mcPro0wP19o/X\nfcxxzx9XZj+CTfs2ccKLJ/Dp+k8rrM+BwgOMmDSCgpICvk8vu8XnNYYBtuVsK3Pb+6vfp9fzvcpk\nSt5OTqEyjoKSAnbn7aZ5fPMKJaYn5z3J0LFDAyWPhwY/RN4jeXRq1Ykfd/0x+x7cF7L3ECxO4lh8\n22KmXzud9APpHCw+yHPDnyNO4pj6q6lsvGcjG+/ZyNBuQ+ncujN/mfMXJq6YCDh9hZTmKRT5i7jy\nhCv598//HXje15e+Tv9R/Xlt6WuckHoCR7Y8kjlb5jBi0gju+fQefj/994yYNAKA/373X84Ycwb+\nUj+T106m7wt9+WLTFxzz32PYlbsLgC3ZW+j+XHe+2fZNyPUoKClg2Nhh/Gnmn7jto9u45J1LDquh\nvGHvBlKeSKHFP1owZe0UbphyAwOPGhhYp2bxzTj96NOZe8NcNty9gQ13b2DUL0axPWc7X2z6AnCm\nBw9+bTBL0pcQJ3F8lPYRH6V9RMt/tKTjUx2Zt3Ve4P+jyF/EtSddW6ZR3D65Pe9c/g7bcrZx45Qb\nUVVu//h2hr8xnJ0HdnJDvxsCwfan3X/KIz95hHN6OueG9naCMzXXuCbl1jLvy2j2D7MpLCnk2fnP\ncvVJV9O1TdfAlo+irNi9go17NzJnyxwu7H0hLRNbsufgHrbnbA98Sb2w8AUW7FjAJ+s/oXvb7gzt\nNpR3Vr1DaotU7j7tbl5Y+AL5xfncO+herpvs1EtH/WIUt318G2+ueJMebXuwOXszD8x4gCRfEuMv\nGc8JL57Abz/7LRv2bmBApwEsTl/MJ+s+YUCnAby98m2uOvEqrn7/ao7vcDyX9rmUv3/9dzbu3cjX\nW79mVeYq3ln1Dpf2uZSXFr7EpNWTmPXDLAZ0GsCYJWP421l/IzkhmecXPE+7Zu1YsGMBLy96mSJ/\nEVPSprB+73qe/fZZ8orzSPQlcqDwAKszV3Pth9ey8JaFTF47mStPuJK3Vr7F5LWTWb93Pef3Op9p\n66cxftl4OrboyPBjh/PRuo/okNwhUIrr2qYrG/du5Olvn2bC8gnkFuUyYtIIvr7ha/45958sTl9M\nq8RWpGWlkVuUy6vfv8q5x55L39S+3Df9PtJz07nulOt45MtHmLx2Mrd9dBs/7vpjnvn2GXbl7aJr\nm66Ac1x/bw48QMvElhF9JlomtuTnx/ycT6/5lOSEZI7vcDzgBI+e7XoG7jes+zDeWP4G5/Q8h06t\nOjF+2Xie/NmT+OJ8XN73cloktODlC17Gr37umnZX4GiipVrKnQPv5LHZTuCMj4snIS6BgyUHSctK\n4+lvn2br/q3M3DyTp799mjVZa7h3+r1s2reJzzd+zq9P+TVjloxhy/4tPL/gec7ofAZPf/s0l/a5\nlJ7tevLyopd5d9W7fLXlK77a8lVgvBe9fRG/OvFXXHPyNYBzvKIXF75I1zZd+cOZfyizh+/oxaPJ\nKcyhdVJrrp98PfsL9/Pi+S8GDmB3ZpcznYkCJAUOWdK5dWce+uIhxiwZA8DDXz6M4mSMt516G6O+\nH8WvP/w18XHxtEpqxYhJI8gpzOE3/X7DsG7DuOj4iyq8F2d0OYN/nf0v7p9xP0PGDmHu1kMbUqce\ndSpDug3h43UfM6z7MHxxPt667C3eXfVulRsCJnJS33Xrmhg4cKAuWrTosB+3J38PK3av4KxxZ9G2\nWVuyC7K59uRrmbB8Av2P7M+LF7zIjVNuRERYnbmaJ372BCNnj+RgyUGObnU08XHxbN2/lRaJLTjl\niFPYun9rma3jkUNHcmHvC7nknUvYlrONVomtyCnMIT4unuSEZPYX7mf8xeO59uRrOW/ieSzbtYw/\nDfkTd01ztuxv6n8Toy8czVPznuKZ+c8w8KiBvH3Z2/R6vhcndDwh8GXbJqkNxaXFLLh5AW2ataHb\nc93440/+yJwtc/hqy1d0bNGRhbcspPtz3WnTrA1/OPMPnHfseQx4ZQB/GvIn2jVrx32f38eHV37I\n9ZOvD9TvgyXEJeBXP6Vayo+O+hFrs9YSJ3HsL9zP4C6DmbdtHu2bt+fxoY9zTMoxXPDmBQAc1eoo\nvrnxG7r/pzt//elfeWzWY846nnYXD37xIBOWTeCUI0/hjoF3cMtHtwSeq1ubbtzU/yYem/0Yl/a5\nlA/WfEC3Nt24d9C9/G767/jDmX/gyXOeJO7PcShKs/hmgQPwBTvw8IGIg0J1TN8wnb/M+QsfXvkh\n6QfSueWjW/jk6k9IbVH2AJKFJYVc8s4lXHL8JUxaPYnrT7meq068it9M+Q3+Uj9vrXwrcF/vbwDO\nl3D5DOHGfjdy/5n3M3zicLbu30qSL4n3rniPX771S3598q85q8dZ3DDlBjokd8AnPnblORlH59ad\nySvKY1/BPj7+1ccM6jyI/qP6k5mfSUFJASOHjuSxoY+xK28XHZI70PXZrgw8aiA92/XkP9/9hxM7\nnsjy2539Cy5991JG9B3B1SddXeFv8tAXD/HEvCdoldiKbm27sTtvN7vzdvPNjd/wf5/+H4vTF3PH\nwDu49dRbufr9q8kvzufDKz+ssONiMC9zmLpuKkO7DeW91e/hVz9Zf8hizpY5vLr4VT6++uNGt4dy\nfROR71V1YJX3i8UAMWnVJK547woA/n3Ov7l/xv0AnN3j7DLlpseGPMZLi16iuLSY7IJs7j/jfv79\nrZNmt0xsSXxcPItvXcxT3zzFS4teYmi3oXyz7RvW3b0ucBz5p795mvtn3M9ff/pX+nTow+WTLg8E\ngPI6Pd2JjNwM9j+0P5A+B3v4i4f517x/lRnr2IvGcn0/5xBWw98YzoIdC9hXsI++qX1Znbmaq0+6\nmjdXvMmmezbRo52zM9CAUQMCGVKX1l3Y/NvN3P3p3YxePJozupzBt9u+5d5B9/LkN0/y5qVvsj1n\nOw988QCTr5xMkb+IK967gpaJLcktyiUhLoGdv99Jh+QOpB9I56hnjgqM95qTrmHiioksuW0JV0y6\ngvV719M8vjkiwtUnXs2rF76KqtL7f71Zv3c9Jx9xMktvW8qOAzvo+mxXFA38TYtLizmzy5nMvn42\nCb4ETnrpJFbuXsm2e7dxwZsXkJWfxTk9z2HcsnEkJyRz4OEDjeJLY9DoQeQV59G9bXc+XvcxKc1T\nuKzPZby6+FXi4+Lp0bYH6/eur/C4B858gCe/eZKU5insPbiX5vFO831Q50HM+PUMVmWu4pSXTwEg\n4/cZtE5qzRljzmBbzjb6HdmPuVvnMu/Gefz3u//yxvI3uKzvZby3+j0u73s5761+jw+u+IBjUo6h\n38v9eObcZ/jdoN9VuS6FJYUMfm0wa7PWsujWRby+5HX+u+C/ZP4hkwnLJnDntDtZeMvCMsc8OlwX\nv30xqzJXsf7uin8TE7lIAwSq2mh/Tj31VK2OnTk7ddq6afr1lq9VVXXulrk6e/Ns9Zf6dXnGcp22\nbpp+tv4zzS3M1bFLxioj0eP/d7wWFBdoyhMp2uafbTTjQIZu279NVVUPFB7QdVnrtNhfrJv2birz\nWqWlpbp692otLS1VVdW1mWu1xF8Sclzb9m/T3bm7Kx13flG+Tt8wXZdnLFd/qV/XZK4pc/vbK95W\nRqJ9X+iryzOWKyNRRqLDxg4rc7/t+7frtHXTdNq6aYHxFpYU6g/7fgisQ2lpqa7fsz6wDt5lbx1m\nbpqpjEQvfefSMuva8amO2vmZztr+ifbKSLTdv9qpv9SvP3rlR8pItNnfmikj0Xlb5wUe9/c5f1dG\nos9++2xg2aIdi/SLjV9oUUmRrstap5+t/0zzivICt6cfSNeMAxmqqppbmKs7cnZoTkGOfrr+U125\na2Wlf8OGZlfuLt2Zs1Oz8rJ02rppmpaVpgcKD+hn6z/TpelL9ZEvHlFGor9885fKSHTwmME6a/Ms\nLS0t1Vun3qqMRPu/3F8ZiR7x1BGafiBdVVX9pX5t/0R77ftC38BrpWWlaat/tFJGov/77n+q6vzt\n+vyvT5n35vyJ56u/1K+qqit3raz08xrKgcIDuiV7i6qqFhQXaFpWWmA8K3atqPHfa9/Bfbo1e2uN\nnyfWAYs0gu/Yev+Sr8lPdQPE4Xr+u+d11uZZqup8CU9cPrFOXvdwFRQX6GMzH9P1e9ZraWmpPvzF\nwzri3RFlvoxrS2lpqT765aO6PGN5meVjl4zVqWun6sTlE/Xydy/XVxa9oqqq87bO079+9VddtGOR\n/n3O3wMBU1U1Ky9Lf/vpbzX7YHatj7Ox25K9RR/4/AHdkbND75l2j+7J3xO4Lb8oX//45R91877N\n+s+v/6kLdyws89jxS8fr5DWTyyz7ctOX+q+v/1Xm778ua50+8PkDmpaVpvdPv1+z8rKiu1Km3kUa\nIGKyxGSMMbEs0hJTwy/SGmOMqRcWIIwxxoRkAcIYY0xIFiCMMcaEZAHCGGNMSBYgjDHGhGQBwhhj\nTEgWIIwxxoTUqHeUE5FMYEs1H94ByKrF4TQETW2dbH0aNlufhi3c+nRT1dRKbgto1AGiJkRkUSR7\nEjYmTW2dbH0aNlufhq021sdKTMYYY0KyAGGMMSakWA4Qr9T3AKKgqa2TrU/DZuvTsNV4fWK2B2GM\nMSa8WM4gjDHGhBGTAUJEhotImohsEJGH6ns81SEiP4jIChFZKiKL3GUpIjJDRNa7v9vV9zgrIyKv\nichuEVkZtCzk+MXxX/f9Wi4iA+pv5KFVsj4jRWSH+x4tFZHzg2572F2fNBE5t35GXTkR6SIis0Rk\njYisEpHfussb5XsUZn0a5XskIs1EZIGILHPX58/u8h4i8p37/rwjIonu8iT3+gb39u4RvVAkZxVq\nSj+AD9gI9AQSgWVA3/oeVzXW4wegQ7llTwIPuZcfAp6o73GGGf8QYACwsqrxA+cDnwICDAK+q+/x\nR7g+I4H7Q9y3r/u5SwJ6uJ9HX32vQ7kxdgIGuJdbAevccTfK9yjM+jTK98j9O7d0LycA37l/93eB\nq9zlLwN3uJfvBF52L18FvBPJ68RiBnEasEFVN6lqEfA2cFE9j6m2XASMcy+PAy6ux7GEpapzgL3l\nFlc2/ouA8eqYD7QVkU51M9LIVLI+lbkIeFtVC1V1M7AB53PZYKhquqoudi8fANYAR9NI36Mw61OZ\nBv0euX/nXPdqgvujwFnAe+7y8u+P9769B5wtIlLV68RigDga2BZ0fTvhPygNlQKfi8j3InKru+wI\nVU0H5x8C6Fhvo6ueysbfmN+z/3NLLq8Flfwa1fq45Yj+OFupjf49Krc+0EjfIxHxichSYDcwAyfL\nyVbVEvcuwWMOrI97+36gfVWvEYsBIlTUbIxTuQar6gDgPOAuERlS3wOKosb6nr0EHAP0A9KBp93l\njWZ9RKQl8D7wO1XNCXfXEMsa3DqFWJ9G+x6pql9V+wGdcbKbPqHu5v6u1vrEYoDYDnQJut4Z2FlP\nY6k2Vd3p/t4NfIjzAdnlpfXu7931N8JqqWz8jfI9U9Vd7j9xKfAqh0oUjWJ9RCQB58t0oqp+4C5u\ntO9RqPVp7O8RgKpmA7NxehBtRSTevSl4zIH1cW9vQwQl0VgMEAuBXm63PxGnYTO1nsd0WESkhYi0\n8i4DPwdW4qzH9e7drgem1M8Iq62y8U8FrnNnygwC9ntljoasXA3+Epz3CJz1ucqdWdID6AUsqOvx\nhePWp8cAa1T1maCbGuV7VNn6NNb3SERSRaSte7k58DOcvsos4HL3buXfH+99uxyYqW7HOqz67sbX\nxw/OjIt1ODW7P9b3eKox/p44MyyWAau8dcCpKX4JrHd/p9T3WMOsw1s4KX0xztbNTZWNHyc9fsF9\nv1YAA+t7/BGuzwR3vMvdf9BOQff/o7s+acB59T3+EOvzY5wSxHJgqftzfmN9j8KsT6N8j4CTgSXu\nuFcCj7nLe+IEsg3AJCDJXd7Mvb7Bvb1nJK9je1IbY4wJKRZLTMYYYyJgAcIYY0xIFiCMMcaEZAHC\nGGNMSBYgjDHGhGQBwpggIuIPOrLnUqniaL8icruIXFcLr/uDiHSo6fMYU5tsmqsxQUQkV1Vb1sPr\n/oCz70BWXb+2MZWxDMKYCLhb+E+4x+BfICLHustHisj97uV7RGS1e+C3t91lKSIy2V02X0ROdpe3\nF5HPRWSJiIwi6Fg5InKt+xpLRWSUiPjqYZWNsQBhTDnNy5WYrgy6LUdVTwP+BzwX4rEPAf1V9WTg\ndnfZn4El7rJHgPHu8seBuaraH2cP3q4AItIHuBLnYIz9AD9wTe2uojGRia/6LsbElIPuF3MobwX9\nfjbE7cuBiSIyGZjsLvsxcBmAqs50M4c2OCcYutRd/omI7HPvfzZwKrDQPVx/cxrfQRdNE2EBwpjI\naSWXPRfgfPFfCPxJRE4g/GGWQz2HAONU9eGaDNSY2mAlJmMid2XQ72+DbxCROKCLqs4CHgDaAi2B\nObglIhEZBmSpcx6C4OXnAd6Jar4ELheRju5tKSLSLYrrZEylLIMwpqzm7lm6PJ+pqjfVNUlEvsPZ\nsPpVucf5gDfc8pEAz6pqtoiMBF4XkeVAPocOufxn4C0RWQx8BWwFUNXVIvIoztkC43CODnsXsKW2\nV9SYqtg0V2MiYNNQTSyyEpMxxpiQLIMwxhgTkmUQxhhjQrIAYYwxJiQLEMYYY0KyAGGMMSYkCxDG\nGGNCsgBhjDEmpP8HJNEg5zzzstIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12916cda0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['episode_reward'], color='green')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total of Rewards')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 10 episodes ...\n",
      "Episode 1: reward: 98.000, steps: 98\n",
      "Episode 2: reward: 65.000, steps: 65\n",
      "Episode 3: reward: 61.000, steps: 61\n",
      "Episode 4: reward: 91.000, steps: 91\n",
      "Episode 5: reward: 72.000, steps: 72\n",
      "Episode 6: reward: 109.000, steps: 109\n",
      "Episode 7: reward: 53.000, steps: 53\n",
      "Episode 8: reward: 83.000, steps: 83\n",
      "Episode 9: reward: 113.000, steps: 113\n",
      "Episode 10: reward: 46.000, steps: 46\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x128e279e8>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.test(env, nb_episodes=10, visualize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
