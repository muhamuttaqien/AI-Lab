{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize a game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ConnectN import ConnectN\n",
    "\n",
    "game_setting = {'size':(6,6), 'N':4, 'pie_rule':True}\n",
    "game = ConnectN(**game_setting)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "% matplotlib notebook\n",
    "\n",
    "from Play import Play\n",
    "\n",
    "\n",
    "gameplay=Play(ConnectN(**game_setting), \n",
    "              player1=None, \n",
    "              player2=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define our policy\n",
    "\n",
    "Please go ahead and define your own policy! See if you can train it under 1000 games and with only 1000 steps of exploration in each move."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from math import *\n",
    "import numpy as np\n",
    "\n",
    "from ConnectN import ConnectN\n",
    "game_setting = {'size':(6,6), 'N':4}\n",
    "game = ConnectN(**game_setting)\n",
    "\n",
    "class Policy(nn.Module):\n",
    "\n",
    "    def __init__(self, game):\n",
    "        super(Policy, self).__init__()\n",
    "\n",
    "        # input = 6x6 board\n",
    "        # convert to 5x5x8\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=2, stride=1, bias=False)\n",
    "        # 5x5x16 to 3x3x32\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, bias=False)\n",
    "\n",
    "        self.size=3*3*32\n",
    "        \n",
    "        # the part for actions\n",
    "        self.fc_action1 = nn.Linear(self.size, self.size//4)\n",
    "        self.fc_action2 = nn.Linear(self.size//4, 36)\n",
    "        \n",
    "        # the part for the value function\n",
    "        self.fc_value1 = nn.Linear(self.size, self.size//6)\n",
    "        self.fc_value2 = nn.Linear(self.size//6, 1)\n",
    "        self.tanh_value = nn.Tanh()\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        y = F.leaky_relu(self.conv1(x))\n",
    "        y = F.leaky_relu(self.conv2(y))\n",
    "        y = y.view(-1, self.size)\n",
    "        \n",
    "        # action head\n",
    "        a = self.fc_action2(F.leaky_relu(self.fc_action1(y)))\n",
    "        \n",
    "        avail = (torch.abs(x.squeeze())!=1).type(torch.FloatTensor)\n",
    "        avail = avail.view(-1, 36)\n",
    "        maxa = torch.max(a)\n",
    "        exp = avail*torch.exp(a-maxa)\n",
    "        prob = exp/torch.sum(exp)\n",
    "        \n",
    "        # value head\n",
    "        value = self.tanh_value(self.fc_value2(F.leaky_relu( self.fc_value1(y) )))\n",
    "        return prob.view(6,6), value\n",
    "\n",
    "policy = Policy(game)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a MCTS player for Play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MCTS\n",
    "\n",
    "from copy import copy\n",
    "\n",
    "def Policy_Player_MCTS(game):\n",
    "    mytree = MCTS.Node(copy(game))\n",
    "    for _ in range(1000):\n",
    "        mytree.explore(policy)\n",
    "       \n",
    "    mytreenext, (v, nn_v, p, nn_p) = mytree.next(temperature=0.1)\n",
    "    \n",
    "    return mytreenext.game.last_move\n",
    "\n",
    "import random\n",
    "\n",
    "def Random_Player(game):\n",
    "    return random.choice(game.available_moves())    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Play a game against a random policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "% matplotlib notebook\n",
    "\n",
    "from Play import Play\n",
    "\n",
    "\n",
    "gameplay=Play(ConnectN(**game_setting), \n",
    "              player1=Policy_Player_MCTS, \n",
    "              player2=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize our alphazero agent and optimizer\n",
    "import torch.optim as optim\n",
    "\n",
    "game=ConnectN(**game_setting)\n",
    "policy = Policy(game)\n",
    "optimizer = optim.Adam(policy.parameters(), lr=.01, weight_decay=1.e-5)\n",
    "\n",
    "! pip install progressbar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beware, training is **VERY VERY** slow!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train our agent\n",
    "\n",
    "from collections import deque\n",
    "import MCTS\n",
    "\n",
    "# try a higher number\n",
    "episodes = 2000\n",
    "\n",
    "import progressbar as pb\n",
    "widget = ['training loop: ', pb.Percentage(), ' ', \n",
    "          pb.Bar(), ' ', pb.ETA() ]\n",
    "timer = pb.ProgressBar(widgets=widget, maxval=episodes).start()\n",
    "\n",
    "outcomes = []\n",
    "policy_loss = []\n",
    "\n",
    "Nmax = 1000\n",
    "\n",
    "for e in range(episodes):\n",
    "\n",
    "    mytree = MCTS.Node(game)\n",
    "    logterm = []\n",
    "    vterm = []\n",
    "    \n",
    "    while mytree.outcome is None:\n",
    "        for _ in range(Nmax):\n",
    "            mytree.explore(policy)\n",
    "            if mytree.N >= Nmax:\n",
    "                break\n",
    "            \n",
    "        current_player = mytree.game.player\n",
    "        mytree, (v, nn_v, p, nn_p) = mytree.next()\n",
    "        mytree.detach_mother()\n",
    "        \n",
    "        loglist = torch.log(nn_p)*p\n",
    "        constant = torch.where(p>0, p*torch.log(p),torch.tensor(0.))\n",
    "        logterm.append(-torch.sum(loglist-constant))\n",
    "\n",
    "        vterm.append(nn_v*current_player)\n",
    "        \n",
    "    # we compute the \"policy_loss\" for computing gradient\n",
    "    outcome = mytree.outcome\n",
    "    outcomes.append(outcome)\n",
    "    \n",
    "    loss = torch.sum( (torch.stack(vterm)-outcome)**2 + torch.stack(logterm) )\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    policy_loss.append(float(loss))\n",
    "\n",
    "    optimizer.step()\n",
    "    \n",
    "    if e%10==0:\n",
    "        print(\"game: \",e+1, \", mean loss: {:3.2f}\".format(np.mean(policy_loss[-20:])),\n",
    "              \", recent outcomes: \", outcomes[-10:])\n",
    "    \n",
    "    if e%500==0:\n",
    "        torch.save(policy,'6-6-4-pie-{:d}.mypolicy'.format(e))\n",
    "    del loss\n",
    "    \n",
    "    timer.update(e+1)\n",
    "    \n",
    "timer.finish()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# setup environment to pit your AI against the challenge policy '6-6-4-pie.policy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "challenge_policy = torch.load('6-6-4-pie.policy')\n",
    "\n",
    "def Challenge_Player_MCTS(game):\n",
    "    mytree = MCTS.Node(copy(game))\n",
    "    for _ in range(1000):\n",
    "        mytree.explore(challenge_policy)\n",
    "       \n",
    "    mytreenext, (v, nn_v, p, nn_p) = mytree.next(temperature=0.1)\n",
    "    \n",
    "    return mytreenext.game.last_move\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let the game begin!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "% matplotlib notebook\n",
    "gameplay=Play(ConnectN(**game_setting), \n",
    "              player2=Policy_Player_MCTS, \n",
    "              player1=Challenge_Player_MCTS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
