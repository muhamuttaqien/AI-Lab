{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup a game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ConnectN import ConnectN\n",
    "\n",
    "game_setting = {'size':(3,3), 'N':3}\n",
    "game = ConnectN(**game_setting)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game.move((0,1))\n",
    "print(game.state)\n",
    "print(game.player)\n",
    "print(game.score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# player -1 move\n",
    "game.move((0,0))\n",
    "# player +1 move\n",
    "game.move((1,1))\n",
    "# player -1 move\n",
    "game.move((1,0))\n",
    "# player +1 move\n",
    "game.move((2,1))\n",
    "\n",
    "print(game.state)\n",
    "print(game.player)\n",
    "print(game.score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Play a game interactively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "% matplotlib notebook\n",
    "\n",
    "from Play import Play\n",
    "\n",
    "\n",
    "gameplay=Play(ConnectN(**game_setting), \n",
    "              player1=None, \n",
    "              player2=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize an AI to play the game\n",
    "We need to define a policy for tic-tac-toe, that takes the game state as input, and outputs a policy and a critic\n",
    "\n",
    "## Tentative Exercise:\n",
    "Code up your own policy for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from math import *\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "class Policy(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        # solution\n",
    "        self.conv = nn.Conv2d(1, 16, kernel_size=2, stride=1, bias=False)\n",
    "        self.size = 2*2*16\n",
    "        self.fc = nn.Linear(self.size,32)\n",
    "\n",
    "        # layers for the policy\n",
    "        self.fc_action1 = nn.Linear(32, 16)\n",
    "        self.fc_action2 = nn.Linear(16, 9)\n",
    "        \n",
    "        # layers for the critic\n",
    "        self.fc_value1 = nn.Linear(32, 8)\n",
    "        self.fc_value2 = nn.Linear(8, 1)\n",
    "        self.tanh_value = nn.Tanh()\n",
    "        '''\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        '''\n",
    "        # solution\n",
    "        y = F.relu(self.conv(x))\n",
    "        y = y.view(-1, self.size)\n",
    "        y = F.relu(self.fc(y))\n",
    "        \n",
    "        \n",
    "        # the action head\n",
    "        a = F.relu(self.fc_action1(y))\n",
    "        a = self.fc_action2(a)\n",
    "        # availability of moves\n",
    "        avail = (torch.abs(x.squeeze())!=1).type(torch.FloatTensor)\n",
    "        avail = avail.view(-1, 9)\n",
    "        \n",
    "        # locations where actions are not possible, we set the prob to zero\n",
    "        maxa = torch.max(a)\n",
    "        # subtract off max for numerical stability (avoids blowing up at infinity)\n",
    "        exp = avail*torch.exp(a-maxa)\n",
    "        prob = exp/torch.sum(exp)\n",
    "        \n",
    "        \n",
    "        # the value head\n",
    "        value = F.relu(self.fc_value1(y))\n",
    "        value = self.tanh_value(self.fc_value2(value))\n",
    "        return prob.view(3,3), value\n",
    "        '''\n",
    "\n",
    "policy = Policy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a player that uses MCTS and the expert policy + critic to play a game\n",
    "\n",
    "We've introduced a new parameter\n",
    "$T$ = temperature\n",
    "\n",
    "This tells us how to choose the next move based on the MCTS results\n",
    "\n",
    "$$p_a = \\frac{N_a^{\\frac{1}{T}}}{\\sum_a N_a^{\\frac{1}{T}}}$$\n",
    "\n",
    "$T \\rightarrow 0$, we choose action with largest $N_a$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MCTS\n",
    "\n",
    "from copy import copy\n",
    "import random\n",
    "\n",
    "def Policy_Player_MCTS(game):\n",
    "    mytree = MCTS.Node(copy(game))\n",
    "    for _ in range(50):\n",
    "        mytree.explore(policy)\n",
    "   \n",
    "    mytreenext, (v, nn_v, p, nn_p) = mytree.next(temperature=0.1)\n",
    "        \n",
    "    return mytreenext.game.last_move\n",
    "\n",
    "def Random_Player(game):\n",
    "    return random.choice(game.available_moves())    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = ConnectN(**game_setting)\n",
    "print(game.state)\n",
    "Policy_Player_MCTS(game)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Play a game against the policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "% matplotlib notebook\n",
    "\n",
    "\n",
    "gameplay=Play(ConnectN(**game_setting), \n",
    "              player1=None, \n",
    "              player2=Policy_Player_MCTS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize our alphazero agent and optimizer\n",
    "import torch.optim as optim\n",
    "\n",
    "game=ConnectN(**game_setting)\n",
    "policy = Policy()\n",
    "optimizer = optim.Adam(policy.parameters(), lr=.01, weight_decay=1.e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tenative exercise:\n",
    "code up the alphazero loss function, defined to be\n",
    "$$L = \\sum_t \\left\\{ (v^{(t)}_\\theta - z)^2  - \\sum_a p^{(t)}_a \\log \\pi_\\theta(a|s_t) \\right\\} + \\textrm{constant}$$ \n",
    "I added a constant term $\\sum_t \\sum_a p^{(t)}\\log p^{(t)}$ so that when $v_\\theta^{(t)} = z$ and $p^{(t)}_a = \\pi_\\theta(a|s_t)$, $L=0$, this way we can have some metric of progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train our agent\n",
    "\n",
    "from collections import deque\n",
    "import MCTS\n",
    "\n",
    "episodes = 400\n",
    "outcomes = []\n",
    "losses = []\n",
    "\n",
    "!pip install progressbar\n",
    "import progressbar as pb\n",
    "widget = ['training loop: ', pb.Percentage(), ' ', \n",
    "          pb.Bar(), ' ', pb.ETA() ]\n",
    "timer = pb.ProgressBar(widgets=widget, maxval=episodes).start()\n",
    "\n",
    "for e in range(episodes):\n",
    "\n",
    "    mytree = MCTS.Node(ConnectN(**game_setting))\n",
    "    vterm = []\n",
    "    logterm = []\n",
    "    \n",
    "    while mytree.outcome is None:\n",
    "        for _ in range(50):\n",
    "            mytree.explore(policy)\n",
    "\n",
    "        current_player = mytree.game.player\n",
    "        mytree, (v, nn_v, p, nn_p) = mytree.next()        \n",
    "        mytree.detach_mother()\n",
    "        \n",
    "\n",
    "        \n",
    "        '''\n",
    "        # solution\n",
    "        # compute prob* log pi \n",
    "        loglist = torch.log(nn_p)*p\n",
    "        \n",
    "        # constant term to make sure if policy result = MCTS result, loss = 0\n",
    "        constant = torch.where(p>0, p*torch.log(p),torch.tensor(0.))\n",
    "        logterm.append(-torch.sum(loglist-constant))\n",
    "        \n",
    "        vterm.append(nn_v*current_player)\n",
    "        '''\n",
    "        \n",
    "    # we compute the \"policy_loss\" for computing gradient\n",
    "    outcome = mytree.outcome\n",
    "    outcomes.append(outcome)\n",
    "    \n",
    "    '''\n",
    "    solution\n",
    "    # loss = torch.sum( (torch.stack(vterm)-outcome)**2 + torch.stack(logterm) )\n",
    "    optimizer.zero_grad()\n",
    "    '''\n",
    "    loss.backward()\n",
    "    losses.append(float(loss))\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (e+1)%50==0:\n",
    "        print(\"game: \",e+1, \", mean loss: {:3.2f}\".format(np.mean(losses[-20:])),\n",
    "              \", recent outcomes: \", outcomes[-10:])\n",
    "    del loss\n",
    "    \n",
    "    timer.update(e+1)\n",
    "    \n",
    "    \n",
    "timer.finish()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot your losses\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "% matplotlib notebook\n",
    "plt.plot(losses)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Play a game against your alphazero agent !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "% matplotlib notebook\n",
    "\n",
    "# as first player\n",
    "gameplay=Play(ConnectN(**game_setting), \n",
    "              player1=None, \n",
    "              player2=Policy_Player_MCTS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "% matplotlib notebook\n",
    "\n",
    "# as second player\n",
    "\n",
    "gameplay=Play(ConnectN(**game_setting), \n",
    "              player2=None, \n",
    "              player1=Policy_Player_MCTS)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
