{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evolution Strategies Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "from evostra import EvolutionStrategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from model import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "POPULATION_SIZE = 20\n",
    "EPISODE_AVERAGE = 1\n",
    "MAX_TIME = 1500\n",
    "SIGMA = 0.1\n",
    "LR = 0.1\n",
    "DECAY_RATE = 0.995\n",
    "INITIAL_EXPLORATION = 1.0\n",
    "FINAL_EXPLORATION = 0.0\n",
    "EXPLORATION_DEC_STEPS = 1e6\n",
    "PRINT_EVERY = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "ENV_NAME = 'BipedalWalker-v2'\n",
    "env = gym.make(ENV_NAME); env.seed(90); # remove unwrapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment Display:\n",
      "State space Box(24,)\n",
      "Action space Box(4,)\n"
     ]
    }
   ],
   "source": [
    "print('Environment Display:')\n",
    "env.reset() # reset environment to a new, random state\n",
    "# env.render()\n",
    "\n",
    "print('State space {}'.format(env.observation_space))\n",
    "print('Action space {}'.format(env.action_space))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define [ES](https://arxiv.org/pdf/1703.03864.pdf) Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ESAgent():\n",
    "    \n",
    "    def __init__(self, env, population_size=20, episode_average=1, max_time=1500, \n",
    "                 sigma=0.1, learning_rate=0.1, decay_rate=0.995, \n",
    "                 initial_exploration=1.0, final_exploration=0.0, exploration_dec_steps=1e6):\n",
    "    \n",
    "        self.env = env\n",
    "                \n",
    "        self.population_size = population_size\n",
    "        self.episode_average = episode_average\n",
    "        self.max_time = max_time\n",
    "        self.sigma = sigma\n",
    "        self.learning_rate = learning_rate\n",
    "        self.decay_rate = decay_rate\n",
    "        self.initial_exploration = initial_exploration\n",
    "        self.final_exploration = final_exploration\n",
    "        self.exploration_dec_steps = exploration_dec_steps\n",
    "        self.agent_history_length = 1\n",
    "        \n",
    "        self.exploration = self.initial_exploration\n",
    "        \n",
    "        self.model = Model()\n",
    "        self.es = EvolutionStrategy(self.model.get_weights(), self.get_reward, \n",
    "                                    self.population_size, self.sigma, self.learning_rate, self.decay_rate, num_threads=1)\n",
    "        \n",
    "        self.scores = []\n",
    "        \n",
    "    def get_predicted_action(self, sequence):\n",
    "        \n",
    "        action = self.model.predict(np.array(sequence))\n",
    "        return action\n",
    "    \n",
    "    def get_reward(self, weights):\n",
    "        \n",
    "        total_reward = 0.0\n",
    "        self.model.set_weights(weights)\n",
    "        \n",
    "        for i_episode in range(1, self.episode_average+1):\n",
    "            \n",
    "            state = self.env.reset()\n",
    "            sequence = [state] * self.agent_history_length\n",
    "            done = False\n",
    "            \n",
    "            while not done:\n",
    "                \n",
    "                self.exploration = max(self.final_exploration, self.exploration - self.initial_exploration / self.exploration_dec_steps)\n",
    "                if random.random() < self.exploration:\n",
    "                    action = self.env.action_space.sample()\n",
    "                else:\n",
    "                    action = self.get_predicted_action(sequence)\n",
    "                \n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                total_reward += reward\n",
    "                sequence = sequence[1:]\n",
    "                sequence.append(next_state)\n",
    "            \n",
    "        average_reward = total_reward/ self.episode_average\n",
    "        self.scores.append(average_reward)\n",
    "        \n",
    "        return average_reward\n",
    "    \n",
    "    def train(self, num_episodes, print_every=10):\n",
    "        self.es.run(num_episodes, print_step=print_every)\n",
    "        self.save(f'../agents/ES_{ENV_NAME}.pth')\n",
    "        \n",
    "        return self.scores\n",
    "    \n",
    "    def save(self, policy_path):\n",
    "        \n",
    "        if not os.path.exists('../agents/'): os.makedirs('../agents/')\n",
    "        with open(policy_path, 'wb') as weights:\n",
    "            pickle.dump(self.es.get_weights(), weights)\n",
    "            \n",
    "    def load(self, policy_path):\n",
    "        \n",
    "        with open(policy_path, 'rb') as weights:\n",
    "            self.model.set_weights(pickle.load(weights))\n",
    "        self.es.weights = self.model.get_weights()\n",
    "        \n",
    "    def watch(self, num_episodes=10, render=True):\n",
    "        \n",
    "        self.model.set_weights(self.es.weights)\n",
    "        for i_episode in range(1, num_episodes+1):\n",
    "            \n",
    "            total_reward = 0.0\n",
    "            state = self.env.reset()\n",
    "            sequence = [state] * self.agent_history_length\n",
    "            done = False\n",
    "            \n",
    "            while not done:\n",
    "                if render:\n",
    "                    self.env.render()\n",
    "                action = self.get_predicted_action(sequence)\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                total_reward += reward\n",
    "                sequence = sequence[1:]\n",
    "                sequence.append(next_state)\n",
    "                \n",
    "            print(f'Episode: {i_episode}, Total Reward: {total_reward:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = ESAgent(env, POPULATION_SIZE, EPISODE_AVERAGE, MAX_TIME, SIGMA, LR, DECAY_RATE, \n",
    "                INITIAL_EXPLORATION, FINAL_EXPLORATION, EXPLORATION_DEC_STEPS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train The Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1, Average Reward: -118.74\n",
      "Episode: 2, Average Reward: -73.00\n",
      "Episode: 3, Average Reward: -109.01\n",
      "Episode: 4, Average Reward: -103.03\n",
      "Episode: 5, Average Reward: -99.42\n",
      "Episode: 6, Average Reward: -88.58\n",
      "Episode: 7, Average Reward: -124.86\n",
      "Episode: 8, Average Reward: -118.71\n",
      "Episode: 9, Average Reward: -96.63\n",
      "Episode: 10, Average Reward: -88.53\n",
      "Episode: 11, Average Reward: -105.72\n",
      "Episode: 12, Average Reward: -106.54\n",
      "Episode: 13, Average Reward: -110.71\n",
      "Episode: 14, Average Reward: -105.52\n",
      "Episode: 15, Average Reward: -111.57\n",
      "Episode: 16, Average Reward: -108.74\n",
      "Episode: 17, Average Reward: -115.19\n",
      "Episode: 18, Average Reward: -105.82\n",
      "Episode: 19, Average Reward: -223.40\n",
      "Episode: 20, Average Reward: -114.24\n",
      "Episode: 21, Average Reward: -118.60\n",
      "Episode: 22, Average Reward: -104.23\n",
      "Episode: 23, Average Reward: -91.33\n",
      "Episode: 24, Average Reward: -106.31\n",
      "Episode: 25, Average Reward: -118.69\n",
      "Episode: 26, Average Reward: -117.17\n",
      "Episode: 27, Average Reward: -107.49\n",
      "Episode: 28, Average Reward: -110.12\n",
      "Episode: 29, Average Reward: -86.82\n",
      "Episode: 30, Average Reward: -89.33\n",
      "Episode: 31, Average Reward: -115.83\n",
      "Episode: 32, Average Reward: -90.65\n",
      "Episode: 33, Average Reward: -114.66\n",
      "Episode: 34, Average Reward: -85.62\n",
      "Episode: 35, Average Reward: -119.25\n",
      "Episode: 36, Average Reward: -90.59\n",
      "Episode: 37, Average Reward: -76.88\n",
      "Episode: 38, Average Reward: -77.26\n",
      "Episode: 39, Average Reward: -103.42\n",
      "Episode: 40, Average Reward: -136.75\n",
      "Episode: 41, Average Reward: -172.36\n",
      "Episode: 42, Average Reward: -115.91\n",
      "Episode: 43, Average Reward: -181.63\n",
      "Episode: 44, Average Reward: -115.07\n",
      "Episode: 45, Average Reward: -125.88\n",
      "Episode: 46, Average Reward: -115.88\n",
      "Episode: 47, Average Reward: -149.23\n",
      "Episode: 48, Average Reward: -124.43\n",
      "Episode: 49, Average Reward: -127.03\n",
      "Episode: 50, Average Reward: -126.63\n",
      "Episode: 51, Average Reward: -118.89\n",
      "Episode: 52, Average Reward: -113.99\n",
      "Episode: 53, Average Reward: -123.02\n",
      "Episode: 54, Average Reward: -130.54\n",
      "Episode: 55, Average Reward: -131.75\n",
      "Episode: 56, Average Reward: -119.47\n",
      "Episode: 57, Average Reward: -130.19\n",
      "Episode: 58, Average Reward: -131.86\n",
      "Episode: 59, Average Reward: -124.34\n",
      "Episode: 60, Average Reward: -121.67\n",
      "Episode: 61, Average Reward: -109.28\n",
      "Episode: 62, Average Reward: -111.89\n",
      "Episode: 63, Average Reward: -113.79\n",
      "Episode: 64, Average Reward: -108.22\n",
      "Episode: 65, Average Reward: -112.18\n",
      "Episode: 66, Average Reward: -214.12\n",
      "Episode: 67, Average Reward: -113.78\n",
      "Episode: 68, Average Reward: -104.01\n",
      "Episode: 69, Average Reward: -100.90\n",
      "Episode: 70, Average Reward: -104.75\n",
      "Episode: 71, Average Reward: -101.96\n",
      "Episode: 72, Average Reward: -102.43\n",
      "Episode: 73, Average Reward: -99.07\n",
      "Episode: 74, Average Reward: -110.50\n",
      "Episode: 75, Average Reward: -117.79\n",
      "Episode: 76, Average Reward: -98.76\n",
      "Episode: 77, Average Reward: -99.54\n",
      "Episode: 78, Average Reward: -99.90\n",
      "Episode: 79, Average Reward: -106.86\n",
      "Episode: 80, Average Reward: -102.95\n",
      "Episode: 81, Average Reward: -101.09\n",
      "Episode: 82, Average Reward: -100.41\n",
      "Episode: 83, Average Reward: -99.94\n",
      "Episode: 84, Average Reward: -101.39\n",
      "Episode: 85, Average Reward: -102.44\n",
      "Episode: 86, Average Reward: -104.27\n",
      "Episode: 87, Average Reward: -102.09\n",
      "Episode: 88, Average Reward: -104.40\n",
      "Episode: 89, Average Reward: -113.31\n",
      "Episode: 90, Average Reward: -103.86\n",
      "Episode: 91, Average Reward: -101.83\n",
      "Episode: 92, Average Reward: -105.67\n",
      "Episode: 93, Average Reward: -100.48\n",
      "Episode: 94, Average Reward: -102.60\n",
      "Episode: 95, Average Reward: -104.72\n",
      "Episode: 96, Average Reward: -104.22\n",
      "Episode: 97, Average Reward: -124.14\n",
      "Episode: 98, Average Reward: -114.69\n",
      "Episode: 99, Average Reward: -113.22\n",
      "Episode: 100, Average Reward: -100.29\n",
      "Episode: 101, Average Reward: -100.48\n",
      "Episode: 102, Average Reward: -102.20\n",
      "Episode: 103, Average Reward: -98.11\n",
      "Episode: 104, Average Reward: -99.72\n",
      "Episode: 105, Average Reward: -97.96\n",
      "Episode: 106, Average Reward: -102.03\n",
      "Episode: 107, Average Reward: -100.44\n",
      "Episode: 108, Average Reward: -100.27\n",
      "Episode: 109, Average Reward: -99.60\n",
      "Episode: 110, Average Reward: -103.11\n",
      "Episode: 111, Average Reward: -100.38\n",
      "Episode: 112, Average Reward: -97.86\n",
      "Episode: 113, Average Reward: -110.42\n",
      "Episode: 114, Average Reward: -100.65\n",
      "Episode: 115, Average Reward: -100.15\n",
      "Episode: 116, Average Reward: -110.50\n",
      "Episode: 117, Average Reward: -101.06\n",
      "Episode: 118, Average Reward: -101.25\n",
      "Episode: 119, Average Reward: -101.59\n",
      "Episode: 120, Average Reward: -103.76\n",
      "Episode: 121, Average Reward: -100.56\n",
      "Episode: 122, Average Reward: -101.14\n",
      "Episode: 123, Average Reward: -110.74\n",
      "Episode: 124, Average Reward: -103.31\n",
      "Episode: 125, Average Reward: -99.76\n",
      "Episode: 126, Average Reward: -105.43\n",
      "Episode: 127, Average Reward: -100.68\n",
      "Episode: 128, Average Reward: -96.56\n",
      "Episode: 129, Average Reward: -101.23\n",
      "Episode: 130, Average Reward: -103.75\n",
      "Episode: 131, Average Reward: -106.82\n",
      "Episode: 132, Average Reward: -102.81\n",
      "Episode: 133, Average Reward: -101.21\n",
      "Episode: 134, Average Reward: -100.88\n",
      "Episode: 135, Average Reward: -98.39\n",
      "Episode: 136, Average Reward: -99.61\n",
      "Episode: 137, Average Reward: -94.10\n",
      "Episode: 138, Average Reward: -97.61\n",
      "Episode: 139, Average Reward: -102.31\n"
     ]
    }
   ],
   "source": [
    "scores = agent.train(num_episodes=1000, print_every=PRINT_EVERY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate The Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(np.arange(len(scores)), scores, color='green')\n",
    "plt.xlabel('Num of episodes')\n",
    "plt.ylabel('Score')\n",
    "if not os.path.exists('../images/'): os.makedirs('../images/')\n",
    "plt.savefig('../images/plot_of_evolution_strategies_evaluation.png'ÃŸ)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¬ Watch The Smart Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "agent.load(f'../agents/ES_{ENV_NAME}.pth');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.watch(num_episodes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
