{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proximal Policy Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import namedtuple, deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.distributions import MultivariateNormal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from buffer import BasicBuffer\n",
    "from model import PolicyNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore', UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "if is_cuda: device = torch.device('cuda')\n",
    "else: device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GAMMA = 0.99\n",
    "PRINT_EVERY = 20\n",
    "\n",
    "LR = 3e-4\n",
    "BETAS = (0.9, 0.999)\n",
    "EPS_CLIP = 0.2\n",
    "ACTION_STD = 0.5\n",
    "K_EPOCHS = 80\n",
    "UPDATE_TIME_STEP = 4000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "ENV_NAME = 'BipedalWalker-v2'\n",
    "env = gym.make(ENV_NAME).unwrapped; env.seed(90);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment Display:\n",
      "State space Box(24,)\n",
      "Action space Box(4,)\n"
     ]
    }
   ],
   "source": [
    "print('Environment Display:')\n",
    "env.reset() # reset environment to a new, random state\n",
    "env.render()\n",
    "\n",
    "print('State space {}'.format(env.observation_space))\n",
    "print('Action space {}'.format(env.action_space))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define [PPO](https://arxiv.org/pdf/1707.06347.pdf) Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PPOAgent():\n",
    "    \"\"\"The Agent that will interacts with and learns from the environment.\"\"\"\n",
    "    \n",
    "    def __init__(self, env, seed):\n",
    "        \"\"\"Initialize an Agent object.\"\"\"\n",
    "        \n",
    "        self.env = env\n",
    "        \n",
    "        self.state_size = env.observation_space.shape[0]\n",
    "        self.action_size = env.action_space.shape[0]\n",
    "        \n",
    "        self.gamma = GAMMA\n",
    "        self.lr = LR\n",
    "        self.betas = BETAS\n",
    "        self.eps_clip = EPS_CLIP\n",
    "        self.action_std = ACTION_STD\n",
    "        self.K_epochs = K_EPOCHS\n",
    "\n",
    "        self.policy = PolicyNetwork(self.state_size, self.action_size, self.action_std, seed).to(device)\n",
    "        self.policy_optimizer = torch.optim.Adam(self.policy.parameters(), lr=self.lr, betas=self.betas)\n",
    "        \n",
    "        self.policy_old = PolicyNetwork(self.state_size, self.action_size, self.action_std, seed).to(device)\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "        \n",
    "        # set buffer\n",
    "        self.buffer = BasicBuffer()\n",
    "        \n",
    "        # set loss function\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "    \n",
    "    def calculate_action(self, state, action_mean, action_var):\n",
    "    \n",
    "        cov_mat = torch.diag(action_var).to(device)\n",
    "        dist = MultivariateNormal(action_mean, cov_mat)\n",
    "        \n",
    "        action = dist.sample()\n",
    "        action_log_prob = dist.log_prob(action)\n",
    "    \n",
    "        self.buffer.states.append(state)\n",
    "        self.buffer.actions.append(action)\n",
    "        self.buffer.log_probs.append(action_log_prob)\n",
    "        \n",
    "        return action.detach()\n",
    "    \n",
    "    def act(self, state):\n",
    "        \n",
    "        state = torch.FloatTensor(state.reshape(1, -1)).to(device)\n",
    "        \n",
    "        action_mean, action_var = self.policy_old.act(state)\n",
    "        action = self.calculate_action(state, action_mean, action_var)\n",
    "        action = action.cpu().data.numpy().flatten()\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def evaluate_policy(self, state, action):\n",
    "        \n",
    "        action_mean, action_var = self.policy.act(state)\n",
    "        action_var = action_var.expand_as(action_mean)\n",
    "        \n",
    "        cov_mat = torch.diag_embed(action_var).to(device)\n",
    "        dist = MultivariateNormal(action_mean, cov_mat)\n",
    "        \n",
    "        action_log_prob = dist.log_prob(action)\n",
    "        dist_entropy = dist.entropy()\n",
    "        state_value = self.policy.evaluate(state)\n",
    "        \n",
    "        return action_log_prob, torch.squeeze(state_value), dist_entropy\n",
    "        \n",
    "    def learn(self):\n",
    "        \n",
    "        rewards = []\n",
    "        discounted_reward = 0\n",
    "        \n",
    "        for reward, done in zip(reversed(self.buffer.rewards), reversed(self.buffer.dones)):\n",
    "            if done:\n",
    "                discounted_reward = 0\n",
    "            discounted_reward = reward + (self.gamma * discounted_reward)\n",
    "            rewards.insert(0, discounted_reward)\n",
    "            \n",
    "        # normalize the rewards\n",
    "        rewards = torch.tensor(rewards).to(device)\n",
    "        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-5)\n",
    "        \n",
    "        # convert list to tensor\n",
    "        old_states = torch.squeeze(torch.stack(self.buffer.states).to(device), 1).detach()\n",
    "        old_actions = torch.squeeze(torch.stack(self.buffer.actions).to(device), 1).detach()\n",
    "        old_log_probs = torch.squeeze(torch.stack(self.buffer.log_probs).to(device), 1).detach()\n",
    "        \n",
    "        # optimize policy for K epochs without generating new trajectories\n",
    "        for _ in range(self.K_epochs):\n",
    "            \n",
    "            # evaluate old actions and values to learn surrogate function\n",
    "            log_probs, state_values, dist_entropy = self.evaluate_policy(old_states, old_actions)\n",
    "            \n",
    "            # find the ratio of pi_theta and pi_theta_old\n",
    "            ratios = torch.exp(log_probs - old_log_probs.detach())\n",
    "            \n",
    "            # compute surrogate function (clip it to ensure that the new policy remains close to the old one)\n",
    "            advantages = rewards - state_values.detach()\n",
    "            surrogate1 = ratios * advantages\n",
    "            surrogate2 = torch.clamp(ratios, 1-self.eps_clip, 1+self.eps_clip) * advantages\n",
    "            loss = -torch.min(surrogate1, surrogate2) + 0.5 * self.mse_loss(state_values, rewards) - 0.01 * dist_entropy\n",
    "            loss = loss.mean()\n",
    "            \n",
    "            # backpropagate gradient loss\n",
    "            self.policy_optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.policy_optimizer.step()\n",
    "        \n",
    "        # copy new weights into old policy\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "        \n",
    "    def save(self, policy_path):\n",
    "        \n",
    "        if not os.path.exists('./agents/'): os.makedirs('./agents/')\n",
    "        torch.save(self.policy.state_dict(), policy_path)\n",
    "        \n",
    "    def watch(self, num_episodes):\n",
    "        \n",
    "        for i_episode in range(1, num_episodes+1):\n",
    "    \n",
    "            state = env.reset()\n",
    "            rewards = []\n",
    "            for time_step in range(1000):\n",
    "\n",
    "                env.render() # render the screen\n",
    "\n",
    "                action = self.act(state) # select an action\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "                state = next_state\n",
    "                rewards.append(reward)\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            print(f'\\rEpisode: {i_episode}, Average Score: {sum(rewards):.2f}')\n",
    "\n",
    "        env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "agent = PPOAgent(env, seed=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train The Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_agent(num_episodes=10000, max_time=1500):\n",
    "    \n",
    "    scores = []\n",
    "    scores_window = deque(maxlen=PRINT_EVERY)\n",
    "    \n",
    "    time_to_update = 0\n",
    "    average_length = 0\n",
    "    \n",
    "    # training loop\n",
    "    for i_episode in range(1, num_episodes+1):\n",
    "\n",
    "        state = env.reset()\n",
    "        score = 0\n",
    "        \n",
    "        for time_step in range(max_time):\n",
    "            \n",
    "            time_to_update +=1\n",
    "            \n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            # saving reward and done\n",
    "            agent.buffer.rewards.append(reward)\n",
    "            agent.buffer.dones.append(done)\n",
    "            \n",
    "            # update agent\n",
    "            if time_to_update % UPDATE_TIME_STEP == 0:\n",
    "                agent.learn()\n",
    "                agent.buffer.clear_memory()\n",
    "                time_to_update = 0\n",
    "                \n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "        average_length += time_step\n",
    "                \n",
    "        scores_window.append(score)\n",
    "        scores.append(score)\n",
    "        \n",
    "        print(f'\\rEpisode: {i_episode}, Average Length: {average_length/PRINT_EVERY}, Average Score: {np.mean(scores_window):.2f}', end='')\n",
    "        \n",
    "        if i_episode % PRINT_EVERY == 0:\n",
    "            print(f'\\rEpisode: {i_episode}, Average Length: {average_length/PRINT_EVERY}, Average Score: {np.mean(scores_window):.2f}')\n",
    "            average_length = 0\n",
    "            \n",
    "        if np.mean(scores_window) >= 300.0:\n",
    "            print(f'\\nEnvironment solved in {i_episode:d} episodes!, Average Length: {average_length}, Average Score: {np.mean(scores_window):.2f}')\n",
    "            agent.save(f'./agents/PPO_{ENV_NAME}.pth')\n",
    "            break\n",
    "            \n",
    "    print('Training completed.')\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 20, Average Length: 517.1, Average Score: -92.231\n",
      "Episode: 40, Average Length: 662.05, Average Score: -86.91\n",
      "Episode: 60, Average Length: 958.15, Average Score: -70.63\n",
      "Episode: 80, Average Length: 1213.1, Average Score: -57.173\n",
      "Episode: 100, Average Length: 1118.55, Average Score: -59.94\n",
      "Episode: 120, Average Length: 1257.5, Average Score: -52.783\n",
      "Episode: 140, Average Length: 1304.4, Average Score: -30.335\n",
      "Episode: 160, Average Length: 1307.1, Average Score: -17.603\n",
      "Episode: 180, Average Length: 1287.0, Average Score: 3.8276\n",
      "Episode: 200, Average Length: 1459.15, Average Score: 31.44\n",
      "Episode: 220, Average Length: 1499.0, Average Score: 54.105\n",
      "Episode: 240, Average Length: 1499.0, Average Score: 69.402\n",
      "Episode: 260, Average Length: 1432.05, Average Score: 73.85\n",
      "Episode: 280, Average Length: 1499.0, Average Score: 98.005\n",
      "Episode: 300, Average Length: 1499.0, Average Score: 109.505\n",
      "Episode: 320, Average Length: 1367.45, Average Score: 95.63\n",
      "Episode: 340, Average Length: 1306.25, Average Score: 86.80\n",
      "Episode: 360, Average Length: 1499.0, Average Score: 126.953\n",
      "Episode: 380, Average Length: 1358.3, Average Score: 100.522\n",
      "Episode: 400, Average Length: 1165.65, Average Score: 73.86\n",
      "Episode: 420, Average Length: 1344.2, Average Score: 108.751\n",
      "Episode: 440, Average Length: 1333.2, Average Score: 107.041\n",
      "Episode: 460, Average Length: 1395.45, Average Score: 123.06\n",
      "Episode: 480, Average Length: 1339.25, Average Score: 112.20\n",
      "Episode: 500, Average Length: 1458.3, Average Score: 131.454\n",
      "Episode: 520, Average Length: 1210.35, Average Score: 91.70\n",
      "Episode: 540, Average Length: 1415.55, Average Score: 130.85\n",
      "Episode: 560, Average Length: 1347.65, Average Score: 127.21\n",
      "Episode: 580, Average Length: 1258.45, Average Score: 115.50\n",
      "Episode: 600, Average Length: 1332.65, Average Score: 139.31\n",
      "Episode: 620, Average Length: 1386.35, Average Score: 142.01\n",
      "Episode: 640, Average Length: 1499.0, Average Score: 187.444\n",
      "Episode: 660, Average Length: 1237.8, Average Score: 124.115\n",
      "Episode: 680, Average Length: 1375.65, Average Score: 175.40\n",
      "Episode: 700, Average Length: 1248.0, Average Score: 157.090\n",
      "Episode: 720, Average Length: 1217.25, Average Score: 153.69\n",
      "Episode: 740, Average Length: 1244.0, Average Score: 157.011\n",
      "Episode: 760, Average Length: 1296.2, Average Score: 175.123\n",
      "Episode: 780, Average Length: 1322.45, Average Score: 184.21\n",
      "Episode: 800, Average Length: 1309.35, Average Score: 168.73\n",
      "Episode: 820, Average Length: 1430.6, Average Score: 204.818\n",
      "Episode: 840, Average Length: 1249.5, Average Score: 174.831\n",
      "Episode: 860, Average Length: 1359.45, Average Score: 207.21\n",
      "Episode: 880, Average Length: 1222.2, Average Score: 172.909\n",
      "Episode: 900, Average Length: 1286.55, Average Score: 189.45\n",
      "Episode: 920, Average Length: 1351.9, Average Score: 202.962\n",
      "Episode: 940, Average Length: 1428.75, Average Score: 234.98\n",
      "Episode: 960, Average Length: 1459.6, Average Score: 233.777\n",
      "Episode: 980, Average Length: 1280.15, Average Score: 204.14\n",
      "Episode: 1000, Average Length: 1204.15, Average Score: 175.10\n",
      "Episode: 1020, Average Length: 1383.7, Average Score: 236.778\n",
      "Episode: 1040, Average Length: 1377.2, Average Score: 240.705\n",
      "Episode: 1060, Average Length: 1462.1, Average Score: 255.338\n",
      "Episode: 1080, Average Length: 1193.15, Average Score: 181.55\n",
      "Episode: 1100, Average Length: 1397.75, Average Score: 232.05\n",
      "Episode: 1120, Average Length: 1338.75, Average Score: 213.37\n",
      "Episode: 1140, Average Length: 1337.55, Average Score: 214.87\n",
      "Episode: 1160, Average Length: 1332.75, Average Score: 220.90\n",
      "Episode: 1180, Average Length: 1449.8, Average Score: 254.613\n",
      "Episode: 1200, Average Length: 1173.6, Average Score: 182.278\n",
      "Episode: 1220, Average Length: 1386.8, Average Score: 238.176\n",
      "Episode: 1240, Average Length: 1381.7, Average Score: 235.315\n",
      "Episode: 1260, Average Length: 1362.55, Average Score: 241.29\n",
      "Episode: 1280, Average Length: 1237.35, Average Score: 196.08\n",
      "Episode: 1300, Average Length: 1422.7, Average Score: 258.086\n",
      "Episode: 1320, Average Length: 1336.35, Average Score: 241.77\n",
      "Episode: 1340, Average Length: 1353.6, Average Score: 238.710\n",
      "Episode: 1360, Average Length: 1400.1, Average Score: 257.371\n",
      "Episode: 1380, Average Length: 1286.35, Average Score: 211.15\n",
      "Episode: 1400, Average Length: 1210.0, Average Score: 205.790\n",
      "Episode: 1420, Average Length: 1250.05, Average Score: 207.84\n",
      "Episode: 1440, Average Length: 1376.35, Average Score: 247.39\n",
      "Episode: 1460, Average Length: 1349.25, Average Score: 227.89\n",
      "Episode: 1480, Average Length: 1369.85, Average Score: 238.07\n",
      "Episode: 1500, Average Length: 1443.5, Average Score: 248.796\n",
      "Episode: 1520, Average Length: 1277.0, Average Score: 209.60\n",
      "Episode: 1540, Average Length: 1404.2, Average Score: 258.954\n",
      "Episode: 1560, Average Length: 1395.95, Average Score: 258.43\n",
      "Episode: 1580, Average Length: 1223.9, Average Score: 203.588\n",
      "Episode: 1600, Average Length: 1405.0, Average Score: 258.222\n",
      "Episode: 1620, Average Length: 1402.1, Average Score: 256.598\n",
      "Episode: 1640, Average Length: 1403.3, Average Score: 256.484\n",
      "Episode: 1660, Average Length: 1337.45, Average Score: 240.42\n",
      "Episode: 1680, Average Length: 1073.55, Average Score: 154.80\n",
      "Episode: 1700, Average Length: 1210.55, Average Score: 194.04\n",
      "Episode: 1720, Average Length: 1163.85, Average Score: 182.02\n",
      "Episode: 1740, Average Length: 1361.15, Average Score: 236.13\n",
      "Episode: 1760, Average Length: 1146.1, Average Score: 183.862\n",
      "Episode: 1780, Average Length: 1333.55, Average Score: 240.27\n",
      "Episode: 1800, Average Length: 1322.05, Average Score: 239.23\n",
      "Episode: 1820, Average Length: 1385.65, Average Score: 255.72\n",
      "Episode: 1840, Average Length: 1107.4, Average Score: 170.94\n",
      "Episode: 1860, Average Length: 1371.85, Average Score: 259.12\n",
      "Episode: 1880, Average Length: 1249.45, Average Score: 221.54\n",
      "Episode: 1900, Average Length: 1264.7, Average Score: 220.707\n",
      "Episode: 1920, Average Length: 1198.15, Average Score: 200.20\n",
      "Episode: 1940, Average Length: 1268.2, Average Score: 218.483\n",
      "Episode: 1960, Average Length: 1313.05, Average Score: 239.33\n",
      "Episode: 1980, Average Length: 1122.85, Average Score: 187.47\n",
      "Episode: 2000, Average Length: 1223.0, Average Score: 212.188\n",
      "Episode: 2020, Average Length: 1338.5, Average Score: 232.690\n",
      "Episode: 2040, Average Length: 1271.45, Average Score: 219.76\n",
      "Episode: 2060, Average Length: 1277.35, Average Score: 218.98\n",
      "Episode: 2080, Average Length: 1251.9, Average Score: 212.172\n",
      "Episode: 2100, Average Length: 1405.75, Average Score: 256.01\n",
      "Episode: 2120, Average Length: 1359.05, Average Score: 234.75\n",
      "Episode: 2140, Average Length: 1414.85, Average Score: 255.52\n",
      "Episode: 2148, Average Length: 486.2, Average Score: 238.653"
     ]
    }
   ],
   "source": [
    "scores = train_agent(num_episodes=10000, max_time=1500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate The Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(np.arange(len(scores)), scores, color='green')\n",
    "plt.xlabel('Num of episodes')\n",
    "plt.ylabel('Score')\n",
    "if not os.path.exists('./images/'): os.makedirs('./images/')\n",
    "plt.savefig('./images/plot_of_ppo_evaluation.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¬ Watch The Smart Agen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "agent.watch(num_episodes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
