{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proximal Policy Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import namedtuple, deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.distributions import MultivariateNormal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from buffer import BasicBuffer\n",
    "from model import PolicyNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore', UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "if is_cuda: device = torch.device('cuda')\n",
    "else: device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GAMMA = 0.99\n",
    "PRINT_EVERY = 20\n",
    "\n",
    "LR = 3e-4\n",
    "BETAS = (0.9, 0.999)\n",
    "EPS_CLIP = 0.2\n",
    "ACTION_STD = 0.5\n",
    "K_EPOCHS = 80\n",
    "UPDATE_EVERY = 4000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME = 'BipedalWalker-v2'\n",
    "env = gym.make(ENV_NAME).unwrapped; env.seed(90);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Environment Display:')\n",
    "env.reset() # reset environment to a new, random state\n",
    "env.render()\n",
    "\n",
    "print('State space {}'.format(env.observation_space))\n",
    "print('Action space {}'.format(env.action_space))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define [PPO](https://arxiv.org/pdf/1707.06347.pdf) Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PPOAgent():\n",
    "    \"\"\"The Agent that will interacts with and learns from the environment.\"\"\"\n",
    "    \n",
    "    def __init__(self, env, seed):\n",
    "        \"\"\"Initialize an Agent object.\"\"\"\n",
    "        \n",
    "        self.env = env\n",
    "        \n",
    "        self.state_size = env.observation_space.shape[0]\n",
    "        self.action_size = env.action_space.shape[0]\n",
    "        \n",
    "        self.gamma = GAMMA\n",
    "        self.lr = LR\n",
    "        self.betas = BETAS\n",
    "        self.eps_clip = EPS_CLIP\n",
    "        self.action_std = ACTION_STD\n",
    "        self.K_epochs = K_EPOCHS\n",
    "\n",
    "        self.policy = PolicyNetwork(self.state_size, self.action_size, self.action_std, seed).to(device)\n",
    "        self.policy_optimizer = torch.optim.Adam(self.policy.parameters(), lr=self.lr, betas=self.betas)\n",
    "        \n",
    "        self.policy_old = PolicyNetwork(self.state_size, self.action_size, self.action_std, seed).to(device)\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "        \n",
    "        # set buffer\n",
    "        self.buffer = BasicBuffer()\n",
    "        \n",
    "        # set loss function\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "    \n",
    "    def calculate_action(self, state, action_mean, action_var):\n",
    "    \n",
    "        cov_mat = torch.diag(action_var).to(device)\n",
    "        dist = MultivariateNormal(action_mean, cov_mat)\n",
    "        \n",
    "        action = dist.sample()\n",
    "        action_log_prob = dist.log_prob(action)\n",
    "    \n",
    "        self.buffer.states.append(state)\n",
    "        self.buffer.actions.append(action)\n",
    "        self.buffer.log_probs.append(action_log_prob)\n",
    "        \n",
    "        return action.detach()\n",
    "    \n",
    "    def act(self, state):\n",
    "        \n",
    "        state = torch.FloatTensor(state.reshape(1, -1)).to(device)\n",
    "        \n",
    "        action_mean, action_var = self.policy_old.act(state)\n",
    "        action = self.calculate_action(state, action_mean, action_var)\n",
    "        action = action.cpu().data.numpy().flatten()\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def evaluate_policy(self, state, action):\n",
    "        \n",
    "        action_mean, action_var = self.policy_old.act(state)\n",
    "        action_var = action_var.expand_as(action_mean)\n",
    "        \n",
    "        cov_mat = torch.diag_embed(action_var).to(device)\n",
    "        dist = MultivariateNormal(action_mean, cov_mat)\n",
    "        \n",
    "        action_log_prob = dist.log_prob(action)\n",
    "        dist_entropy = dist.entropy()\n",
    "        state_value = self.policy.evaluate(state)\n",
    "        \n",
    "        return action_log_prob, torch.squeeze(state_value), dist_entropy\n",
    "        \n",
    "    def learn(self):\n",
    "        \n",
    "        rewards = []\n",
    "        discounted_reward = 0\n",
    "        \n",
    "        for reward, done in zip(reversed(self.buffer.rewards), reversed(self.buffer.dones)):\n",
    "            if done:\n",
    "                discounted_reward = 0\n",
    "            discounted_reward = reward + (self.gamma * discounted_reward)\n",
    "            rewards.insert(0, discounted_reward)\n",
    "            \n",
    "        # normalize the rewards\n",
    "        rewards = torch.tensor(rewards).to(device)\n",
    "        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-5)        \n",
    "        \n",
    "        # convert list to tensor\n",
    "        old_states = torch.squeeze(torch.stack(self.buffer.states).to(device), 1).detach()\n",
    "        old_actions = torch.squeeze(torch.stack(self.buffer.actions).to(device), 1).detach()\n",
    "        old_log_probs = torch.squeeze(torch.stack(self.buffer.log_probs).to(device), 1).detach()\n",
    "        \n",
    "        # optimize policy for K epochs:\n",
    "        for _ in range(self.K_epochs):\n",
    "            \n",
    "            # evaluating old actions and values\n",
    "            log_probs, state_values, dist_entropy = self.evaluate_policy(old_states, old_actions)\n",
    "            \n",
    "            # finding the ratio (pi_theta / pi_theta_old)\n",
    "            ratios = torch.exp(log_probs - old_log_probs.detach())\n",
    "            \n",
    "            # finding surrogate loss\n",
    "            advantages = rewards - state_values.detach()\n",
    "            surrogate1 = ratios * advantages\n",
    "            surrogate2 = torch.clamp(ratios, 1-self.eps_clip, 1+self.eps_clip) * advantages\n",
    "            loss = -torch.min(surrogate1, surrogate2) + 0.5 * self.mse_loss(state_values, rewards) - 0.01 * dist_entropy\n",
    "            loss = loss.mean()\n",
    "            \n",
    "            # backpropagate gradient loss\n",
    "            self.policy_optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.policy_optimizer.step()\n",
    "        \n",
    "        # copy new weights into old policy\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "        \n",
    "    def save(self, policy_path):\n",
    "        \n",
    "        if not os.path.exists('./agents/'): os.makedirs('./agents/')\n",
    "        torch.save(self.policy.state_dict(), policy_path)\n",
    "        \n",
    "    def watch(self, num_episodes):\n",
    "        \n",
    "        for i_episode in range(1, num_episodes+1):\n",
    "    \n",
    "            state = env.reset()\n",
    "            rewards = []\n",
    "            for time_step in range(1000):\n",
    "\n",
    "                env.render() # render the screen\n",
    "\n",
    "                action = self.act(state) # select an action\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "                state = next_state\n",
    "                rewards.append(reward)\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            print(f'\\rEpisode: {i_episode}, Average Score: {sum(rewards):.2f}')\n",
    "\n",
    "        env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "agent = PPOAgent(env, seed=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train The Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(num_episodes=10000, max_time=1500):\n",
    "    \n",
    "    scores = []\n",
    "    scores_window = deque(maxlen=PRINT_EVERY)\n",
    "\n",
    "    # training loop\n",
    "    for i_episode in range(1, num_episodes+1):\n",
    "\n",
    "        state = env.reset()\n",
    "        score = 0\n",
    "        \n",
    "        for time_step in range(1, max_time):\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            # saving reward and done\n",
    "            agent.buffer.rewards.append(reward)\n",
    "            agent.buffer.dones.append(done)\n",
    "            \n",
    "            # update agent\n",
    "            if time_step % UPDATE_EVERY == 0:\n",
    "                agent.learn()\n",
    "                agent.buffer.clear_memory()\n",
    "                \n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "        scores_window.append(score)\n",
    "        scores.append(score)\n",
    "        \n",
    "        print(f'\\rEpisode: {i_episode}, Average Score: {np.mean(scores_window):.2f}', end='')\n",
    "        \n",
    "        if i_episode % PRINT_EVERY == 0:\n",
    "            print(f'\\rEpisode: {i_episode}, Average Score: {np.mean(scores_window):.2f}')\n",
    "        if np.mean(scores_window) >= 300.0:\n",
    "            print(f'\\nEnvironment solved in {i_episode:d} episodes! Average Score: {np.mean(scores_window):.2f}')\n",
    "            agent.save(f'./agents/PPO_{ENV_NAME}.pth')\n",
    "            break\n",
    "            \n",
    "    print('Training completed.')\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = train_agent(num_episodes=10000, max_time=1500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate The Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(np.arange(len(scores)), scores, color='green')\n",
    "plt.xlabel('Num of episodes')\n",
    "plt.ylabel('Score')\n",
    "if not os.path.exists('./images/'): os.makedirs('./images/')\n",
    "plt.savefig('./images/plot_of_ppo_evaluation.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¬ Watch The Smart Agen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "agent.watch(num_episodes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
