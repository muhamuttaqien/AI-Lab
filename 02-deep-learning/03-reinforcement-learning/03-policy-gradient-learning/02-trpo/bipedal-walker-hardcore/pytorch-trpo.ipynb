{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trust Region Policy Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import scipy.optimize\n",
    "from collections import namedtuple, deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from torch.distributions import MultivariateNormal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from buffer import BasicBuffer\n",
    "from model import PolicyNetwork, ValueNetwork\n",
    "from running_state import ZFilter\n",
    "from trpo import trpo_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore', UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "if is_cuda: device = torch.device('cuda')\n",
    "else: device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.utils.backcompat.broadcast_warning.enabled = True\n",
    "torch.utils.backcompat.keepdim_warning.enabled = True\n",
    "\n",
    "torch.set_default_tensor_type('torch.DoubleTensor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GAMMA = 0.995\n",
    "TAU = 0.97\n",
    "PRINT_EVERY = 1\n",
    "\n",
    "DECAY_RATE = 1e-3\n",
    "MAX_KL = 1e-2\n",
    "DAMPING = 1e-1\n",
    "BATCH_SIZE = 5000\n",
    "MAX_TIME_STEP = 25000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ENV_NAME = 'BipedalWalkerHardcore-v2'\n",
    "env = gym.make(ENV_NAME).unwrapped; env.seed(90);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Environment Display:')\n",
    "env.reset() # reset environment to a new, random state\n",
    "# env.render()\n",
    "\n",
    "print('State space {}'.format(env.observation_space))\n",
    "print('Action space {}'.format(env.action_space))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define [TRPO](https://arxiv.org/pdf/1502.05477.pdf) Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TRPOAgent():\n",
    "    \"\"\"The Agent that will interacts with and learns from the environment.\"\"\"\n",
    "    \n",
    "    def __init__(self, env, seed):\n",
    "        \"\"\"Initialize an Agent object.\"\"\"\n",
    "        \n",
    "        self.env = env\n",
    "    \n",
    "        self.state_size = env.observation_space.shape[0]\n",
    "        self.action_size = env.action_space.shape[0]\n",
    "                \n",
    "        self.gamma = GAMMA\n",
    "        self.tau = TAU\n",
    "        self.decay_rate = DECAY_RATE\n",
    "        self.max_kl = MAX_KL\n",
    "        self.damping = DAMPING\n",
    "        \n",
    "        self.policy = PolicyNetwork(self.state_size, self.action_size, seed).to(device)\n",
    "        self.value = ValueNetwork(self.state_size, seed).to(device)\n",
    "        \n",
    "        # set buffer\n",
    "        self.buffer = BasicBuffer(seed=90)\n",
    "        \n",
    "        # set Zfilter\n",
    "        self.running_state = ZFilter((self.state_size,), clip=5)\n",
    "        self.running_reward = ZFilter((1,), demean=False, clip=10)\n",
    "        \n",
    "    def memorize(self, state, action, reward, next_state, mask):\n",
    "        \n",
    "        self.buffer.add(state, np.array([action]), reward, next_state, mask)\n",
    "        \n",
    "    def act(self, state):\n",
    "        \n",
    "        state = torch.from_numpy(state).unsqueeze(0)\n",
    "        action_mean, _, action_std = self.policy(Variable(state))\n",
    "        action = torch.normal(action_mean, action_std)\n",
    "        \n",
    "        return action.data[0].numpy()\n",
    "    \n",
    "    def learn(self):\n",
    "    \n",
    "        experiences = self.buffer.sample()\n",
    "        self.update_params(experiences)\n",
    "    \n",
    "    def update_params(self, experiences):\n",
    "        \n",
    "        states = torch.Tensor(experiences.state)\n",
    "        actions = torch.Tensor(np.concatenate(experiences.action, 0))\n",
    "        rewards = torch.Tensor(experiences.reward)\n",
    "        masks = torch.Tensor(experiences.mask)\n",
    "        \n",
    "        returns = torch.Tensor(actions.size(0), 1)\n",
    "        deltas = torch.Tensor(actions.size(0), 1)\n",
    "        advantages = torch.Tensor(actions.size(0), 1)\n",
    "        \n",
    "        old_return = 0\n",
    "        old_value = 0\n",
    "        old_advantage = 0\n",
    "    \n",
    "        values = self.value(Variable(states))\n",
    "    \n",
    "        for i in reversed(range(rewards.size(0))):\n",
    "            \n",
    "            returns[i] = rewards[i] + self.gamma * old_return * masks[i]\n",
    "            deltas[i] = rewards[i] + self.gamma * old_value * masks[i] - values.data[i]\n",
    "            advantages[i] = deltas[i] + self.gamma * self.tau * old_advantage * masks[i]\n",
    "            \n",
    "            old_return = returns[i, 0]\n",
    "            old_value = values.data[i, 0]\n",
    "            old_advantage = advantages[i, 0]\n",
    "            \n",
    "        targets = Variable(returns)\n",
    "\n",
    "        # original code uses the same LBFGS to optimize the value loss\n",
    "        def get_value_loss(flat_params):\n",
    "\n",
    "            set_flat_params_to(self.value, torch.Tensor(flat_params))\n",
    "            for param in self.value.parameters():\n",
    "                if param.grad is not None:\n",
    "                    param.grad.data.fill_(0)\n",
    "\n",
    "            values_ = self.value(Variable(states))\n",
    "\n",
    "            value_loss = (values_ - targets).pow(2).mean()\n",
    "\n",
    "            # weight decay\n",
    "            for param in self.value.parameters():\n",
    "                value_loss += param.pow(2).sum() * self.decay_rate\n",
    "            value_loss.backward()\n",
    "\n",
    "            return (value_loss.data.double().numpy(), get_flat_grad_from(self.value).data.double().numpy())\n",
    "        \n",
    "        flat_params, _, opt_info = scipy.optimize.fmin_l_bfgs_b(get_value_loss, get_flat_params_from(self.value).double().numpy(), maxiter=25)\n",
    "        set_flat_params_to(self.value, torch.Tensor(flat_params))\n",
    "        \n",
    "        advantages = (advantages - advantages.mean()) / advantages.std()\n",
    "        \n",
    "        action_means, action_log_stds, action_stds = self.policy(Variable(states))\n",
    "        fixed_log_prob = normal_log_density(Variable(actions), action_means, action_log_stds, action_stds).data.clone()\n",
    "        \n",
    "        def get_policy_loss(volatile=False):\n",
    "\n",
    "            if volatile:\n",
    "                with torch.no_grad():\n",
    "                    action_means, action_log_stds, action_stds = self.policy(Variable(states))\n",
    "            else:\n",
    "                action_means, action_log_stds, action_stds = self.policy(Variable(states))\n",
    "\n",
    "            log_prob = normal_log_density(Variable(actions), action_means, action_log_stds, action_stds)\n",
    "            action_loss = -Variable(advantages) * torch.exp(log_prob - Variable(fixed_log_prob))\n",
    "\n",
    "            return action_loss.mean()\n",
    "\n",
    "        def get_kl():\n",
    "\n",
    "            mean, log_std, std = self.policy(states)\n",
    "\n",
    "            mean_old = Variable(mean.data)\n",
    "            log_std_old = Variable(log_std.data)\n",
    "            std_old = Variable(std.data)\n",
    "\n",
    "            kl = log_std - log_std_old + (std_old.pow(2) + (mean_old - mean).pow(2)) / (2.0 * std.pow(2)) - 0.5\n",
    "            return kl.sum(1, keepdim=True)\n",
    "        \n",
    "        policy_loss = trpo_step(self.policy, get_policy_loss, get_kl, self.max_kl, self.damping)\n",
    "        \n",
    "    def save(self, policy_path, value_path):\n",
    "        \n",
    "        if not os.path.exists('./agents/'): os.makedirs('./agents/')\n",
    "        torch.save(self.policy.state_dict(), policy_path); torch.save(self.value.state_dict(), value_path)\n",
    "    \n",
    "    def watch(self, num_episodes):\n",
    "        \n",
    "        for i_episode in range(1, num_episodes+1):\n",
    "    \n",
    "            state = env.reset()\n",
    "            state = agent.running_state(state)\n",
    "            \n",
    "            reward_sum = 0\n",
    "            \n",
    "            for time_step in range(10000):\n",
    "\n",
    "                env.render() # render the screen\n",
    "\n",
    "                action = self.act(state) # select an action\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "                next_state = agent.running_state(next_state)\n",
    "\n",
    "                state = next_state\n",
    "                reward_sum += reward\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            print(f'\\rEpisode: {i_episode}, Last Reward: {reward_sum:.2f}')\n",
    "\n",
    "        env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "agent = TRPOAgent(env, seed=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train The Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_agent(num_episodes=10000):\n",
    "    \n",
    "    scores = []\n",
    "    scores_window = deque(maxlen=PRINT_EVERY)\n",
    "    \n",
    "    for i_episode in range(1, num_episodes+1):\n",
    "        \n",
    "        agent.buffer.clear_memory()\n",
    "        \n",
    "        num_steps = 0\n",
    "        num_episodes = 0\n",
    "        reward_batch = 0\n",
    "        \n",
    "        while num_steps < BATCH_SIZE:\n",
    "\n",
    "            state = env.reset()\n",
    "            state = agent.running_state(state)\n",
    "\n",
    "            reward_sum = 0\n",
    "\n",
    "            for time_step in range(MAX_TIME_STEP): \n",
    "                \n",
    "                action = agent.act(state)\n",
    "                next_state, reward, done, _ = env.step(action)            \n",
    "                next_state = agent.running_state(next_state)\n",
    "\n",
    "                mask = 0 if done else 1\n",
    "                agent.memorize(state, action, reward, next_state, mask)\n",
    "                \n",
    "                state = next_state\n",
    "                reward_sum += reward\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            num_steps += (time_step-1)\n",
    "            num_episodes += 1\n",
    "            reward_batch += reward_sum\n",
    "\n",
    "        reward_batch /= num_episodes\n",
    "        \n",
    "        scores_window.append(reward_batch)\n",
    "        scores.append(reward_batch)\n",
    "        \n",
    "        agent.learn()\n",
    "        \n",
    "        if i_episode % PRINT_EVERY == 0:\n",
    "            print(f'Episode {i_episode}, Last Reward: {reward_sum:.2f}, Average Score: {reward_batch:.2f}')\n",
    "            \n",
    "        if np.mean(scores_window) >= 250.0:\n",
    "            print(f'\\nEnvironment solved in {i_episode:d} episodes!, Last Reward: {reward_sum:.2f}, Average Score: {reward_batch:.2f}')\n",
    "            agent.save(f'./agents/POLICY_{ENV_NAME}.pth', f'./agents/VALUE_{ENV_NAME}.pth')\n",
    "            break\n",
    "    \n",
    "    print('Training completed.')\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scores = train_agent(num_episodes=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate The Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(np.arange(len(scores)), scores, color='green')\n",
    "plt.xlabel('Num of episodes')\n",
    "plt.ylabel('Score')\n",
    "if not os.path.exists('./images/'): os.makedirs('./images/')\n",
    "plt.savefig('./images/plot_of_trpo_evaluation.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¬ Watch The Smart Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "agent.policy.load_state_dict(torch.load(f'./agents/POLICY_{ENV_NAME}.pth'));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "agent.watch(num_episodes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
