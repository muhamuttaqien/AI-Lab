{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## no_grad"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "with torch.no_grad():\n",
    "    return policy_net(state).max(1)[1].view(1, 1)\n",
    "\n",
    "return policy_net(state).detach().max(1)[1].view(1, 1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# torch.no_grad says that no operation should build the graph\n",
    "# The returned result will be the same, but the version with torch.no_grad will use less memory\n",
    "# because it knows from the beginning that no gradients are needed so it doesn’t need to keep intermediary results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(5, 5) # requires_grad=False by default\n",
    "y = torch.randn(5, 5) # requires_grad=False by default\n",
    "z = torch.randn((5, 5), requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = x + y\n",
    "a.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = a + z\n",
    "b.requires_grad"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# if there’s a single input to an operation that requires gradient, its output will also require gradient\n",
    "# conversely, only if all inputs don’t require gradient, the output also won’t require it\n",
    "# backward computation is never performed in the subgraphs, where all tensors didn’t require gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## detach"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# returns a new Tensor, detached from the current computational graph\n",
    "# the result will never require gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References\n",
    "* https://pytorch.org/docs/stable/notes/autograd.html\n",
    "* https://pytorch.org/docs/stable/autograd.html#torch.Tensor.detach\n",
    "* https://discuss.pytorch.org/t/detach-no-grad-and-requires-grad/16915/2\n",
    "* https://discuss.pytorch.org/t/runtimeerror-trying-to-backward-through-the-graph-a-second-time-but-the-buffers-have-already-been-freed-specify-retain-graph-true-when-calling-backward-the-first-time/6795\n",
    "* https://discuss.pytorch.org/t/what-is-the-difference-between-tensors-and-variables-in-pytorch/4914"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
