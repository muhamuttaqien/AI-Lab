{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings; warnings.filterwarnings('ignore')\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import reader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init_scale = 0.1 # initial weight scale\n",
    "learning_rate = 1.0 # initial learning rate\n",
    "max_grad_norm = 5 # maximum permissible norm for the gradient clipping\n",
    "num_layers = 2 # the number of layers in our model\n",
    "num_steps = 20 # the total number of recurrence steps, also known as the number of layers when our RNN is unfolded\n",
    "hidden_size_l1 = 256 # the number of processing units (neurons) in the hidden layers\n",
    "hidden_size_l2 = 128\n",
    "max_epoch_decay_lr = 4 # the maximum number of epochs trained with the initial learning rate\n",
    "num_epochs = 15 # the total number of epochs in training\n",
    "keep_prob = 1 # at 1, we ignore the Dropout Layer wrapping\n",
    "decay = 0.5 # the decay for the learning rate\n",
    "batch_size = 60 # the size for each batch of data\n",
    "vocab_size = 10000 # the size of our vocabulary\n",
    "embedding_vector_size = 200\n",
    "is_training = 1 # training flag to separate training from testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Interactive Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir = './datasets/data/simple-examples/data/' # data directory for our datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reads the data and separates it into training, validation and testing data\n",
    "raw_data = reader.ptb_raw_data(data_dir)\n",
    "train_data, valid_data, test_data, vocab, word_to_id = raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def id_to_word(id_list):\n",
    "    line = []\n",
    "    for w in id_list:\n",
    "        for word, wid in word_to_id.items():\n",
    "            if wid == w:\n",
    "                line.append(word)\n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total of Training Data:', len(train_data))\n",
    "print('Word Examples:', id_to_word(train_data[0:8]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iterator = reader.ptb_iterator(train_data, batch_size, num_steps)\n",
    "first_tupple = iterator.__next__()\n",
    "X = first_tupple[0]\n",
    "y = first_tupple[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_data = tf.placeholder(tf.int32, [batch_size, num_steps])\n",
    "targets = tf.placeholder(tf.int32, [batch_size, num_steps])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create LSTM Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LSTM_cells = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cell = tf.contrib.rnn.BasicLSTMCell(hidden_size_l1, forget_bias=0.0)\n",
    "LSTM_cells.append(cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cell = tf.contrib.rnn.BasicLSTMCell(hidden_size_l2, forget_bias=0.0)\n",
    "LSTM_cells.append(cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stacked_LSTM = tf.contrib.rnn.MultiRNNCell(LSTM_cells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "initial_state = stacked_LSTM.zero_state(batch_size, tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create The Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_vocab = tf.get_variable('embedding_vocab', [vocab_size, embedding_vector_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs = tf.nn.embedding_lookup(embedding_vocab, input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lstm_out, hidden_state = tf.nn.dynamic_rnn(stacked_LSTM, inputs, initial_state=initial_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output = tf.reshape(lstm_out, [-1, hidden_size_l2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "softmax_W = tf.get_variable('softmax_W', [hidden_size_l2, vocab_size])\n",
    "softmax_b = tf.get_variable('softmax_b', [vocab_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_op = tf.matmul(output, softmax_W) + softmax_b\n",
    "logits_op = tf.reshape(logits_op, [batch_size, num_steps, vocab_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "probs_op = tf.nn.softmax(logits_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_op = tf.argmax(probs_op, axis=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_op = tf.contrib.seq2seq.sequence_loss(logits_op, targets, tf.ones([batch_size, num_steps], dtype=tf.float32), \n",
    "                                           average_across_timesteps=False, average_across_batch=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_op = tf.reduce_sum(loss_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = tf.Variable(0.0, trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_vars = tf.trainable_variables()\n",
    "grads, _ = tf.clip_by_global_norm(tf.gradients(loss_op, train_vars), max_grad_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer_op = tf.train.GradientDescentOptimizer(lr)\n",
    "optimizer_op = optimizer_op.apply_gradients(zip(grads, train_vars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train The Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_model(data, optimizer_op, verbose=False):\n",
    "    \n",
    "    epoch_size = ((len(data) // batch_size) - 1) // num_steps\n",
    "    start_time = time.time()\n",
    "\n",
    "    losses = 0.0\n",
    "    iters = 0\n",
    "\n",
    "    state = sess.run(initial_state)\n",
    "\n",
    "    for step, (X, y) in enumerate(reader.ptb_iterator(data, batch_size, num_steps)):\n",
    "\n",
    "        loss, state, words, _ = sess.run([loss_op, hidden_state, words_op, optimizer_op], \n",
    "                                          feed_dict={input_data: X, targets: y, initial_state: state})\n",
    "        losses += loss\n",
    "        iters += num_steps\n",
    "\n",
    "        if verbose and step % (epoch_size // 10) == 10:\n",
    "            speed = iters * batch_size / (time.time() - start_time)\n",
    "            print(f'Iteration: {step}/ {epoch_size}, Perplexity: {np.exp(losses/iters):.3f}, Speed: {speed:.0f} wps')\n",
    "\n",
    "    perplexity = np.exp(losses / iters)\n",
    "    \n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    \n",
    "    sess.run(init_op) # run the init_op using an interactive session\n",
    "\n",
    "    initializer = tf.random_uniform_initializer(-init_scale, init_scale)\n",
    "    \n",
    "    for i_epoch in range(1, num_epochs+1):\n",
    "    \n",
    "        # define the decay for this epoch\n",
    "        lr_decay = decay ** max(i_epoch - max_epoch_decay_lr, 0.0)\n",
    "\n",
    "        learning_rate = tf.assign(lr, learning_rate*lr_decay); learning_rate = sess.run(learning_rate)\n",
    "        print(f'Epoch: {i_epoch}, Learning Rate: {learning_rate:.3f}')\n",
    "\n",
    "        # run the loop for this epoch in the training model\n",
    "        train_perplexity = run_model(train_data, optimizer_op, verbose=True)\n",
    "        print(f'Epoch {i_epoch}, Train Perplexity: {train_perplexity:3f}')\n",
    "        \n",
    "        # run the loop for this epoch in the validation model\n",
    "        valid_perplexity = run_model(valid_data, tf.no_op())\n",
    "        print(f'Epoch {i_epoch}, Valid Perplexity: {valid_perplexity:3f}')\n",
    "        \n",
    "    # run the loop in the testing model to see how effective was our training\n",
    "    test_perplexity = run_model(test_data, tf.no_op())\n",
    "    print(f'Training LSTM Model is done. Test Perplexity: {test_perplexity:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
