{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings; warnings.filterwarnings('ignore')\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import reader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init_scale = 0.1 # initial weight scale\n",
    "learning_rate = 1.0 # initial learning rate\n",
    "max_grad_norm = 5 # maximum permissible norm for the gradient clipping\n",
    "num_layers = 2 # the number of layers in our model\n",
    "num_steps = 20 # the total number of recurrence steps, also known as the number of layers when our RNN is unfolded\n",
    "hidden_size_l1 = 256 # the number of processing units (neurons) in the hidden layers\n",
    "hidden_size_l2 = 128\n",
    "max_epoch_decay_lr = 4 # the maximum number of epochs trained with the initial learning rate\n",
    "num_epochs = 15 # the total number of epochs in training\n",
    "keep_prob = 1 # at 1, we ignore the Dropout Layer wrapping\n",
    "decay = 0.5 # the decay for the learning rate\n",
    "batch_size = 60 # the size for each batch of data\n",
    "vocab_size = 10000 # the size of our vocabulary\n",
    "embedding_vector_size = 200\n",
    "is_training = 1 # training flag to separate training from testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Interactive Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir = './datasets/data/simple-examples/data/' # data directory for our datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reads the data and separates it into training, validation and testing data\n",
    "raw_data = reader.ptb_raw_data(data_dir)\n",
    "train_data, valid_data, test_data, vocab, word_to_id = raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def id_to_word(id_list):\n",
    "    line = []\n",
    "    for w in id_list:\n",
    "        for word, wid in word_to_id.items():\n",
    "            if wid == w:\n",
    "                line.append(word)\n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total of Training Data: 929589\n",
      "Word Examples: ['aer', 'banknote', 'berlitz', 'calloway', 'centrust', 'cluett', 'fromstein', 'gitano']\n"
     ]
    }
   ],
   "source": [
    "print('Total of Training Data:', len(train_data))\n",
    "print('Word Examples:', id_to_word(train_data[0:8]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iterator = reader.ptb_iterator(train_data, batch_size, num_steps)\n",
    "first_tupple = iterator.__next__()\n",
    "X = first_tupple[0]\n",
    "y = first_tupple[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_data = tf.placeholder(tf.int32, [batch_size, num_steps])\n",
    "targets = tf.placeholder(tf.int32, [batch_size, num_steps])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create LSTM Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LSTM_cells = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cell = tf.contrib.rnn.BasicLSTMCell(hidden_size_l1, forget_bias=0.0)\n",
    "LSTM_cells.append(cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cell = tf.contrib.rnn.BasicLSTMCell(hidden_size_l2, forget_bias=0.0)\n",
    "LSTM_cells.append(cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stacked_LSTM = tf.contrib.rnn.MultiRNNCell(LSTM_cells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "initial_state = stacked_LSTM.zero_state(batch_size, tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create The Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_vocab = tf.get_variable('embedding_vocab', [vocab_size, embedding_vector_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs = tf.nn.embedding_lookup(embedding_vocab, input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lstm_out, hidden_state = tf.nn.dynamic_rnn(stacked_LSTM, inputs, initial_state=initial_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output = tf.reshape(lstm_out, [-1, hidden_size_l2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "softmax_W = tf.get_variable('softmax_W', [hidden_size_l2, vocab_size])\n",
    "softmax_b = tf.get_variable('softmax_b', [vocab_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_op = tf.matmul(output, softmax_W) + softmax_b\n",
    "logits_op = tf.reshape(logits_op, [batch_size, num_steps, vocab_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "probs_op = tf.nn.softmax(logits_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_op = tf.argmax(probs_op, axis=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_op = tf.contrib.seq2seq.sequence_loss(logits_op, targets, tf.ones([batch_size, num_steps], dtype=tf.float32), \n",
    "                                           average_across_timesteps=False, average_across_batch=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_op = tf.reduce_sum(loss_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = tf.Variable(0.0, trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_vars = tf.trainable_variables()\n",
    "grads, _ = tf.clip_by_global_norm(tf.gradients(loss_op, train_vars), max_grad_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer_op = tf.train.GradientDescentOptimizer(lr)\n",
    "optimizer_op = optimizer_op.apply_gradients(zip(grads, train_vars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train The Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_model(data, optimizer_op, verbose=False):\n",
    "    \n",
    "    epoch_size = ((len(data) // batch_size) - 1) // num_steps\n",
    "    start_time = time.time()\n",
    "\n",
    "    losses = 0.0\n",
    "    iters = 0\n",
    "\n",
    "    state = sess.run(initial_state)\n",
    "\n",
    "    for step, (X, y) in enumerate(reader.ptb_iterator(data, batch_size, num_steps)):\n",
    "\n",
    "        loss, state, words, _ = sess.run([loss_op, hidden_state, words_op, optimizer_op], \n",
    "                                          feed_dict={input_data: X, targets: y, initial_state: state})\n",
    "        losses += loss\n",
    "        iters += num_steps\n",
    "\n",
    "        if verbose and step % (epoch_size // 10) == 10:\n",
    "            speed = iters * batch_size / (time.time() - start_time)\n",
    "            print(f'Iteration: {step}/ {epoch_size}, Perplexity: {np.exp(losses/iters):.3f}, Speed: {speed:.0f} wps')\n",
    "\n",
    "    perplexity = np.exp(losses / iters)\n",
    "    \n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Learning Rate: 1.000\n",
      "Iteration: 10/ 774, Perplexity: 4403.066, Speed: 1705 wps\n",
      "Iteration: 87/ 774, Perplexity: 1289.160, Speed: 1591 wps\n",
      "Iteration: 164/ 774, Perplexity: 1041.208, Speed: 1472 wps\n",
      "Iteration: 241/ 774, Perplexity: 892.711, Speed: 1253 wps\n",
      "Iteration: 318/ 774, Perplexity: 800.996, Speed: 1197 wps\n",
      "Iteration: 395/ 774, Perplexity: 727.190, Speed: 1294 wps\n",
      "Iteration: 472/ 774, Perplexity: 669.708, Speed: 1375 wps\n",
      "Iteration: 549/ 774, Perplexity: 616.279, Speed: 1438 wps\n",
      "Iteration: 626/ 774, Perplexity: 571.154, Speed: 1460 wps\n",
      "Iteration: 703/ 774, Perplexity: 533.093, Speed: 1486 wps\n",
      "Epoch 1, Train Perplexity: 504.587058\n",
      "Epoch 1, Valid Perplexity: 306.678596\n",
      "Epoch: 2, Learning Rate: 1.000\n",
      "Iteration: 10/ 774, Perplexity: 318.602, Speed: 2045 wps\n",
      "Iteration: 87/ 774, Perplexity: 272.840, Speed: 2033 wps\n",
      "Iteration: 164/ 774, Perplexity: 260.731, Speed: 2044 wps\n",
      "Iteration: 241/ 774, Perplexity: 248.666, Speed: 1997 wps\n",
      "Iteration: 318/ 774, Perplexity: 244.548, Speed: 1922 wps\n",
      "Iteration: 395/ 774, Perplexity: 237.175, Speed: 1899 wps\n",
      "Iteration: 472/ 774, Perplexity: 231.641, Speed: 1900 wps\n",
      "Iteration: 549/ 774, Perplexity: 223.695, Speed: 1918 wps\n",
      "Iteration: 626/ 774, Perplexity: 217.164, Speed: 1938 wps\n",
      "Iteration: 703/ 774, Perplexity: 211.931, Speed: 1953 wps\n",
      "Epoch 2, Train Perplexity: 208.355291\n",
      "Epoch 2, Valid Perplexity: 186.537925\n",
      "Epoch: 3, Learning Rate: 1.000\n",
      "Iteration: 10/ 774, Perplexity: 202.588, Speed: 1880 wps\n",
      "Iteration: 87/ 774, Perplexity: 174.678, Speed: 1843 wps\n",
      "Iteration: 164/ 774, Perplexity: 169.448, Speed: 1887 wps\n",
      "Iteration: 241/ 774, Perplexity: 164.003, Speed: 1940 wps\n",
      "Iteration: 318/ 774, Perplexity: 164.072, Speed: 1973 wps\n",
      "Iteration: 395/ 774, Perplexity: 161.128, Speed: 1991 wps\n",
      "Iteration: 472/ 774, Perplexity: 159.419, Speed: 1987 wps\n",
      "Iteration: 549/ 774, Perplexity: 155.451, Speed: 1955 wps\n",
      "Iteration: 626/ 774, Perplexity: 152.377, Speed: 1934 wps\n",
      "Iteration: 703/ 774, Perplexity: 150.340, Speed: 1931 wps\n",
      "Epoch 3, Train Perplexity: 149.181451\n",
      "Epoch 3, Valid Perplexity: 163.257417\n",
      "Epoch: 4, Learning Rate: 1.000\n",
      "Iteration: 10/ 774, Perplexity: 161.876, Speed: 2068 wps\n",
      "Iteration: 87/ 774, Perplexity: 138.601, Speed: 2070 wps\n",
      "Iteration: 164/ 774, Perplexity: 135.541, Speed: 1950 wps\n",
      "Iteration: 241/ 774, Perplexity: 131.959, Speed: 1923 wps\n",
      "Iteration: 318/ 774, Perplexity: 132.622, Speed: 1901 wps\n",
      "Iteration: 395/ 774, Perplexity: 130.655, Speed: 1916 wps\n",
      "Iteration: 472/ 774, Perplexity: 129.792, Speed: 1942 wps\n",
      "Iteration: 549/ 774, Perplexity: 126.891, Speed: 1962 wps\n",
      "Iteration: 626/ 774, Perplexity: 124.821, Speed: 1937 wps\n",
      "Iteration: 703/ 774, Perplexity: 123.642, Speed: 1876 wps\n",
      "Epoch 4, Train Perplexity: 123.102769\n",
      "Epoch 4, Valid Perplexity: 148.089507\n",
      "Epoch: 5, Learning Rate: 0.500\n",
      "Iteration: 10/ 774, Perplexity: 134.203, Speed: 1695 wps\n",
      "Iteration: 87/ 774, Perplexity: 114.404, Speed: 1719 wps\n",
      "Iteration: 164/ 774, Perplexity: 111.392, Speed: 1729 wps\n",
      "Iteration: 241/ 774, Perplexity: 108.069, Speed: 1773 wps\n",
      "Iteration: 318/ 774, Perplexity: 108.373, Speed: 1705 wps\n",
      "Iteration: 395/ 774, Perplexity: 106.342, Speed: 1659 wps\n",
      "Iteration: 472/ 774, Perplexity: 105.567, Speed: 1621 wps\n",
      "Iteration: 549/ 774, Perplexity: 102.867, Speed: 1665 wps\n",
      "Iteration: 626/ 774, Perplexity: 100.935, Speed: 1695 wps\n",
      "Iteration: 703/ 774, Perplexity: 99.793, Speed: 1700 wps\n",
      "Epoch 5, Train Perplexity: 99.215125\n",
      "Epoch 5, Valid Perplexity: 134.573996\n",
      "Epoch: 6, Learning Rate: 0.125\n",
      "Iteration: 10/ 774, Perplexity: 117.726, Speed: 898 wps\n",
      "Iteration: 87/ 774, Perplexity: 102.006, Speed: 1239 wps\n",
      "Iteration: 164/ 774, Perplexity: 99.651, Speed: 1423 wps\n",
      "Iteration: 241/ 774, Perplexity: 96.693, Speed: 1514 wps\n",
      "Iteration: 318/ 774, Perplexity: 96.987, Speed: 1523 wps\n",
      "Iteration: 395/ 774, Perplexity: 94.960, Speed: 1534 wps\n",
      "Iteration: 472/ 774, Perplexity: 94.095, Speed: 1546 wps\n",
      "Iteration: 549/ 774, Perplexity: 91.414, Speed: 1557 wps\n",
      "Iteration: 626/ 774, Perplexity: 89.361, Speed: 1568 wps\n",
      "Iteration: 703/ 774, Perplexity: 87.958, Speed: 1569 wps\n",
      "Epoch 6, Train Perplexity: 87.055931\n",
      "Epoch 6, Valid Perplexity: 127.985901\n",
      "Epoch: 7, Learning Rate: 0.016\n",
      "Iteration: 10/ 774, Perplexity: 109.798, Speed: 1397 wps\n",
      "Iteration: 87/ 774, Perplexity: 96.567, Speed: 1509 wps\n",
      "Iteration: 164/ 774, Perplexity: 94.754, Speed: 1600 wps\n",
      "Iteration: 241/ 774, Perplexity: 92.192, Speed: 1642 wps\n",
      "Iteration: 318/ 774, Perplexity: 92.649, Speed: 1632 wps\n",
      "Iteration: 395/ 774, Perplexity: 90.804, Speed: 1505 wps\n",
      "Iteration: 472/ 774, Perplexity: 90.029, Speed: 1473 wps\n",
      "Iteration: 549/ 774, Perplexity: 87.439, Speed: 1467 wps\n",
      "Iteration: 626/ 774, Perplexity: 85.397, Speed: 1470 wps\n",
      "Iteration: 703/ 774, Perplexity: 83.921, Speed: 1468 wps\n",
      "Epoch 7, Train Perplexity: 82.882474\n",
      "Epoch 7, Valid Perplexity: 125.996764\n",
      "Epoch: 8, Learning Rate: 0.001\n",
      "Iteration: 10/ 774, Perplexity: 108.018, Speed: 1719 wps\n",
      "Iteration: 87/ 774, Perplexity: 94.944, Speed: 1796 wps\n",
      "Iteration: 164/ 774, Perplexity: 93.362, Speed: 1779 wps\n",
      "Iteration: 241/ 774, Perplexity: 90.945, Speed: 1801 wps\n",
      "Iteration: 318/ 774, Perplexity: 91.491, Speed: 1818 wps\n",
      "Iteration: 395/ 774, Perplexity: 89.720, Speed: 1808 wps\n",
      "Iteration: 472/ 774, Perplexity: 88.998, Speed: 1718 wps\n",
      "Iteration: 549/ 774, Perplexity: 86.473, Speed: 1704 wps\n",
      "Iteration: 626/ 774, Perplexity: 84.459, Speed: 1711 wps\n",
      "Iteration: 703/ 774, Perplexity: 83.003, Speed: 1692 wps\n",
      "Epoch 8, Train Perplexity: 81.964388\n",
      "Epoch 8, Valid Perplexity: 125.730888\n",
      "Epoch: 9, Learning Rate: 0.000\n",
      "Iteration: 10/ 774, Perplexity: 107.836, Speed: 1710 wps\n",
      "Iteration: 87/ 774, Perplexity: 94.780, Speed: 1648 wps\n",
      "Iteration: 164/ 774, Perplexity: 93.201, Speed: 1627 wps\n",
      "Iteration: 241/ 774, Perplexity: 90.803, Speed: 1565 wps\n",
      "Iteration: 318/ 774, Perplexity: 91.360, Speed: 1529 wps\n",
      "Iteration: 395/ 774, Perplexity: 89.600, Speed: 1528 wps\n",
      "Iteration: 472/ 774, Perplexity: 88.884, Speed: 1554 wps\n",
      "Iteration: 549/ 774, Perplexity: 86.369, Speed: 1551 wps\n",
      "Iteration: 626/ 774, Perplexity: 84.362, Speed: 1567 wps\n",
      "Iteration: 703/ 774, Perplexity: 82.912, Speed: 1550 wps\n",
      "Epoch 9, Train Perplexity: 81.875528\n",
      "Epoch 9, Valid Perplexity: 125.723762\n",
      "Epoch: 10, Learning Rate: 0.000\n",
      "Iteration: 10/ 774, Perplexity: 107.831, Speed: 1081 wps\n",
      "Iteration: 87/ 774, Perplexity: 94.775, Speed: 1622 wps\n",
      "Iteration: 164/ 774, Perplexity: 93.197, Speed: 1620 wps\n",
      "Iteration: 241/ 774, Perplexity: 90.799, Speed: 1599 wps\n",
      "Iteration: 318/ 774, Perplexity: 91.356, Speed: 1563 wps\n",
      "Iteration: 395/ 774, Perplexity: 89.596, Speed: 1616 wps\n",
      "Iteration: 472/ 774, Perplexity: 88.881, Speed: 1643 wps\n",
      "Iteration: 549/ 774, Perplexity: 86.366, Speed: 1670 wps\n",
      "Iteration: 626/ 774, Perplexity: 84.359, Speed: 1697 wps\n",
      "Iteration: 703/ 774, Perplexity: 82.909, Speed: 1709 wps\n",
      "Epoch 10, Train Perplexity: 81.872817\n",
      "Epoch 10, Valid Perplexity: 125.723712\n",
      "Epoch: 11, Learning Rate: 0.000\n",
      "Iteration: 10/ 774, Perplexity: 107.831, Speed: 1829 wps\n",
      "Iteration: 87/ 774, Perplexity: 94.775, Speed: 1873 wps\n",
      "Iteration: 164/ 774, Perplexity: 93.197, Speed: 1897 wps\n",
      "Iteration: 241/ 774, Perplexity: 90.799, Speed: 1891 wps\n",
      "Iteration: 318/ 774, Perplexity: 91.356, Speed: 1905 wps\n",
      "Iteration: 395/ 774, Perplexity: 89.596, Speed: 1901 wps\n",
      "Iteration: 472/ 774, Perplexity: 88.881, Speed: 1894 wps\n",
      "Iteration: 549/ 774, Perplexity: 86.366, Speed: 1901 wps\n",
      "Iteration: 626/ 774, Perplexity: 84.359, Speed: 1888 wps\n",
      "Iteration: 703/ 774, Perplexity: 82.909, Speed: 1894 wps\n",
      "Epoch 11, Train Perplexity: 81.872793\n",
      "Epoch 11, Valid Perplexity: 125.723708\n",
      "Epoch: 12, Learning Rate: 0.000\n",
      "Iteration: 10/ 774, Perplexity: 107.831, Speed: 2000 wps\n",
      "Iteration: 87/ 774, Perplexity: 94.775, Speed: 1814 wps\n",
      "Iteration: 164/ 774, Perplexity: 93.197, Speed: 1864 wps\n",
      "Iteration: 241/ 774, Perplexity: 90.799, Speed: 1890 wps\n",
      "Iteration: 318/ 774, Perplexity: 91.356, Speed: 1900 wps\n",
      "Iteration: 395/ 774, Perplexity: 89.596, Speed: 1894 wps\n",
      "Iteration: 472/ 774, Perplexity: 88.881, Speed: 1901 wps\n",
      "Iteration: 549/ 774, Perplexity: 86.366, Speed: 1893 wps\n",
      "Iteration: 626/ 774, Perplexity: 84.359, Speed: 1901 wps\n",
      "Iteration: 703/ 774, Perplexity: 82.909, Speed: 1904 wps\n",
      "Epoch 12, Train Perplexity: 81.872793\n",
      "Epoch 12, Valid Perplexity: 125.723708\n",
      "Epoch: 13, Learning Rate: 0.000\n",
      "Iteration: 10/ 774, Perplexity: 107.831, Speed: 1886 wps\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 87/ 774, Perplexity: 94.775, Speed: 1941 wps\n",
      "Iteration: 164/ 774, Perplexity: 93.197, Speed: 1933 wps\n",
      "Iteration: 241/ 774, Perplexity: 90.799, Speed: 1930 wps\n",
      "Iteration: 318/ 774, Perplexity: 91.356, Speed: 1905 wps\n",
      "Iteration: 395/ 774, Perplexity: 89.596, Speed: 1783 wps\n",
      "Iteration: 472/ 774, Perplexity: 88.881, Speed: 1820 wps\n",
      "Iteration: 549/ 774, Perplexity: 86.366, Speed: 1824 wps\n",
      "Iteration: 626/ 774, Perplexity: 84.359, Speed: 1807 wps\n",
      "Iteration: 703/ 774, Perplexity: 82.909, Speed: 1788 wps\n",
      "Epoch 13, Train Perplexity: 81.872793\n",
      "Epoch 13, Valid Perplexity: 125.723708\n",
      "Epoch: 14, Learning Rate: 0.000\n",
      "Iteration: 10/ 774, Perplexity: 107.831, Speed: 1890 wps\n",
      "Iteration: 87/ 774, Perplexity: 94.775, Speed: 1951 wps\n",
      "Iteration: 164/ 774, Perplexity: 93.197, Speed: 1998 wps\n",
      "Iteration: 241/ 774, Perplexity: 90.799, Speed: 1951 wps\n",
      "Iteration: 318/ 774, Perplexity: 91.356, Speed: 1948 wps\n",
      "Iteration: 395/ 774, Perplexity: 89.596, Speed: 1927 wps\n",
      "Iteration: 472/ 774, Perplexity: 88.881, Speed: 1903 wps\n",
      "Iteration: 549/ 774, Perplexity: 86.366, Speed: 1897 wps\n",
      "Iteration: 626/ 774, Perplexity: 84.359, Speed: 1907 wps\n",
      "Iteration: 703/ 774, Perplexity: 82.909, Speed: 1906 wps\n",
      "Epoch 14, Train Perplexity: 81.872793\n",
      "Epoch 14, Valid Perplexity: 125.723708\n",
      "Epoch: 15, Learning Rate: 0.000\n",
      "Iteration: 10/ 774, Perplexity: 107.831, Speed: 2010 wps\n",
      "Iteration: 87/ 774, Perplexity: 94.775, Speed: 2059 wps\n",
      "Iteration: 164/ 774, Perplexity: 93.197, Speed: 2020 wps\n",
      "Iteration: 241/ 774, Perplexity: 90.799, Speed: 2014 wps\n",
      "Iteration: 318/ 774, Perplexity: 91.356, Speed: 2015 wps\n",
      "Iteration: 395/ 774, Perplexity: 89.596, Speed: 1986 wps\n",
      "Iteration: 472/ 774, Perplexity: 88.881, Speed: 1999 wps\n",
      "Iteration: 549/ 774, Perplexity: 86.366, Speed: 2006 wps\n",
      "Iteration: 626/ 774, Perplexity: 84.359, Speed: 2014 wps\n",
      "Iteration: 703/ 774, Perplexity: 82.909, Speed: 1999 wps\n",
      "Epoch 15, Train Perplexity: 81.872793\n",
      "Epoch 15, Valid Perplexity: 125.723708\n",
      "Training LSTM Model is done. Test Perplexity: 122.423\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    \n",
    "    sess.run(init_op) # run the init_op using an interactive session\n",
    "\n",
    "    initializer = tf.random_uniform_initializer(-init_scale, init_scale)\n",
    "    \n",
    "    for i_epoch in range(1, num_epochs+1):\n",
    "    \n",
    "        # define the decay for this epoch\n",
    "        lr_decay = decay ** max(i_epoch - max_epoch_decay_lr, 0.0)\n",
    "\n",
    "        learning_rate = tf.assign(lr, learning_rate*lr_decay); learning_rate = sess.run(learning_rate)\n",
    "        print(f'Epoch: {i_epoch}, Learning Rate: {learning_rate:.3f}')\n",
    "\n",
    "        # run the loop for this epoch in the training model\n",
    "        train_perplexity = run_model(train_data, optimizer_op, verbose=True)\n",
    "        print(f'Epoch {i_epoch}, Train Perplexity: {train_perplexity:3f}')\n",
    "        \n",
    "        # run the loop for this epoch in the validation model\n",
    "        valid_perplexity = run_model(valid_data, tf.no_op())\n",
    "        print(f'Epoch {i_epoch}, Valid Perplexity: {valid_perplexity:3f}')\n",
    "        \n",
    "    # run the loop in the testing model to see how effective was our training\n",
    "    test_perplexity = run_model(test_data, tf.no_op())\n",
    "    print(f'Training LSTM Model is done. Test Perplexity: {test_perplexity:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
