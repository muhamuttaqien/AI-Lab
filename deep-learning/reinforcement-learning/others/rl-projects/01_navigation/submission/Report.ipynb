{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Udacity - Navigation Project Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This report explains about the result of Navigation Project as the 1st project of Deep Reinforcement Learning [Udacity](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) Course. Written by [muhamuttaqien](https://github.com/muhamuttaqien)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/yellow_bananas_collector.gif'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Learning Algorithm & The Choosen Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Navigation project, I tried to solve the environment using one of the most state-of-the-art algorithm, called **Deep Q-Networks**, instead of traditional RL techniques that use a finite MDP (which in this case the agent has no any knowledge of the MDP) to model an environment and limits to only discrete state and actions spaces. Deep Q-Networks algorithms is basically using non-linear function approximators to calculate the value of actions based directly on observation from the environment and represented as a Deep Neural Network. The approximating function can either map a state to its value, or a state-action pair to the corresponding Q value for every possible action and by using neural network, it can help capture complex non-linear relationships across combination of features between input state and output value. Since Reinforcement Learning method is notoriously unstable when neural networks are used to represent the action values, I tried to solve the instabilities issue by applying two key features during learning process, namely **Experience Replay** and **Fixed Q-targets**.\n",
    "\n",
    "1. Experience Replay\n",
    "   * Consider the very basic online Q-learning algorithm where the agent interacts with the environment and at each time step, the agent obtain a state, action, reward and next state tuple, just learn from it and then directly discard it then moves on to the next new tuple in the following time step. But this seems will little wasteful. The main idea is the agent could actually learn more from those experience tuples if they are stored somewhere, moreover, some states are pretty rare to come by and some actions can be pretty costly. So it would be nice if we can recall such experiences for learning process. This is exactly what a experience replay allows us to do where we store each experience tuple in the buffer as the agent keep interacting with the environment and then randomly sample a small batch of tuples from it in order to learn. This random sampling also prevent action values from oscillating or diverging catastrophically due to high correlation problem caused by learning those experience tuples in sequential order. I implemented this feature by defining dedicated class named `ReplayBuffer.`\n",
    "   \n",
    "2. Fixed-Q-targets\n",
    "   * In Deep Q-Network, when we want to calculate the TD error (loss of learning), we can calculate the difference between TD target (Q target) and the current estimation of Q value. Since the real TD target is unknown yet, we need to estimate it by using the Bellman equation. The TD target is actually just the reward of taking an action at certain state plus the discounted highest Q value for the next state. However, there is a big correlation problem when we use the same parameters (weights) for estimating the Q target and the Q value since it means that at every step of training, the Q values shift but also the Q targets shift. When we're getting closer to our target but the target is also moving. This leads to a big oscillation in training. Instead, we can use the idea of fixed Q-targets by using a separate network with a fixed parameters for estimating the Q target. At every certain step, we can copy the parameters from our DQN network to update the separate network. The implementation of fixed-q-targets can be noticed within `learn` function of class named `DQNAgent`.\n",
    "   \n",
    "Moreover, I also here implemented other improvement variants of Deep Q-Networks algorithm such as **Double DQN** and **Dueling DQN**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters chosen for optimally training the DQN network was that `100000` for buffer size. We set that number as maximum memory capacity for the agent to memorize all experiences obtained during learning. When the capacity reached the number of batch size, which is `64`, then the buffer randomly sampled the experiences for the agent to learn from the past. The learning step was performed for every 4 step done by the agent in order to gradually update the parameters (weights) of DQN network by learning rate `0.0005`. We set gamma value `0.99` since the agent was supposed to care about future rewards and eventually collected yellow banana as many as possible. Lastly, since we use separate network for estimating Q target, we set `0.001` as tau value so we can gradually copy the parameters of our trained DQN network to update the separate network and made the learning algorithm more stable. These parameters was defined on `dqn_agent.py` file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Architecture for Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model architecture implemented here did't use convolutional neural network that takes in the difference between the current and previous screen patches. Instead, I used a fully connected layer that takes state vectors containing information the agent observed as input and then outputs Q value for each available action can be performed on the environment. The model tried to predict the expected return of taking each action given the current input, mapping state into action values.\n",
    "\n",
    "The Neural Network model defined on `model.py` file consists of three fully connected layers, with 64 neuron units on the hidden layer. The network uses rectifier activation function called ReLU to introduce non-linearity for each hidden layers. The function is a simple calculation that returns the value provided as input directly, or the value 0.0 if the input is 0.0 or less. Also, I set the seed on the network to allow reproducible randomness or stochasticity behaviour to the neural network. When the DQN agent was initialized, it automatically set Adam as the optimizer that performs stochastic gradient descent procedure to update network weights during training, and mean squared error to calculate the average of the squares of the difference between Q target and estimation of Q value. All work was super easily done by [PyTorch](http://pytorch.org/) Deep Learning framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Plot of Rewards & Number of Episodes Needed to Solve The Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I set the total of episodes and max time both 1000 as plotted on the figure below and the result showed that the total scores moderately increased overtime as the more episodes taken by the agent to observe the environment. During learning process, I allow the agent to perform epsilon-greedy policy with the settings eps_start 1.0, eps_end 0.01 and eps_decay 0.995 as simple alternative to greedy action selection that behaves greedily most of the time, but every once in a while with small probability epsilon, instead select randomly from among all the possible actions with equal probability. This technique avoids exploration-exploitation dilemma that is the most common issue occurs in RL world."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src='./images/plot_of_dqn_agent_evaluation.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result, my agent finally solved the task to colect yellow bananas with average score 13 in 405 episodes, which is very good and outperfroms the benchmark result (1800 episodes). We can see the statistics of obtained scores progress on `Navigation` jupyter notebook, and also the demonstration of how the trained agent smartly behaves to the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Future Ideas for Improving The Agent's Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the date of submission, I plan some further improvement ideas that focus on utilizing Deep Q-Networks as one of powerful algorithms on Reinforcement Learning world. I'm eager to apply other variants of DQN such as Prioritized-Replay DQN, Noisy DQN and Pixels-to-Actions CNN-DQN which are considered as the extension of vanilla DQN that will perform better. Moreoever, I'm also interested in playing with some hyper-parameters of DQN such as `epsilon`, `gamma`, `tau`, `memory capacity` and `max times` taken by the agent that looks promising to improve the agent's performance and gain higher total scores on the environment. I'm excited about!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Github Link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My workplace repository can be easily accessed from [muhamuttaqien](https://github.com/muhamuttaqien/AI-Lab/tree/master/deep-learning/reinforcement-learning/projects/01_navigation/submission) Github profile."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
