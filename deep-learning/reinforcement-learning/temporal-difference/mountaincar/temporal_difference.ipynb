{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.collections as mc\n",
    "\n",
    "import gym\n",
    "from mountaincar import MountainCarEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')\n",
    "np.set_printoptions(precision=3, linewidth=120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore The Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = MountainCarEnv(); env.seed(90);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Environment Display:')\n",
    "state = env.reset() # reset environment to a new, random state\n",
    "score = 0\n",
    "\n",
    "for time_step in range(200):\n",
    "    \n",
    "    action = env.action_space.sample()\n",
    "    env.render()\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    score += reward\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "print('State space', env.observation_space)\n",
    "print('Action space', env.action_space)\n",
    "print('Final score:', score)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore state (observation) space\n",
    "print('State space:', env.observation_space)\n",
    "print('- Low:', env.observation_space.low)\n",
    "print('- High:', env.observation_space.high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the action space\n",
    "print(\"Action space:\", env.action_space)\n",
    "print('0 Push left')\n",
    "print('1 No push')\n",
    "print('2 Push right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discretize the State Space with a Uniform Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_uniform_grid(low, high, bins=(10, 10)):\n",
    "    \"\"\"Define a uniformly-spaced grid that can be used to discretize a space.\"\"\"\n",
    "    \n",
    "    grid = [np.linspace(low[dim], high[dim], bins[dim] + 1)[1:-1] for dim in range(len(bins))]\n",
    "    return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discretize(sample, grid):\n",
    "    \"\"\"Discretize a sample as per given grid.\"\"\"\n",
    "    return list(int(np.digitize(s, g)) for s, g in zip(sample, grid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "low = [-1.0, -5.0]\n",
    "high = [1.0, 5.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grid = create_uniform_grid(low, high)\n",
    "samples = np.array([[-0.8 , -4.8], [-0.81, -4.1], [-0.8 , -4.0], [-0.5 ,  0.0], \n",
    "                    [ 0.2 , -1.9], [ 0.8 ,  4.0], [ 0.81,  4.1], [ 0.73 ,  4.32]])\n",
    "\n",
    "discretized_samples = np.array([discretize(sample, grid) for sample in samples])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize The Discritization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def visualize_samples(samples, discretized_samples, grid, low=None, high=None, env=None):\n",
    "    \"\"\"Visualize original and discretized samples on a given 2-dimensional grid.\"\"\"\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    \n",
    "    # show grid\n",
    "    ax.xaxis.set_major_locator(plt.FixedLocator(grid[0]))\n",
    "    ax.yaxis.set_major_locator(plt.FixedLocator(grid[1]))\n",
    "    ax.grid(True)\n",
    "    \n",
    "    # if bounds (low, high) are specified, use them to set axis limits\n",
    "    if low is not None and high is not None:\n",
    "        ax.set_xlim(low[0], high[0])\n",
    "        ax.set_ylim(low[1], high[1])\n",
    "    else:\n",
    "        # otherwise use first, list grid locations as low, high (for further mapping discretized samples)\n",
    "        low = [splits[0] for splits in grid]\n",
    "        high = [splits[-1] for splits in grid]\n",
    "        \n",
    "    # map each discretized sample (which is really an index) to the center of corresponding grid cell\n",
    "    grid_extended = np.hstack((np.array([low]).T, grid, np.array([high]).T))  # add low and high ends\n",
    "    grid_centers = (grid_extended[:, 1:] + grid_extended[:, :-1]) / 2  # compute center of each grid cell\n",
    "    locs = np.stack(grid_centers[i, discretized_samples[:, i]] for i in range(len(grid))).T  # map discretized samples\n",
    "    \n",
    "    ax.plot(samples[:, 0], samples[:, 1], 'o')  # plot original samples\n",
    "    ax.plot(locs[:, 0], locs[:, 1], 's')  # plot discretized samples in mapped locations\n",
    "    ax.add_collection(mc.LineCollection(list(zip(samples, locs)), colors='orange'))  # add a line connecting each original-discretized sample\n",
    "    ax.legend(['original', 'discretized'])\n",
    "    \n",
    "    if env == 'mountaincar':\n",
    "        # axis labels for MountainCar-v0 state space\n",
    "        plt.xlabel('position'); plt.ylabel('velocity');\n",
    "    else:\n",
    "        plt.xlabel('x-axis'); plt.ylabel('y-axis');\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "visualize_samples(samples, discretized_samples, grid, low, high)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply The Uniform Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "state_grid = create_uniform_grid(env.observation_space.low, env.observation_space.high, bins=(10, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# obtain some samples from the space, discretize them, and then visualize them\n",
    "state_samples = np.array([env.observation_space.sample() for _ in range(10)])\n",
    "discretized_state_samples = np.array([discretize(sample, state_grid) for sample in state_samples])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_samples(state_samples, discretized_state_samples, state_grid, \n",
    "                  env.observation_space.low, env.observation_space.high, env='mountaincar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    \"\"\"Q-Learning agent that can act on a continuous state space by discretizing it.\"\"\"\n",
    "    \n",
    "    def __init__(self, env, state_grid, alpha=0.02, gamma=0.99, \n",
    "                 epsilon=1.0, epsilon_decay_rate=0.9995, min_epsilon=.01, seed=505):\n",
    "        \"\"\"Initialize variables, create grid for discretization.\"\"\"        \n",
    "        pass\n",
    "    \n",
    "    def preprocess_state(self, state):\n",
    "        \"\"\"Map a continuous state to its discretized representation.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def reset_episode(self, state):\n",
    "        \"\"\"Reset variables for a new episode.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def reset_exploration(self, epsilon=None):\n",
    "        \"\"\"Reset exploration rate used when training.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def act(self, state, reward=None, done=None, mode='train'):\n",
    "        \"\"\"Pick next action and update internal Q table (when mode != 'test').\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_agent = QLearningAgent(env, state_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
