{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "from gym.envs.toy_text import discrete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "UP = 0\n",
    "RIGHT = 1\n",
    "DOWN = 2\n",
    "LEFT = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CliffWalkingEnv(discrete.DiscreteEnv):\n",
    "    \"\"\"\n",
    "    This is a simple implementation of the Gridworld Cliff\n",
    "    reinforcement learning task.\n",
    "    Adapted from Example 6.6 (page 106) from Reinforcement Learning: An Introduction\n",
    "    by Sutton and Barto:\n",
    "    http://incompleteideas.net/book/bookdraft2018jan1.pdf\n",
    "    With inspiration from:\n",
    "    https://github.com/dennybritz/reinforcement-learning/blob/master/lib/envs/cliff_walking.py\n",
    "    The board is a 4x12 matrix, with (using Numpy matrix indexing):\n",
    "        [3, 0] as the start at bottom-left\n",
    "        [3, 11] as the goal at bottom-right\n",
    "        [3, 1..10] as the cliff at bottom-center\n",
    "    Each time step incurs -1 reward, and stepping into the cliff incurs -100 reward\n",
    "    and a reset to the start. An episode terminates when the agent reaches the goal.\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class CliffWalkingEnv(discrete.DiscreteEnv):\n",
    "    metadata = {'render.modes': ['human', 'ansi']}\n",
    "\n",
    "    def __init__(self):\n",
    "        self.shape = (4, 12)\n",
    "        self.start_state_index = np.ravel_multi_index((3, 0), self.shape)\n",
    "\n",
    "        nS = np.prod(self.shape)\n",
    "        nA = 4\n",
    "\n",
    "        # Cliff Location\n",
    "        self._cliff = np.zeros(self.shape, dtype=np.bool)\n",
    "        self._cliff[3, 1:-1] = True\n",
    "\n",
    "        # Calculate transition probabilities and rewards\n",
    "        P = {}\n",
    "        for s in range(nS):\n",
    "            position = np.unravel_index(s, self.shape)\n",
    "            P[s] = {a: [] for a in range(nA)}\n",
    "            P[s][UP] = self._calculate_transition_prob(position, [-1, 0])\n",
    "            P[s][RIGHT] = self._calculate_transition_prob(position, [0, 1])\n",
    "            P[s][DOWN] = self._calculate_transition_prob(position, [1, 0])\n",
    "            P[s][LEFT] = self._calculate_transition_prob(position, [0, -1])\n",
    "\n",
    "        # Calculate initial state distribution\n",
    "        # We always start in state (3, 0)\n",
    "        isd = np.zeros(nS)\n",
    "        isd[self.start_state_index] = 1.0\n",
    "\n",
    "        super(CliffWalkingEnv, self).__init__(nS, nA, P, isd)\n",
    "\n",
    "    def _limit_coordinates(self, coord):\n",
    "        \"\"\"\n",
    "        Prevent the agent from falling out of the grid world\n",
    "        :param coord:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        coord[0] = min(coord[0], self.shape[0] - 1)\n",
    "        coord[0] = max(coord[0], 0)\n",
    "        coord[1] = min(coord[1], self.shape[1] - 1)\n",
    "        coord[1] = max(coord[1], 0)\n",
    "        return coord\n",
    "\n",
    "    def _calculate_transition_prob(self, current, delta):\n",
    "        \"\"\"\n",
    "        Determine the outcome for an action. Transition Prob is always 1.0.\n",
    "        :param current: Current position on the grid as (row, col)\n",
    "        :param delta: Change in position for transition\n",
    "        :return: (1.0, new_state, reward, done)\n",
    "        \"\"\"\n",
    "        new_position = np.array(current) + np.array(delta)\n",
    "        new_position = self._limit_coordinates(new_position).astype(int)\n",
    "        new_state = np.ravel_multi_index(tuple(new_position), self.shape)\n",
    "        if self._cliff[tuple(new_position)]:\n",
    "            return [(1.0, self.start_state_index, -100, False)]\n",
    "\n",
    "        terminal_state = (self.shape[0] - 1, self.shape[1] - 1)\n",
    "        is_done = tuple(new_position) == terminal_state\n",
    "        return [(1.0, new_state, -1, is_done)]\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        outfile = sys.stdout\n",
    "\n",
    "        for s in range(self.nS):\n",
    "            position = np.unravel_index(s, self.shape)\n",
    "            if self.s == s:\n",
    "                output = \" x \"\n",
    "            # Print terminal state\n",
    "            elif position == (3, 11):\n",
    "                output = \" T \"\n",
    "            elif self._cliff[position]:\n",
    "                output = \" C \"\n",
    "            else:\n",
    "                output = \" o \"\n",
    "\n",
    "            if position[1] == 0:\n",
    "                output = output.lstrip()\n",
    "            if position[1] == self.shape[1] - 1:\n",
    "                output = output.rstrip()\n",
    "                output += '\\n'\n",
    "\n",
    "            outfile.write(output)\n",
    "        outfile.write('\\n')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
