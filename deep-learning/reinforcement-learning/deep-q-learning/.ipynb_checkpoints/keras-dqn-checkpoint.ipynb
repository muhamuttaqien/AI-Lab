{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME = 'CartPole-v0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(ENV_NAME)\n",
    "env.seed(9)\n",
    "\n",
    "nb_actions = env.action_space.n\n",
    "np.random.seed(9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build DQN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/angga.muhammad/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                80        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 34        \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 114\n",
      "Trainable params: 114\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('linear'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "MEMORY_LIMIT = 50000\n",
    "WINDOW_LENGTH = 1\n",
    "MODEL_UPDATE = 1e-2\n",
    "NB_WARMUP=10\n",
    "NB_STEPS = 5000\n",
    "LR = 1e-3\n",
    "LOSS = 'mae'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Memory, Policy & Init DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = SequentialMemory(limit=MEMORY_LIMIT, window_length=WINDOW_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = EpsGreedyQPolicy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, policy=policy, \n",
    "               nb_steps_warmup=NB_WARMUP, target_model_update=MODEL_UPDATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.compile(Adam(lr=LR), metrics=[LOSS])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 5000 steps ...\n",
      "WARNING:tensorflow:From /Users/angga.muhammad/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "   71/5000: episode: 1, duration: 3.245s, episode steps: 71, steps per second: 22, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: -0.067 [-0.732, 0.411], loss: 0.430876, mean_absolute_error: 0.493167, mean_q: 0.140679\n",
      "  107/5000: episode: 2, duration: 0.598s, episode steps: 36, steps per second: 60, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.106 [-0.845, 0.214], loss: 0.330674, mean_absolute_error: 0.443146, mean_q: 0.331545\n",
      "  141/5000: episode: 3, duration: 0.565s, episode steps: 34, steps per second: 60, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.118 [-0.737, 0.205], loss: 0.276651, mean_absolute_error: 0.452350, mean_q: 0.499531\n",
      "  181/5000: episode: 4, duration: 0.666s, episode steps: 40, steps per second: 60, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.046 [-1.066, 0.618], loss: 0.213504, mean_absolute_error: 0.485473, mean_q: 0.727230\n",
      "  216/5000: episode: 5, duration: 0.583s, episode steps: 35, steps per second: 60, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.110 [-0.822, 0.403], loss: 0.167573, mean_absolute_error: 0.566245, mean_q: 0.992377\n",
      "  251/5000: episode: 6, duration: 0.583s, episode steps: 35, steps per second: 60, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.104 [-0.776, 0.206], loss: 0.128940, mean_absolute_error: 0.660144, mean_q: 1.260851\n",
      "  287/5000: episode: 7, duration: 0.598s, episode steps: 36, steps per second: 60, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.083 [-0.883, 0.382], loss: 0.104669, mean_absolute_error: 0.773128, mean_q: 1.535021\n",
      "  329/5000: episode: 8, duration: 0.699s, episode steps: 42, steps per second: 60, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.085 [-0.693, 0.208], loss: 0.089199, mean_absolute_error: 0.909887, mean_q: 1.817391\n",
      "  361/5000: episode: 9, duration: 0.534s, episode steps: 32, steps per second: 60, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.114 [-0.778, 0.345], loss: 0.081532, mean_absolute_error: 1.042320, mean_q: 2.095921\n",
      "  400/5000: episode: 10, duration: 0.649s, episode steps: 39, steps per second: 60, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.487 [0.000, 1.000], mean observation: -0.073 [-0.873, 0.236], loss: 0.087764, mean_absolute_error: 1.196428, mean_q: 2.359590\n",
      "  445/5000: episode: 11, duration: 0.746s, episode steps: 45, steps per second: 60, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: -0.027 [-0.898, 0.584], loss: 0.097534, mean_absolute_error: 1.368631, mean_q: 2.695738\n",
      "  471/5000: episode: 12, duration: 0.437s, episode steps: 26, steps per second: 60, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.067 [-0.912, 0.538], loss: 0.113710, mean_absolute_error: 1.505623, mean_q: 2.962045\n",
      "  492/5000: episode: 13, duration: 0.346s, episode steps: 21, steps per second: 61, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: -0.075 [-0.894, 0.438], loss: 0.137794, mean_absolute_error: 1.597760, mean_q: 3.111208\n",
      "  509/5000: episode: 14, duration: 0.281s, episode steps: 17, steps per second: 60, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: -0.106 [-1.014, 0.583], loss: 0.123756, mean_absolute_error: 1.686505, mean_q: 3.288338\n",
      "  524/5000: episode: 15, duration: 0.251s, episode steps: 15, steps per second: 60, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.086 [-1.197, 0.758], loss: 0.154281, mean_absolute_error: 1.763970, mean_q: 3.437878\n",
      "  541/5000: episode: 16, duration: 0.286s, episode steps: 17, steps per second: 59, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.588 [0.000, 1.000], mean observation: -0.085 [-1.426, 0.781], loss: 0.190804, mean_absolute_error: 1.858799, mean_q: 3.585893\n",
      "  555/5000: episode: 17, duration: 0.233s, episode steps: 14, steps per second: 60, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.643 [0.000, 1.000], mean observation: -0.095 [-1.510, 0.810], loss: 0.317878, mean_absolute_error: 1.944122, mean_q: 3.684208\n",
      "  566/5000: episode: 18, duration: 0.180s, episode steps: 11, steps per second: 61, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.125 [-1.768, 1.018], loss: 0.220915, mean_absolute_error: 1.961422, mean_q: 3.752356\n",
      "  578/5000: episode: 19, duration: 0.199s, episode steps: 12, steps per second: 60, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.097 [-1.984, 1.205], loss: 0.257589, mean_absolute_error: 2.032204, mean_q: 3.869833\n",
      "  590/5000: episode: 20, duration: 0.200s, episode steps: 12, steps per second: 60, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.114 [-1.945, 1.198], loss: 0.208784, mean_absolute_error: 2.078312, mean_q: 4.023693\n",
      "  602/5000: episode: 21, duration: 0.199s, episode steps: 12, steps per second: 60, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.115 [-1.640, 0.982], loss: 0.306382, mean_absolute_error: 2.186887, mean_q: 4.202361\n",
      "  614/5000: episode: 22, duration: 0.201s, episode steps: 12, steps per second: 60, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.112 [-1.913, 1.167], loss: 0.349844, mean_absolute_error: 2.242720, mean_q: 4.282550\n",
      "  625/5000: episode: 23, duration: 0.181s, episode steps: 11, steps per second: 61, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.143 [-1.731, 0.943], loss: 0.284369, mean_absolute_error: 2.277378, mean_q: 4.348232\n",
      "  637/5000: episode: 24, duration: 0.207s, episode steps: 12, steps per second: 58, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.101 [-1.864, 1.157], loss: 0.355003, mean_absolute_error: 2.320895, mean_q: 4.462011\n",
      "  648/5000: episode: 25, duration: 0.179s, episode steps: 11, steps per second: 62, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.108 [-1.848, 1.129], loss: 0.273732, mean_absolute_error: 2.338272, mean_q: 4.509521\n",
      "  657/5000: episode: 26, duration: 0.150s, episode steps: 9, steps per second: 60, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.144 [-2.801, 1.746], loss: 0.346705, mean_absolute_error: 2.464264, mean_q: 4.701422\n",
      "  670/5000: episode: 27, duration: 0.215s, episode steps: 13, steps per second: 60, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.099 [-1.738, 1.152], loss: 0.460138, mean_absolute_error: 2.503170, mean_q: 4.762285\n",
      "  682/5000: episode: 28, duration: 0.196s, episode steps: 12, steps per second: 61, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.122 [-1.478, 0.773], loss: 0.359288, mean_absolute_error: 2.555484, mean_q: 4.853576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  695/5000: episode: 29, duration: 0.217s, episode steps: 13, steps per second: 60, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.097 [-1.685, 1.005], loss: 0.423695, mean_absolute_error: 2.609868, mean_q: 4.969396\n",
      "  705/5000: episode: 30, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.125 [-1.531, 0.978], loss: 0.578688, mean_absolute_error: 2.711513, mean_q: 5.073770\n",
      "  723/5000: episode: 31, duration: 0.307s, episode steps: 18, steps per second: 59, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: -0.096 [-1.139, 0.738], loss: 0.569537, mean_absolute_error: 2.740453, mean_q: 5.113036\n",
      "  739/5000: episode: 32, duration: 0.259s, episode steps: 16, steps per second: 62, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.121 [-0.961, 0.552], loss: 0.388498, mean_absolute_error: 2.765262, mean_q: 5.262582\n",
      "  752/5000: episode: 33, duration: 0.217s, episode steps: 13, steps per second: 60, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: -0.094 [-1.219, 0.800], loss: 0.594672, mean_absolute_error: 2.858340, mean_q: 5.405354\n",
      "  763/5000: episode: 34, duration: 0.180s, episode steps: 11, steps per second: 61, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.101 [-1.474, 0.997], loss: 0.564750, mean_absolute_error: 2.901053, mean_q: 5.445578\n",
      "  774/5000: episode: 35, duration: 0.186s, episode steps: 11, steps per second: 59, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.090 [-2.074, 1.410], loss: 0.425040, mean_absolute_error: 2.894492, mean_q: 5.549500\n",
      "  787/5000: episode: 36, duration: 0.216s, episode steps: 13, steps per second: 60, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: -0.100 [-1.499, 0.992], loss: 0.703091, mean_absolute_error: 3.034275, mean_q: 5.681404\n",
      "  803/5000: episode: 37, duration: 0.262s, episode steps: 16, steps per second: 61, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.089 [-1.204, 0.815], loss: 0.518141, mean_absolute_error: 3.038801, mean_q: 5.722808\n",
      "  821/5000: episode: 38, duration: 0.315s, episode steps: 18, steps per second: 57, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.082 [-0.937, 0.586], loss: 0.596699, mean_absolute_error: 3.108805, mean_q: 5.872601\n",
      "  832/5000: episode: 39, duration: 0.186s, episode steps: 11, steps per second: 59, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.122 [-1.723, 0.958], loss: 0.895685, mean_absolute_error: 3.225121, mean_q: 6.023602\n",
      "  847/5000: episode: 40, duration: 0.248s, episode steps: 15, steps per second: 61, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.091 [-1.457, 0.840], loss: 0.737128, mean_absolute_error: 3.236213, mean_q: 6.035600\n",
      "  861/5000: episode: 41, duration: 0.233s, episode steps: 14, steps per second: 60, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.112 [-0.971, 0.618], loss: 0.837167, mean_absolute_error: 3.296447, mean_q: 6.102393\n",
      "  878/5000: episode: 42, duration: 0.288s, episode steps: 17, steps per second: 59, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: -0.086 [-0.957, 0.568], loss: 0.927853, mean_absolute_error: 3.355808, mean_q: 6.203304\n",
      "  891/5000: episode: 43, duration: 0.217s, episode steps: 13, steps per second: 60, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.099 [-1.721, 0.957], loss: 0.787146, mean_absolute_error: 3.380875, mean_q: 6.302860\n",
      "  903/5000: episode: 44, duration: 0.196s, episode steps: 12, steps per second: 61, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.099 [-1.551, 0.986], loss: 1.020115, mean_absolute_error: 3.461699, mean_q: 6.442167\n",
      "  914/5000: episode: 45, duration: 0.182s, episode steps: 11, steps per second: 60, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.099 [-1.567, 1.022], loss: 0.749163, mean_absolute_error: 3.442784, mean_q: 6.457051\n",
      "  927/5000: episode: 46, duration: 0.219s, episode steps: 13, steps per second: 59, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: -0.122 [-1.284, 0.738], loss: 0.732063, mean_absolute_error: 3.480241, mean_q: 6.561091\n",
      "  942/5000: episode: 47, duration: 0.248s, episode steps: 15, steps per second: 61, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: -0.082 [-1.214, 0.769], loss: 1.014578, mean_absolute_error: 3.586394, mean_q: 6.652349\n",
      "  962/5000: episode: 48, duration: 0.332s, episode steps: 20, steps per second: 60, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.084 [-1.082, 0.749], loss: 0.949887, mean_absolute_error: 3.610548, mean_q: 6.632220\n",
      "  978/5000: episode: 49, duration: 0.267s, episode steps: 16, steps per second: 60, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.103 [-0.937, 0.630], loss: 0.669764, mean_absolute_error: 3.605912, mean_q: 6.781614\n",
      "  993/5000: episode: 50, duration: 0.265s, episode steps: 15, steps per second: 57, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: -0.090 [-1.166, 0.742], loss: 0.882086, mean_absolute_error: 3.703236, mean_q: 6.977137\n",
      " 1008/5000: episode: 51, duration: 0.232s, episode steps: 15, steps per second: 65, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: -0.065 [-1.223, 0.818], loss: 0.889432, mean_absolute_error: 3.749574, mean_q: 7.027860\n",
      " 1021/5000: episode: 52, duration: 0.225s, episode steps: 13, steps per second: 58, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: -0.102 [-1.204, 0.744], loss: 0.910376, mean_absolute_error: 3.779217, mean_q: 7.067062\n",
      " 1035/5000: episode: 53, duration: 0.231s, episode steps: 14, steps per second: 61, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.095 [-1.423, 0.947], loss: 0.976545, mean_absolute_error: 3.828141, mean_q: 7.135676\n",
      " 1047/5000: episode: 54, duration: 0.193s, episode steps: 12, steps per second: 62, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.108 [-1.475, 0.974], loss: 0.779348, mean_absolute_error: 3.826098, mean_q: 7.265872\n",
      " 1058/5000: episode: 55, duration: 0.182s, episode steps: 11, steps per second: 60, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: -0.119 [-1.232, 0.772], loss: 0.944142, mean_absolute_error: 3.892281, mean_q: 7.382309\n",
      " 1071/5000: episode: 56, duration: 0.218s, episode steps: 13, steps per second: 60, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: -0.106 [-1.244, 0.832], loss: 1.046512, mean_absolute_error: 3.944650, mean_q: 7.402929\n",
      " 1083/5000: episode: 57, duration: 0.199s, episode steps: 12, steps per second: 60, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.080 [-1.525, 1.020], loss: 0.947365, mean_absolute_error: 3.942673, mean_q: 7.404459\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1098/5000: episode: 58, duration: 0.248s, episode steps: 15, steps per second: 60, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: -0.089 [-1.264, 0.810], loss: 1.139373, mean_absolute_error: 4.025370, mean_q: 7.495666\n",
      " 1114/5000: episode: 59, duration: 0.266s, episode steps: 16, steps per second: 60, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.120 [-0.974, 0.544], loss: 0.908806, mean_absolute_error: 4.022348, mean_q: 7.592436\n",
      " 1133/5000: episode: 60, duration: 0.322s, episode steps: 19, steps per second: 59, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.096 [-1.069, 0.590], loss: 1.005674, mean_absolute_error: 4.108882, mean_q: 7.726799\n",
      " 1153/5000: episode: 61, duration: 0.102s, episode steps: 20, steps per second: 196, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.074 [-0.934, 0.575], loss: 0.802253, mean_absolute_error: 4.120795, mean_q: 7.842575\n",
      " 1171/5000: episode: 62, duration: 0.090s, episode steps: 18, steps per second: 200, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.077 [-0.826, 0.436], loss: 1.168846, mean_absolute_error: 4.225451, mean_q: 7.911173\n",
      " 1188/5000: episode: 63, duration: 0.072s, episode steps: 17, steps per second: 237, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: -0.065 [-1.154, 0.808], loss: 0.938564, mean_absolute_error: 4.240248, mean_q: 8.052602\n",
      " 1204/5000: episode: 64, duration: 0.108s, episode steps: 16, steps per second: 148, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.069 [-0.911, 0.635], loss: 1.082224, mean_absolute_error: 4.303813, mean_q: 8.085016\n",
      " 1220/5000: episode: 65, duration: 0.064s, episode steps: 16, steps per second: 252, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.089 [-0.988, 0.616], loss: 1.321275, mean_absolute_error: 4.383622, mean_q: 8.170725\n",
      " 1235/5000: episode: 66, duration: 0.062s, episode steps: 15, steps per second: 241, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: -0.113 [-1.206, 0.743], loss: 1.336202, mean_absolute_error: 4.392755, mean_q: 8.196784\n",
      " 1250/5000: episode: 67, duration: 0.076s, episode steps: 15, steps per second: 197, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: -0.091 [-1.198, 0.817], loss: 1.167477, mean_absolute_error: 4.399294, mean_q: 8.238896\n",
      " 1267/5000: episode: 68, duration: 0.097s, episode steps: 17, steps per second: 175, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: -0.100 [-1.203, 0.740], loss: 1.791856, mean_absolute_error: 4.507966, mean_q: 8.204800\n",
      " 1285/5000: episode: 69, duration: 0.102s, episode steps: 18, steps per second: 176, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.100 [-0.943, 0.543], loss: 1.363868, mean_absolute_error: 4.492562, mean_q: 8.286791\n",
      " 1309/5000: episode: 70, duration: 0.123s, episode steps: 24, steps per second: 195, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.104 [-0.758, 0.352], loss: 1.334266, mean_absolute_error: 4.547036, mean_q: 8.459957\n",
      " 1330/5000: episode: 71, duration: 0.083s, episode steps: 21, steps per second: 253, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: -0.114 [-1.469, 0.925], loss: 1.416009, mean_absolute_error: 4.605944, mean_q: 8.600838\n",
      " 1348/5000: episode: 72, duration: 0.141s, episode steps: 18, steps per second: 128, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.083 [-1.120, 0.597], loss: 1.233275, mean_absolute_error: 4.613269, mean_q: 8.574828\n",
      " 1368/5000: episode: 73, duration: 0.084s, episode steps: 20, steps per second: 239, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.108 [-0.904, 0.569], loss: 1.311253, mean_absolute_error: 4.662139, mean_q: 8.751076\n",
      " 1394/5000: episode: 74, duration: 0.105s, episode steps: 26, steps per second: 249, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.079 [-0.933, 0.550], loss: 1.220520, mean_absolute_error: 4.714150, mean_q: 8.845016\n",
      " 1421/5000: episode: 75, duration: 0.182s, episode steps: 27, steps per second: 148, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.095 [-0.904, 0.605], loss: 1.394108, mean_absolute_error: 4.790746, mean_q: 8.992318\n",
      " 1442/5000: episode: 76, duration: 0.101s, episode steps: 21, steps per second: 208, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.088 [-0.857, 0.547], loss: 1.277969, mean_absolute_error: 4.827492, mean_q: 8.991108\n",
      " 1458/5000: episode: 77, duration: 0.063s, episode steps: 16, steps per second: 252, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.106 [-0.900, 0.606], loss: 1.583780, mean_absolute_error: 4.895239, mean_q: 9.123806\n",
      " 1485/5000: episode: 78, duration: 0.135s, episode steps: 27, steps per second: 201, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.116 [-0.710, 0.380], loss: 1.485108, mean_absolute_error: 4.910336, mean_q: 9.088211\n",
      " 1578/5000: episode: 79, duration: 0.447s, episode steps: 93, steps per second: 208, episode reward: 93.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.148 [-1.146, 1.872], loss: 1.428402, mean_absolute_error: 5.052483, mean_q: 9.473330\n",
      " 1624/5000: episode: 80, duration: 0.231s, episode steps: 46, steps per second: 199, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.457 [0.000, 1.000], mean observation: -0.087 [-0.733, 0.437], loss: 1.162490, mean_absolute_error: 5.180298, mean_q: 9.805573\n",
      " 1691/5000: episode: 81, duration: 0.251s, episode steps: 67, steps per second: 267, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.507 [0.000, 1.000], mean observation: 0.051 [-0.337, 0.942], loss: 1.360347, mean_absolute_error: 5.332275, mean_q: 10.073202\n",
      " 1726/5000: episode: 82, duration: 0.133s, episode steps: 35, steps per second: 263, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.090 [-0.172, 0.822], loss: 1.200312, mean_absolute_error: 5.450364, mean_q: 10.394769\n",
      " 1748/5000: episode: 83, duration: 0.093s, episode steps: 22, steps per second: 238, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.095 [-0.417, 0.936], loss: 1.088917, mean_absolute_error: 5.533441, mean_q: 10.702335\n",
      " 1782/5000: episode: 84, duration: 0.276s, episode steps: 34, steps per second: 123, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.035 [-0.423, 0.904], loss: 1.644909, mean_absolute_error: 5.641223, mean_q: 10.687012\n",
      " 1806/5000: episode: 85, duration: 0.102s, episode steps: 24, steps per second: 235, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: 0.081 [-0.576, 1.313], loss: 1.434126, mean_absolute_error: 5.710291, mean_q: 10.812305\n",
      " 1823/5000: episode: 86, duration: 0.107s, episode steps: 17, steps per second: 158, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.353 [0.000, 1.000], mean observation: 0.074 [-1.023, 1.777], loss: 1.842764, mean_absolute_error: 5.867900, mean_q: 11.113657\n",
      " 1834/5000: episode: 87, duration: 0.054s, episode steps: 11, steps per second: 202, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.135 [-1.346, 2.321], loss: 1.853364, mean_absolute_error: 5.917300, mean_q: 11.181871\n",
      " 1843/5000: episode: 88, duration: 0.053s, episode steps: 9, steps per second: 168, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.146 [-1.765, 2.826], loss: 2.250663, mean_absolute_error: 5.902842, mean_q: 11.116119\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1852/5000: episode: 89, duration: 0.194s, episode steps: 9, steps per second: 46, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.139 [-1.760, 2.821], loss: 1.418601, mean_absolute_error: 5.901606, mean_q: 11.350758\n",
      " 1862/5000: episode: 90, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.153 [-1.975, 3.115], loss: 1.906922, mean_absolute_error: 5.952682, mean_q: 11.442248\n",
      " 1873/5000: episode: 91, duration: 0.183s, episode steps: 11, steps per second: 60, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.118 [-1.792, 2.800], loss: 2.172411, mean_absolute_error: 6.043588, mean_q: 11.571087\n",
      " 1884/5000: episode: 92, duration: 0.182s, episode steps: 11, steps per second: 60, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.141 [-1.752, 2.827], loss: 1.397928, mean_absolute_error: 6.034951, mean_q: 11.562194\n",
      " 1894/5000: episode: 93, duration: 0.200s, episode steps: 10, steps per second: 50, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.132 [-1.950, 3.063], loss: 2.865048, mean_absolute_error: 6.174376, mean_q: 11.432566\n",
      " 1904/5000: episode: 94, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.118 [-1.987, 2.992], loss: 2.580298, mean_absolute_error: 6.216292, mean_q: 11.496932\n",
      " 1917/5000: episode: 95, duration: 0.217s, episode steps: 13, steps per second: 60, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.154 [0.000, 1.000], mean observation: 0.104 [-1.789, 2.801], loss: 1.888195, mean_absolute_error: 6.086380, mean_q: 11.484921\n",
      " 1927/5000: episode: 96, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.118 [-2.002, 3.022], loss: 2.024437, mean_absolute_error: 6.126765, mean_q: 11.685816\n",
      " 1940/5000: episode: 97, duration: 0.217s, episode steps: 13, steps per second: 60, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.112 [-0.804, 1.489], loss: 3.059312, mean_absolute_error: 6.240342, mean_q: 11.614697\n",
      " 1965/5000: episode: 98, duration: 0.416s, episode steps: 25, steps per second: 60, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.440 [0.000, 1.000], mean observation: 0.108 [-0.578, 1.456], loss: 1.533302, mean_absolute_error: 6.195661, mean_q: 11.660492\n",
      " 1978/5000: episode: 99, duration: 0.218s, episode steps: 13, steps per second: 60, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.106 [-0.619, 1.302], loss: 2.367191, mean_absolute_error: 6.262838, mean_q: 11.894065\n",
      " 2078/5000: episode: 100, duration: 1.665s, episode steps: 100, steps per second: 60, episode reward: 100.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.106 [-0.726, 0.350], loss: 2.838108, mean_absolute_error: 6.469171, mean_q: 12.075288\n",
      " 2112/5000: episode: 101, duration: 0.571s, episode steps: 34, steps per second: 60, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.110 [-0.740, 0.396], loss: 2.293654, mean_absolute_error: 6.508656, mean_q: 12.154798\n",
      " 2163/5000: episode: 102, duration: 0.843s, episode steps: 51, steps per second: 60, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.102 [-0.682, 0.217], loss: 2.036196, mean_absolute_error: 6.554492, mean_q: 12.450364\n",
      " 2241/5000: episode: 103, duration: 1.300s, episode steps: 78, steps per second: 60, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.041 [-1.490, 0.426], loss: 2.479683, mean_absolute_error: 6.707004, mean_q: 12.656378\n",
      " 2278/5000: episode: 104, duration: 0.614s, episode steps: 37, steps per second: 60, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: 0.084 [-0.378, 1.067], loss: 2.221331, mean_absolute_error: 6.778529, mean_q: 12.776488\n",
      " 2343/5000: episode: 105, duration: 1.085s, episode steps: 65, steps per second: 60, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: -0.065 [-0.603, 0.379], loss: 2.060640, mean_absolute_error: 6.860448, mean_q: 13.000033\n",
      " 2410/5000: episode: 106, duration: 1.116s, episode steps: 67, steps per second: 60, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.048 [-0.737, 0.398], loss: 2.620044, mean_absolute_error: 6.988627, mean_q: 13.168001\n",
      " 2428/5000: episode: 107, duration: 0.298s, episode steps: 18, steps per second: 60, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.106 [-0.855, 0.367], loss: 1.730077, mean_absolute_error: 6.958575, mean_q: 13.240407\n",
      " 2448/5000: episode: 108, duration: 0.331s, episode steps: 20, steps per second: 60, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.107 [-0.988, 0.380], loss: 2.001129, mean_absolute_error: 6.992719, mean_q: 13.268957\n",
      " 2485/5000: episode: 109, duration: 0.617s, episode steps: 37, steps per second: 60, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.459 [0.000, 1.000], mean observation: -0.087 [-0.708, 0.415], loss: 2.730831, mean_absolute_error: 7.029978, mean_q: 13.232473\n",
      " 2567/5000: episode: 110, duration: 1.364s, episode steps: 82, steps per second: 60, episode reward: 82.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: -0.080 [-0.788, 0.358], loss: 2.525933, mean_absolute_error: 7.105692, mean_q: 13.391590\n",
      " 2622/5000: episode: 111, duration: 0.918s, episode steps: 55, steps per second: 60, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.527 [0.000, 1.000], mean observation: 0.092 [-0.288, 0.808], loss: 1.796509, mean_absolute_error: 7.196578, mean_q: 13.850410\n",
      " 2665/5000: episode: 112, duration: 0.715s, episode steps: 43, steps per second: 60, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.059 [-0.379, 1.039], loss: 2.635510, mean_absolute_error: 7.361931, mean_q: 14.143975\n",
      " 2716/5000: episode: 113, duration: 0.847s, episode steps: 51, steps per second: 60, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.100 [-0.670, 0.278], loss: 2.500623, mean_absolute_error: 7.454952, mean_q: 14.248322\n",
      " 2765/5000: episode: 114, duration: 0.818s, episode steps: 49, steps per second: 60, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.076 [-0.839, 0.380], loss: 2.137291, mean_absolute_error: 7.531181, mean_q: 14.493052\n",
      " 2841/5000: episode: 115, duration: 1.266s, episode steps: 76, steps per second: 60, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.096 [-0.703, 0.422], loss: 2.867832, mean_absolute_error: 7.611362, mean_q: 14.474586\n",
      " 2929/5000: episode: 116, duration: 1.533s, episode steps: 88, steps per second: 57, episode reward: 88.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: -0.074 [-0.755, 0.358], loss: 2.617866, mean_absolute_error: 7.694311, mean_q: 14.771967\n",
      " 2979/5000: episode: 117, duration: 0.885s, episode steps: 50, steps per second: 57, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.057 [-0.350, 0.761], loss: 2.388910, mean_absolute_error: 7.831057, mean_q: 15.050550\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3028/5000: episode: 118, duration: 0.883s, episode steps: 49, steps per second: 56, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: -0.014 [-1.186, 0.578], loss: 1.999811, mean_absolute_error: 7.867227, mean_q: 15.118134\n",
      " 3053/5000: episode: 119, duration: 0.430s, episode steps: 25, steps per second: 58, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.106 [-0.729, 0.375], loss: 2.579384, mean_absolute_error: 8.050916, mean_q: 15.439906\n",
      " 3097/5000: episode: 120, duration: 0.752s, episode steps: 44, steps per second: 59, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.067 [-0.507, 1.037], loss: 2.962845, mean_absolute_error: 8.042192, mean_q: 15.359389\n",
      " 3140/5000: episode: 121, duration: 0.729s, episode steps: 43, steps per second: 59, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: 0.043 [-0.415, 1.057], loss: 2.979563, mean_absolute_error: 8.099468, mean_q: 15.460127\n",
      " 3176/5000: episode: 122, duration: 0.701s, episode steps: 36, steps per second: 51, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.094 [-0.257, 0.987], loss: 2.461145, mean_absolute_error: 8.175597, mean_q: 15.690084\n",
      " 3212/5000: episode: 123, duration: 0.798s, episode steps: 36, steps per second: 45, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.048 [-0.838, 0.423], loss: 1.899418, mean_absolute_error: 8.193214, mean_q: 15.857573\n",
      " 3238/5000: episode: 124, duration: 0.534s, episode steps: 26, steps per second: 49, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.097 [-1.148, 0.760], loss: 2.453042, mean_absolute_error: 8.259048, mean_q: 15.916697\n",
      " 3343/5000: episode: 125, duration: 2.100s, episode steps: 105, steps per second: 50, episode reward: 105.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.020 [-0.738, 0.336], loss: 2.831714, mean_absolute_error: 8.340479, mean_q: 15.990280\n",
      " 3371/5000: episode: 126, duration: 0.582s, episode steps: 28, steps per second: 48, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: -0.098 [-0.832, 0.382], loss: 1.420416, mean_absolute_error: 8.481577, mean_q: 16.535362\n",
      " 3430/5000: episode: 127, duration: 1.412s, episode steps: 59, steps per second: 42, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.065 [-0.618, 0.248], loss: 3.127564, mean_absolute_error: 8.514071, mean_q: 16.384855\n",
      " 3480/5000: episode: 128, duration: 1.109s, episode steps: 50, steps per second: 45, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.080 [-0.714, 0.282], loss: 2.495029, mean_absolute_error: 8.530756, mean_q: 16.419104\n",
      " 3519/5000: episode: 129, duration: 0.925s, episode steps: 39, steps per second: 42, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.125 [-0.761, 0.293], loss: 1.571579, mean_absolute_error: 8.536423, mean_q: 16.623926\n",
      " 3570/5000: episode: 130, duration: 1.402s, episode steps: 51, steps per second: 36, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.120 [-0.692, 0.157], loss: 2.867447, mean_absolute_error: 8.657988, mean_q: 16.619608\n",
      " 3637/5000: episode: 131, duration: 1.380s, episode steps: 67, steps per second: 49, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.143 [-0.808, 0.459], loss: 2.733170, mean_absolute_error: 8.746241, mean_q: 16.901196\n",
      " 3696/5000: episode: 132, duration: 1.392s, episode steps: 59, steps per second: 42, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.149 [-0.760, 0.479], loss: 2.453895, mean_absolute_error: 8.795341, mean_q: 17.060162\n",
      " 3723/5000: episode: 133, duration: 0.509s, episode steps: 27, steps per second: 53, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: 0.086 [-0.374, 1.169], loss: 2.415397, mean_absolute_error: 8.955738, mean_q: 17.533596\n",
      " 3757/5000: episode: 134, duration: 0.604s, episode steps: 34, steps per second: 56, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.087 [-0.540, 0.946], loss: 3.018495, mean_absolute_error: 9.034447, mean_q: 17.602488\n",
      " 3813/5000: episode: 135, duration: 1.226s, episode steps: 56, steps per second: 46, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.482 [0.000, 1.000], mean observation: -0.116 [-0.758, 0.440], loss: 2.186913, mean_absolute_error: 8.972811, mean_q: 17.458776\n",
      " 3856/5000: episode: 136, duration: 0.940s, episode steps: 43, steps per second: 46, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.465 [0.000, 1.000], mean observation: -0.119 [-0.713, 0.427], loss: 3.523243, mean_absolute_error: 9.180665, mean_q: 17.766283\n",
      " 3925/5000: episode: 137, duration: 1.637s, episode steps: 69, steps per second: 42, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.070 [-0.650, 0.205], loss: 2.387834, mean_absolute_error: 9.185130, mean_q: 17.886808\n",
      " 3964/5000: episode: 138, duration: 0.821s, episode steps: 39, steps per second: 47, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.141 [-0.663, 0.365], loss: 3.123145, mean_absolute_error: 9.339676, mean_q: 18.102003\n",
      " 4025/5000: episode: 139, duration: 1.374s, episode steps: 61, steps per second: 44, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.111 [-0.319, 0.800], loss: 3.098989, mean_absolute_error: 9.348557, mean_q: 18.170902\n",
      " 4095/5000: episode: 140, duration: 1.488s, episode steps: 70, steps per second: 47, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.142 [-0.834, 0.258], loss: 2.663538, mean_absolute_error: 9.550812, mean_q: 18.650894\n",
      " 4158/5000: episode: 141, duration: 1.232s, episode steps: 63, steps per second: 51, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: 0.111 [-0.242, 0.999], loss: 3.076409, mean_absolute_error: 9.659284, mean_q: 18.892221\n",
      " 4203/5000: episode: 142, duration: 0.970s, episode steps: 45, steps per second: 46, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.149 [-0.169, 0.697], loss: 3.362685, mean_absolute_error: 9.753885, mean_q: 18.966291\n",
      " 4267/5000: episode: 143, duration: 1.105s, episode steps: 64, steps per second: 58, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.547 [0.000, 1.000], mean observation: 0.161 [-0.297, 1.090], loss: 2.875922, mean_absolute_error: 9.905671, mean_q: 19.318459\n",
      " 4329/5000: episode: 144, duration: 1.174s, episode steps: 62, steps per second: 53, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.129 [-0.729, 0.269], loss: 2.916699, mean_absolute_error: 9.978608, mean_q: 19.533541\n",
      " 4417/5000: episode: 145, duration: 1.472s, episode steps: 88, steps per second: 60, episode reward: 88.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.523 [0.000, 1.000], mean observation: 0.088 [-0.294, 0.882], loss: 4.222862, mean_absolute_error: 10.096759, mean_q: 19.560009\n",
      " 4475/5000: episode: 146, duration: 1.131s, episode steps: 58, steps per second: 51, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.466 [0.000, 1.000], mean observation: -0.087 [-0.702, 0.260], loss: 4.097860, mean_absolute_error: 10.284737, mean_q: 19.934656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4557/5000: episode: 147, duration: 1.510s, episode steps: 82, steps per second: 54, episode reward: 82.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.106 [-0.770, 0.359], loss: 3.906746, mean_absolute_error: 10.318641, mean_q: 20.052610\n",
      " 4609/5000: episode: 148, duration: 0.890s, episode steps: 52, steps per second: 58, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.162 [-0.802, 0.366], loss: 4.436498, mean_absolute_error: 10.330420, mean_q: 19.978947\n",
      " 4685/5000: episode: 149, duration: 1.291s, episode steps: 76, steps per second: 59, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: 0.078 [-0.550, 0.891], loss: 3.650803, mean_absolute_error: 10.339061, mean_q: 20.154089\n",
      " 4716/5000: episode: 150, duration: 0.558s, episode steps: 31, steps per second: 56, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.548 [0.000, 1.000], mean observation: 0.101 [-0.559, 0.902], loss: 2.318303, mean_absolute_error: 10.360644, mean_q: 20.408672\n",
      " 4751/5000: episode: 151, duration: 0.779s, episode steps: 35, steps per second: 45, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.543 [0.000, 1.000], mean observation: 0.149 [-0.180, 0.673], loss: 3.928339, mean_absolute_error: 10.671265, mean_q: 20.799532\n",
      " 4791/5000: episode: 152, duration: 0.702s, episode steps: 40, steps per second: 57, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.132 [-0.236, 0.889], loss: 3.804376, mean_absolute_error: 10.729256, mean_q: 20.975086\n",
      " 4849/5000: episode: 153, duration: 1.085s, episode steps: 58, steps per second: 53, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.534 [0.000, 1.000], mean observation: 0.105 [-0.403, 0.699], loss: 3.873753, mean_absolute_error: 10.884906, mean_q: 21.141983\n",
      " 4919/5000: episode: 154, duration: 1.338s, episode steps: 70, steps per second: 52, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.152 [-0.446, 0.876], loss: 4.466312, mean_absolute_error: 10.872909, mean_q: 21.073275\n",
      " 4984/5000: episode: 155, duration: 1.060s, episode steps: 65, steps per second: 61, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: -0.108 [-0.756, 0.215], loss: 3.969351, mean_absolute_error: 10.930387, mean_q: 21.260321\n",
      "done, took 83.978 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xb3a6ec4e0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.fit(env, nb_steps=NB_STEPS, visualize=True, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 10 episodes ...\n",
      "Episode 1: reward: 70.000, steps: 70\n",
      "Episode 2: reward: 82.000, steps: 82\n",
      "Episode 3: reward: 197.000, steps: 197\n",
      "Episode 4: reward: 94.000, steps: 94\n",
      "Episode 5: reward: 200.000, steps: 200\n",
      "Episode 6: reward: 143.000, steps: 143\n",
      "Episode 7: reward: 58.000, steps: 58\n",
      "Episode 8: reward: 200.000, steps: 200\n",
      "Episode 9: reward: 77.000, steps: 77\n",
      "Episode 10: reward: 85.000, steps: 85\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xb4290deb8>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.test(env, nb_episodes=10, visualize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
