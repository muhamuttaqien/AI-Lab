{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import all libraries\n",
    "import keras\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "\n",
    "nlp=spacy.load(\"en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "train = pd.read_csv(\"./datasets/training.1600000.processed.noemoticon.csv\" , encoding= \"latin-1\")\n",
    "Y_train = train[train.columns[0]]\n",
    "X_train = train[train.columns[5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split the data into test and train\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "trainset1x, trainset2x, trainset1y, trainset2y = train_test_split(X_train.values, Y_train.values, test_size=0.02,random_state=42 )\n",
    "trainset2y=pd.get_dummies(trainset2y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>@natestamp Probably with your girlfriend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>It's my first day of school! Starting my cours...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>@moneyhighway @BudgetPulse Ty both  I apprecia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>inconsistent method signatures make me sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>@meg2e63 hows the beach retreat?  I wanted to ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text\n",
       "395          @natestamp Probably with your girlfriend \n",
       "396  It's my first day of school! Starting my cours...\n",
       "397  @moneyhighway @BudgetPulse Ty both  I apprecia...\n",
       "398        inconsistent method signatures make me sad \n",
       "399  @meg2e63 hows the beach retreat?  I wanted to ..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(data=trainset2x, columns=['text']).tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  function to remove stopwords\n",
    "def stopwords(sentence):\n",
    "    new=[]\n",
    "    sentence=nlp(sentence)\n",
    "    for w in sentence:\n",
    "        if (w.is_stop == False) & (w.pos_ !=\"PUNCT\"):\n",
    "            new.append(w.string.strip())\n",
    "        c=\" \".join(str(x) for x in new)\n",
    "    \n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to lemmatize the tweets\n",
    "def lemmatize(sentence):\n",
    "    sentence=nlp(sentence)\n",
    "    str=\"\"\n",
    "    for w in sentence:\n",
    "        str+=\" \"+w.lemma_\n",
    "        \n",
    "    return nlp(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# loading the glove model\n",
    "def loadGloveModel(gloveFile):\n",
    "    print(\"Loading Glove Model...\")\n",
    "    \n",
    "    f = open(gloveFile,'r')\n",
    "    model = {}\n",
    "    \n",
    "    for line in f:\n",
    "        splitLine = line.split()\n",
    "        word = splitLine[0]\n",
    "        embedding = [float(val) for val in splitLine[1:]]\n",
    "        model[word] = embedding\n",
    "    \n",
    "    print (\"Done.\"),len(model),(\" words loaded!\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Glove Model\n"
     ]
    }
   ],
   "source": [
    "# load the glove model\n",
    "glove_model = loadGloveModel(\"./datasets/glove/glove.twitter.27B.200d.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorizing the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# vectorising the sentences\n",
    "def sent_vectorizer(sent, model):\n",
    "    sent_vec = np.zeros(200)\n",
    "    numw = 0\n",
    "    for w in sent.split():\n",
    "        try:\n",
    "            sent_vec = np.add(sent_vec, model[str(w)])\n",
    "            numw+=1\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    return sent_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain a clean vector\n",
    "cleanvector = []\n",
    "for i in range(trainset2x.shape[0]):\n",
    "    document=trainset2x[i]\n",
    "    document=document.lower()\n",
    "    document=lemmatize(document)\n",
    "    document=str(document)\n",
    "    cleanvector.append(sent_vectorizer(document, glove_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# getting the input and output in proper shape\n",
    "cleanvector=np.array(cleanvector)\n",
    "cleanvector =cleanvector.reshape(len(cleanvector),200,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizing the sequences\n",
    "tokenizer = Tokenizer(num_words=16000)\n",
    "tokenizer.fit_on_texts(trainset2x)\n",
    "sequences = tokenizer.texts_to_sequences(trainset2x)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "data = pad_sequences(sequences, maxlen=15, padding=\"post\")\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data vector preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reshape the data and preparing to train\n",
    "data = data.reshape(len(cleanvector),15,1)\n",
    "from sklearn.model_selection import train_test_split\n",
    "trainx, validx, trainy, validy = train_test_split(data, trainset2y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate the number of words\n",
    "nb_words=len(tokenizer.word_index)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain theembedding matrix\n",
    "embedding_matrix = np.zeros((nb_words, 200))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = glove_model.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainy = np.array(trainy)\n",
    "validy = np.array(validy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trainx.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# building a simple RNN model\n",
    "def build_rnn_model():\n",
    "    model = Sequential()\n",
    "    model.add(keras.layers.InputLayer(input_shape=(15,1)))\n",
    "    keras.layers.embeddings.Embedding(nb_words, 15, weights=[embedding_matrix], input_length=15, trainable=False)\n",
    " \n",
    "    model.add(keras.layers.recurrent.SimpleRNN(units=100, activation='relu', use_bias=True))\n",
    "    model.add(keras.layers.Dense(units=1000, input_dim=2000, activation='sigmoid'))\n",
    "    model.add(keras.layers.Dense(units=500, input_dim=1000, activation='relu'))\n",
    "    model.add(keras.layers.Dense(units=2, input_dim=500, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compiling the model\n",
    "rnn_model = build_rnn_model()\n",
    "rnn_model.fit(trainx, trainy, epochs=10, batch_size=120, validation_data=(validx, validy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sentence = \"I love this impressive RNN model so much!\"\" # pass into the pipeline\n",
    "# prediction = rnn_model.predict(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
