{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage.transform\n",
    "from scipy.misc import imread, imresize\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "if is_cuda: device = torch.device('cuda')\n",
    "else: device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def caption_image(encoder, decoder, image_path, word_vocab_path, beam_size=3):\n",
    "    \n",
    "    # load word vocab in word2index and then convert into index2word\n",
    "    with open(word_vocab_path, 'r') as file: word_vocab = json.load(file)\n",
    "    index2word = { index:word for word, index in word_vocab.items() }\n",
    "\n",
    "    k = beam_size\n",
    "    vocab_size = len(word_vocab)\n",
    "    \n",
    "    # read and process image\n",
    "    image = imread(image_path)\n",
    "    if len(image.shape) == 2:\n",
    "        image = image[:, :, np.newaxis]\n",
    "        image = np.concatenate([image, image, image], axis=2)\n",
    "    \n",
    "    image = imresize(image, (256, 256))\n",
    "    image = image.transpose(2, 0, 1)\n",
    "    image = image/ 255.\n",
    "    image = torch.FloatTensor(image).to(device)\n",
    "    transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean = (0.485, 0.456, 0.406),\n",
    "                                                     std = (0.229, 0.224, 0.225))])\n",
    "    \n",
    "    image = transform(image) # (3, 256, 256)\n",
    "    image = image.unsqueeze(0) # (1, 3, 256, 256)\n",
    "    \n",
    "    # encoding process\n",
    "    features = encoder(image)\n",
    "    encoder_image_size = features.size(1)\n",
    "    encoder_dim = features.size(3)\n",
    "\n",
    "    # flatten features/ encoding outputs\n",
    "    encoder_outputs = features.view(1, -1, encoder_dim) # (1, num_pixels, encoder_dim)\n",
    "    num_pixels = encoder_outputs.size(1)\n",
    "    \n",
    "    # treat the problem as having a batch size of k\n",
    "    encoder_outputs = encoder_outputs.expand(k, num_pixels, encoder_dim) # (k, num_pixels, encoder_dim)\n",
    "\n",
    "    # tensor to store top k previous words at each step (starting from just <start> token)\n",
    "    k_prev_words = torch.LongTensor([[word_vocab['<start>']]] * k).to(device) # (k, 1)\n",
    "\n",
    "    # tensor to store top k sequences (starting from just <start> token)\n",
    "    sequences = k_prev_words # (k, 1)\n",
    "\n",
    "    # tensor to store top k sequences' scores (starting from 0 values)\n",
    "    top_k_scores = torch.zeros(k, 1).to(device) # (k, 1)\n",
    "\n",
    "    # tensor to store top k sequences' alphas (starting from 1 values)\n",
    "    sequences_alpha = torch.ones(k, 1, encoder_image_size, encoder_image_size).to(device) # (k, 1, encoder_image_size, encoder_image_size)\n",
    "    \n",
    "    # lists to store completed sequences and scores\n",
    "    complete_sequences = list()\n",
    "    complete_sequences_scores = list()\n",
    "    complete_sequences_alpha = list()\n",
    "\n",
    "    # decoding process\n",
    "    step = 1\n",
    "    hidden, cell = decoder.init_state(encoder_outputs)\n",
    "    \n",
    "    # s is a number less than or equal to k, since sequences are removed from this process once they hit <end> token\n",
    "    while True:\n",
    "        \n",
    "        embeddings = decoder.embedding_layer(k_prev_words).squeeze(1) # (s, embedding_dim)\n",
    "        attention_weighted_encoding, alpha = decoder.attention_layer(encoder_outputs, hidden) # (s, encoder_dim), (s, num_pixels)\n",
    "\n",
    "        alpha = alpha.view(-1, encoder_image_size, encoder_image_size) # (s, encoder_image_size, encoder_image_size)\n",
    "        \n",
    "        gate = decoder.sigmoid(decoder.f_beta(hidden)) # gating scalar, (s, encoder_dim)\n",
    "        attention_weighted_encoding = gate * attention_weighted_encoding\n",
    "\n",
    "        hidden, cell = decoder.decode_step(torch.cat([embeddings, attention_weighted_encoding], dim=1), (hidden, cell)) # (s, decoder_dim)\n",
    "\n",
    "        scores = decoder.fc_layer(hidden) # (s, vocab_size)\n",
    "        scores = F.log_softmax(scores, dim=1)\n",
    "        \n",
    "        # add scores\n",
    "        scores = top_k_scores.expand_as(scores) + scores # (s, vocab_size)\n",
    "\n",
    "        # for the first step, all k points will have the same scores (since same k previous words, hidden, cell)\n",
    "        if step == 1:\n",
    "            top_k_scores, top_k_words = scores[0].topk(k, 0, True, True) # (s)\n",
    "        else:\n",
    "            # unroll and find top scores, and their unrolled indices\n",
    "            top_k_scores, top_k_words = scores.view(-1).topk(k, 0, True, True) # (s)\n",
    "\n",
    "        # convert unrolled indices to actual indices of scores\n",
    "        prev_word_indices = top_k_words / vocab_size # (s)\n",
    "        next_word_indices = top_k_words % vocab_size # (s)\n",
    "        \n",
    "        # add new words to sequences\n",
    "        sequences = torch.cat([sequences[prev_word_indices], next_word_indices.unsqueeze(1)], dim=1) # (s, step+1)\n",
    "        sequences_alpha = torch.cat([sequences_alpha[prev_word_indices], alpha[prev_word_indices].unsqueeze(1)], \n",
    "                                     dim=1) # (s, step+1, encoder_image_size, encoder_image_size)\n",
    "        \n",
    "        incomplete_indices = [indices for indices, next_word in enumerate(next_word_indices) if\n",
    "                              next_word != word_vocab['<end>']]\n",
    "        complete_indices = list(set(range(len(next_word_indices))) - set(incomplete_indices))\n",
    "\n",
    "        # set aside complete sequences\n",
    "        if len(complete_indices) > 0:\n",
    "            complete_sequences.extend(sequences[complete_indices].tolist())\n",
    "            complete_sequences_alpha.extend(sequences_alpha[complete_indices].tolist())\n",
    "            complete_sequences_scores.extend(top_k_scores[complete_indices])\n",
    "        k -= len(complete_indices) # reduce beam length accordingly\n",
    "\n",
    "        # process with incomplete sequences\n",
    "        if k==0: break\n",
    "\n",
    "        sequences = sequences[incomplete_indices]\n",
    "        sequences_alpha = sequences_alpha[incomplete_indices]\n",
    "        hidden = hidden[prev_word_indices[incomplete_indices]]\n",
    "        cell = cell[prev_word_indices[incomplete_indices]]\n",
    "        encoder_outputs = encoder_outputs[prev_word_indices[incomplete_indices]]\n",
    "        top_k_scores = top_k_scores[incomplete_indices].unsqueeze(1)\n",
    "        k_prev_words = next_word_indices[incomplete_indices].unsqueeze(1)\n",
    "\n",
    "        # break if thins have been going on too long\n",
    "        if step > 50: break\n",
    "        step += 1\n",
    "        \n",
    "    i = complete_sequences_scores.index(max(complete_sequences_scores))\n",
    "    sequence = complete_sequences[i]\n",
    "    alphas = complete_sequences_alpha[i]\n",
    "     \n",
    "    return sequence, alphas, index2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_image(image_path, sequence, alphas, index2word, smooth=True):\n",
    "    \n",
    "    image = Image.open(image_path)\n",
    "    image = image.resize([14 * 24, 14 * 24], Image.LANCZOS)\n",
    "\n",
    "    words = [index2word[index] for index in sequence]\n",
    "    \n",
    "    for t in range(len(words)):\n",
    "        \n",
    "        if t > 50: break\n",
    "            \n",
    "        plt.subplot(np.ceil(len(words)/ 5.), 5, t + 1)\n",
    "        plt.text(0, 1, '%s' % (words[t]), color='black', backgroundcolor='white', fontsize=12)\n",
    "        plt.imshow(image)\n",
    "        \n",
    "        alphas = torch.FloatTensor(alphas)\n",
    "        current_alpha = alphas[t, :]\n",
    "        if smooth:\n",
    "            alpha = skimage.transform.pyramid_expand(current_alpha.numpy(), upscale=24, sigma=8)\n",
    "        else:\n",
    "            alpha = skimage.transform.resize(current_alpha.numpy(), [14 * 24, 14 * 24])\n",
    "            \n",
    "        if t == 0:\n",
    "            plt.imshow(alpha, alpha=0)\n",
    "        else:\n",
    "            plt.imshow(alpha, alpha=0.8)\n",
    "            \n",
    "        plt.set_cmap(cm.Greys_r)\n",
    "        plt.axis('off')\n",
    "        \n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_path, device):\n",
    "\n",
    "    checkpoint = torch.load(model_path)\n",
    "    \n",
    "    # init the networks\n",
    "    encoder = checkpoint['encoder']\n",
    "    encoder = encoder.to(device)\n",
    "    \n",
    "    decoder = checkpoint['decoder']\n",
    "    decoder = decoder.to(device)\n",
    "    \n",
    "    # set the networks into eval mode\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "    return encoder, decocer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caption The Image!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = './weights/'\n",
    "IMAGE_PATH = './images/'\n",
    "WORD_VOCAB_PATH = './datasets/data/'\n",
    "BEAM_SIZE = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder, decoder = load_image(MODEL_PATH, device)\n",
    "sequence, alphas, index2word = caption_image(encoder, decoder, IMAGE_PATH, WORD_VOCAB_PATH, beam_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_image(WORD_VOCAB_PATH, sequence, alphas, index2word, smooth=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
