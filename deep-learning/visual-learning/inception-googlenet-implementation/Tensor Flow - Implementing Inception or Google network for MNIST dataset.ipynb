{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_set = pd.read_csv('../datasets/mnist_train.csv', header=None)\n",
    "test_set = pd.read_csv('../datasets/mnist_test.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>775</th>\n",
       "      <th>776</th>\n",
       "      <th>777</th>\n",
       "      <th>778</th>\n",
       "      <th>779</th>\n",
       "      <th>780</th>\n",
       "      <th>781</th>\n",
       "      <th>782</th>\n",
       "      <th>783</th>\n",
       "      <th>784</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0    1    2    3    4    5    6    7    8    9   ...   775  776  777  778  \\\n",
       "0    5    0    0    0    0    0    0    0    0    0 ...     0    0    0    0   \n",
       "1    0    0    0    0    0    0    0    0    0    0 ...     0    0    0    0   \n",
       "2    4    0    0    0    0    0    0    0    0    0 ...     0    0    0    0   \n",
       "3    1    0    0    0    0    0    0    0    0    0 ...     0    0    0    0   \n",
       "4    9    0    0    0    0    0    0    0    0    0 ...     0    0    0    0   \n",
       "\n",
       "   779  780  781  782  783  784  \n",
       "0    0    0    0    0    0    0  \n",
       "1    0    0    0    0    0    0  \n",
       "2    0    0    0    0    0    0  \n",
       "3    0    0    0    0    0    0  \n",
       "4    0    0    0    0    0    0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get labels in own array\n",
    "train_labels = np.array(train_set[0])\n",
    "test_labels = np.array(test_set[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5],\n",
       "       [0],\n",
       "       [4],\n",
       "       ...,\n",
       "       [5],\n",
       "       [6],\n",
       "       [8]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one hot encode the labels\n",
    "train_labels = (np.arange(10) == train_labels[:, None]).astype(np.float32)\n",
    "test_labels = (np.arange(10) == test_labels[:, None]).astype(np.float32)\n",
    "type(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop the labels column from training dataframe\n",
    "train_data = train_set.drop(0, axis=1)\n",
    "test_data = test_set.drop(0, axis=1)\n",
    "type(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# put in correct float32 array format\n",
    "train_data = np.array(train_data).astype(np.float32)\n",
    "test_data = np.array(test_data).astype(np.float32)\n",
    "type(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 28, 28, 1), (60000, 10))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reformat the data so it's not flat (4D tensor)\n",
    "train_data = train_data.reshape(len(train_data), 28, 28, 1)\n",
    "test_data = test_data.reshape(len(test_data), 28, 28, 1)\n",
    "train_data.shape, train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get a validation set and remove it from the train set so we can monitor how training is going\n",
    "train_data, val_data, train_labels, val_labels = train_data[0: (len(train_data)-500), :, :, :], train_data[(len(train_data)-500):len(train_data), :, :, :], \\\n",
    "                                                 train_labels[0: (len(train_labels)-500), :], train_labels[(len(train_labels)-500):len(train_labels), :]                                            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Batch Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# returns accuracy of model\n",
    "def accuracy(target, predictions):\n",
    "    return(100.0 * np.sum(np.argmax(target, 1) == np.argmax(predictions, 1))/ target.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# need to batch the test data because running low on memory\n",
    "class test_batchs:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.batch_index = 0\n",
    "        \n",
    "    def next_batch(self, batch_size):\n",
    "        if (batch_size + self.batch_index) > self.data.shape[0]:\n",
    "            print(\"Batch sized is messed up.\")\n",
    "        batch = self.data[self.batch_index:(self.batch_index + batch_size), :, :, :]\n",
    "        \n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set the test batch size\n",
    "test_batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# training batch size\n",
    "train_batch_size = 50\n",
    "\n",
    "# number of feature maps output by each tower inside the first and second Inception module\n",
    "map1 = 32\n",
    "map2 = 64\n",
    "\n",
    "# number of hidden nodes\n",
    "num_fc1 = 700 #1028\n",
    "num_fc2 = 10\n",
    "\n",
    "# number of feature maps output by each 1×1 convolution that precedes a large convolution\n",
    "reduce1x1 = 16\n",
    "\n",
    "# dropout rate for nodes in the hidden layer during training\n",
    "dropout = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/muhamuttaqien/Desktop/Artificial Intelligence/AI-playground/deep-learning/visual-learning/inception-googlenet-implementation/models/mnist_inception_model.ckpt'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#use os to get our current working directory so we can save variable there\n",
    "file_path = os.getcwd() + '/models/mnist_inception_model.ckpt'\n",
    "file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time for the bulk of the work, which will require Tensorflow.\n",
    "\n",
    "1. Once the graph is defined, create placeholders that hold the training data, training labels, validation data, and validation labels\n",
    "2. Then create some helper functions which assist in defining tensors, 2D convolutions, and max pooling\n",
    "3. Next, use the helper functions and hyperparameters to create variables in both Inception modules\n",
    "4. Then, create another function that  takes data as input and passes it through the Inception modules and fully connected layers and outputs the logits\n",
    "5. Finally, define the loss to be cross-entropy, use Adam to optimize, and create ops for converting data to predictions, initializing variables, and saving all variables in the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <img src=\"./naive-inception-module.png\" />\n",
    "    <center><caption>Figure 1. Naive Module</caption></center>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # train data and labels\n",
    "    tf_X = tf.placeholder(tf.float32, shape=(train_batch_size, 28, 28, 1))\n",
    "    tf_y = tf.placeholder(tf.float32, shape=(train_batch_size, 10))\n",
    "    \n",
    "    # validation data\n",
    "    tf_val_X = tf.placeholder(tf.float32, shape=(len(val_data), 28, 28, 1))\n",
    "    \n",
    "    # test data\n",
    "    tf_test_X = tf.placeholder(tf.float32, shape=(test_batch_size, 28, 28, 1))\n",
    "    \n",
    "    def create_weight(size, name):\n",
    "        return tf.Variable(tf.truncated_normal(size, stddev=0.1), name=name)\n",
    "    \n",
    "    def create_bias(size, name):\n",
    "        return tf.Variable(tf.constant(0.1, shape=size), name=name)\n",
    "    \n",
    "    def conv2d_s1(x, W):\n",
    "        return tf.nn.conv2d(x, W, strides=[1,1,1,1], padding='SAME')\n",
    "     \n",
    "    def max_pool_3x3_s1(x):\n",
    "        return tf.nn.max_pool(x, ksize=[1,3,3,1], strides=[1,1,1,1], padding='SAME')\n",
    "    \n",
    "    # Defining Inception Module-1\n",
    "    #\n",
    "    # follows input\n",
    "    W_conv1_1x1_1 = create_weight([1,1,1,map1], 'W_conv1_1x1_1')\n",
    "    b_conv1_1x1_1 = create_bias([map1], 'b_conv1_1x1_1')\n",
    "    \n",
    "    # follows input\n",
    "    W_conv1_1x1_2 = create_weight([1,1,1,reduce1x1], 'W_conv1_1x1_2')\n",
    "    b_conv1_1x1_2 = create_bias([reduce1x1], 'b_conv1_1x1_2')\n",
    "    \n",
    "    # follows input\n",
    "    W_conv1_1x1_3 = create_weight([1,1,1,reduce1x1], 'W_conv1_1x1_3')\n",
    "    b_conv1_1x1_3 = create_bias([reduce1x1], 'b_conv1_1x1_3')\n",
    "    \n",
    "    # follows 1x1_2\n",
    "    W_conv1_3x3 = create_weight([3, 3, reduce1x1, map1], 'W_conv1_3x3')\n",
    "    b_conv1_3x3 = create_bias([map1], 'b_conv1_3x3')\n",
    "    \n",
    "    # follows 1x1_3\n",
    "    W_conv1_5x5 = create_weight([5, 5, reduce1x1, map1], 'W_conv1_5x5')\n",
    "    b_conv1_5x5 = create_bias([map1], 'b_conv1_5x5')\n",
    "    \n",
    "    # follows max pooling\n",
    "    W_conv1_1x1_4 = create_weight([1,1,1,map1], 'W_conv1_1x1_4')\n",
    "    b_conv1_1x1_4 = create_bias([map1], 'b_conv1_1x1_4')\n",
    "    \n",
    "    # Defining Inception Module-2\n",
    "    #\n",
    "    # follows Inception1\n",
    "    W_conv2_1x1_1 = create_weight([1,1,4*map1,map2], 'W_conv2_1x1_1')\n",
    "    b_conv2_1x1_1 = create_bias([map2], 'b_conv2_1x1_1')\n",
    "\n",
    "    # follows Inception1\n",
    "    W_conv2_1x1_2 = create_weight([1,1,4*map1,reduce1x1], 'W_conv2_1x1_2')\n",
    "    b_conv2_1x1_2 = create_bias([reduce1x1], 'b_conv2_1x1_2')\n",
    "    \n",
    "    # follows Inception1\n",
    "    W_conv2_1x1_3 = create_weight([1,1,4*map1,reduce1x1], 'W_conv2_1x1_3')\n",
    "    b_conv2_1x1_3 = create_bias([reduce1x1], 'b_conv2_1x1_3')\n",
    "    \n",
    "    # follows 1x1_2\n",
    "    W_conv2_3x3 = create_weight([3,3,reduce1x1,map2], 'W_conv2_3x3')\n",
    "    b_conv2_3x3 = create_bias([map2], 'b_conv2_3x3')\n",
    "    \n",
    "    # follows 1x1_3\n",
    "    W_conv2_5x5 = create_weight([5,5,reduce1x1,map2], 'W_conv2_5x5')\n",
    "    b_conv2_5x5 = create_bias([map2], 'b_conv2_5x5')\n",
    "    \n",
    "    # follows max pooling\n",
    "    W_conv2_1x1_4 = create_weight([1,1,4*map1, map2], 'W_conv2_1x1_4')\n",
    "    b_conv2_1x1_4 = create_bias([map2], 'b_conv2_1x1_4')\n",
    "    \n",
    "    # Defining Fully-connected Layers\n",
    "    # since padding is same, the feature map with there will be 4 28*28*map2\n",
    "    W_fc1 = create_weight([28*28*(4*map2), num_fc1], 'W_fc1')\n",
    "    b_fc1 = create_bias([num_fc1], 'b_fc1')\n",
    "    \n",
    "    W_fc2 = create_weight([num_fc1, num_fc2], 'W_fc2')\n",
    "    b_fc2 = create_bias([num_fc2], 'b_fc2')\n",
    "    \n",
    "    def model(x, train=True):\n",
    "        # Inception Module 1\n",
    "        conv1_1x1_1 = conv2d_s1(x, W_conv1_1x1_1) + b_conv1_1x1_1\n",
    "        conv1_1x1_2 = tf.nn.relu(conv2d_s1(x, W_conv1_1x1_2) + b_conv1_1x1_2)\n",
    "        conv1_1x1_3 = tf.nn.relu(conv2d_s1(x, W_conv1_1x1_3) + b_conv1_1x1_3)\n",
    "        conv1_3x3 = conv2d_s1(conv1_1x1_2, W_conv1_3x3) + b_conv1_3x3\n",
    "        conv1_5x5 = conv2d_s1(conv1_1x1_3, W_conv1_5x5) + b_conv1_5x5\n",
    "        maxpool_1 = max_pool_3x3_s1(x)\n",
    "        conv1_1x1_4 = conv2d_s1(maxpool_1, W_conv1_1x1_4) + b_conv1_1x1_4\n",
    "        \n",
    "        # concatenate all the feature maps and hit them with a relu\n",
    "        inception_1 = tf.nn.relu(tf.concat([conv1_1x1_1, conv1_3x3, conv1_5x5, conv1_1x1_4], 3))\n",
    " \n",
    "        # Inception Module 2\n",
    "        conv2_1x1_1 = conv2d_s1(inception_1, W_conv2_1x1_1) + b_conv2_1x1_1\n",
    "        conv2_1x1_2 = tf.nn.relu(conv2d_s1(inception_1, W_conv2_1x1_2) + b_conv1_1x1_2)\n",
    "        conv2_1x1_3 = tf.nn.relu(conv2d_s1(inception_1, W_conv2_1x1_3) + b_conv2_1x1_3)\n",
    "        conv2_3x3 = conv2d_s1(conv2_1x1_2, W_conv2_3x3) + b_conv2_3x3\n",
    "        conv2_5x5 = conv2d_s1(conv2_1x1_3, W_conv2_5x5) + b_conv2_5x5\n",
    "        maxpool_2 = max_pool_3x3_s1(inception_1)\n",
    "        conv2_1x1_4 = conv2d_s1(maxpool_2, W_conv2_1x1_4) + b_conv2_1x1_4\n",
    "        \n",
    "        # concatenate all the feature maps and hit them with a relu\n",
    "        inception_2 = tf.nn.relu(tf.concat([conv2_1x1_1, conv2_3x3, conv2_5x5, conv2_1x1_4], 3))\n",
    "        \n",
    "        # flatten features for fully-connected layer\n",
    "        inception_2_flat = tf.reshape(inception_2, [-1, 28*28*4*map2])\n",
    "        \n",
    "        # Fully-connected layers\n",
    "        if train:\n",
    "            h_fc1 = tf.nn.dropout(tf.nn.relu(tf.matmul(inception_2_flat, W_fc1) + b_fc1), dropout)\n",
    "        else:\n",
    "            h_fc1 = tf.nn.relu(tf.matmul(inception_2_flat, W_fc1) + b_fc1)\n",
    "            \n",
    "        return tf.matmul(h_fc1, W_fc2) + b_fc2\n",
    "    \n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = model(tf_X), labels = tf_y))\n",
    "    opt = tf.train.AdamOptimizer(1e-4).minimize(loss)\n",
    "    \n",
    "    predictions_val = tf.nn.softmax(model(tf_val_X, train=False))\n",
    "    predictions_test = tf.nn.softmax(model(tf_test_X, train=False))\n",
    "      \n",
    "    # add an op to initialize the variables\n",
    "    init = tf.global_variables_initializer() # initialize_all_variables\n",
    "\n",
    "    # add ops to save and restore all the variables so we can pick up later\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    # later, launch the model, initialize the variables, do some work, and save the variables to disk\n",
    "    with tf.Session() as sess:\n",
    "      sess.run(init)\n",
    "      \n",
    "      # save the variables to disk\n",
    "      save_path = saver.save(sess, file_path)\n",
    "      print(\"Model saved in path: %s.\" % save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the model, set the number of training steps, create a session, initialize variables, and run the optimizer op for each batch of training data.  You’ll want to see how your model is progressing, so run the op for getting your validation predictions every 100 steps.  When training is done, output the test data accuracy and save the model.  I also created a flag use_previous that allows you to load a model from the file_path to continue training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <img src=\"./inception-module.png\" />\n",
    "    <center><caption>Figure 2. Inception Module</caption></center>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a session\n",
    "sess = tf.Session(graph=graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialize variables\n",
    "sess.run(init)\n",
    "print(\"Model initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set use_previous=1 to use file_path model\n",
    "# set use_previous=0 to start model from scratch\n",
    "use_previous = 1\n",
    "\n",
    "# use the previous model or don't and initialize variables\n",
    "if use_previous:\n",
    "    saver.restore(sess, file_path)\n",
    "    print(\"Model restored.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# training the model right now!\n",
    "num_steps = 20 # 20000\n",
    "\n",
    "for s in range(num_steps):\n",
    "    offset = (s*train_batch_size) % (len(train_data)-train_batch_size)\n",
    "    train_batch, test_batch = train_data[offset:(offset + train_batch_size), :], train_labels[offset:(offset + train_batch_size), :]\n",
    "    \n",
    "    feed_dict = {tf_X: train_batch, tf_y: test_batch}\n",
    "    _, loss_value = sess.run([opt, loss], feed_dict=feed_dict)\n",
    "    \n",
    "    if s%100 == 0:\n",
    "        feed_dict = {tf_val_X: val_data}\n",
    "        preds = sess.run(predictions_val, feed_dict=feed_dict)\n",
    "        \n",
    "        print(\"Step: \" +str(s))\n",
    "        print(\"Validation accuracy: \"+str(accuracy(val_labels, preds)))\n",
    "        print(\" \")\n",
    "        \n",
    "    # get test accuracy and save the model\n",
    "    if s == (num_steps-1):\n",
    "        # create an array to store the outputs for the test\n",
    "        result = np.array([]).reshape(0, 10)\n",
    "        \n",
    "        # use the batches class\n",
    "        batch_test_X = test_batchs(test_data)\n",
    "        \n",
    "        for i in range(int(len(test_data)/ test_batch_size)):\n",
    "            feed_dict = {tf_test_X: batch_test_X.next_batch(test_batch_size)}\n",
    "            preds = sess.run(predictions_test, feed_dict=feed_dict)\n",
    "            result = np.concatenate((result, preds), axis=0)\n",
    "\n",
    "        print(\"Test accuracy: \"+str(accuracy(test_labels, result)))\n",
    "        \n",
    "        save_path = saver.save(sess, file_path)\n",
    "        print(\"Model saved.\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
