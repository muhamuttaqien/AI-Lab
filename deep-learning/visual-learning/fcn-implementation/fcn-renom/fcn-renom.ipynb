{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No traceback available to show.\n"
     ]
    }
   ],
   "source": [
    "%tb\n",
    "\n",
    "import os\n",
    "import os.path as osp \n",
    "import re\n",
    "\n",
    "import renom as rm\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from renom.cuda import set_cuda_active\n",
    "from renom_img.api.utility.evaluate import Fast_Segmentation_Evaluator\n",
    "from renom_img.api.utility.misc.display import draw_segment\n",
    "from renom_img.api.utility.augmentation import Augmentation\n",
    "from renom_img.api.utility.augmentation.process import Flip, WhiteNoise, ContrastNorm\n",
    "from PIL import Image\n",
    "\n",
    "set_cuda_active(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_unique_label(lbl_list):\n",
    "    uniq_label = []\n",
    "    for i in tqdm.trange(len(lbl_list)):\n",
    "        lbl_file = lbl_list[i]\n",
    "        lbl = Image.open(lbl_file)\n",
    "        lbl = np.array(lbl, dtype=np.int32)\n",
    "        for l in set(lbl.flatten()):\n",
    "            if not l in uniq_label:\n",
    "                uniq_label.append(l)\n",
    "    return uniq_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_image_path_list = []\n",
    "train_annotation_path_list = []\n",
    "with open(\"./CamVid/train.txt\") as f:\n",
    "    txt = f.readlines()\n",
    "    txt = [line.replace(\"/SegNet/\",\"./\").split(\" \") for line in txt]\n",
    "    for i in range(len(txt)):\n",
    "        train_image_path_list.append(txt[i][0])\n",
    "        train_annotation_path_list.append(txt[i][1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valid_image_path_list = []\n",
    "valid_annotation_path_list = []\n",
    "with open(\"./CamVid/val.txt\") as f:\n",
    "    txt = f.readlines()\n",
    "    txt = [line.replace(\"/SegNet/\",\"./\").split(\" \") for line in txt]\n",
    "    for i in range(len(txt)):\n",
    "        valid_image_path_list.append(txt[i][0])\n",
    "        valid_annotation_path_list.append(txt[i][1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_image_path_list = []\n",
    "test_annotation_path_list = []\n",
    "with open(\"./CamVid/test.txt\") as f:\n",
    "    txt = f.readlines()\n",
    "    txt = [line.replace(\"/SegNet/\",\"./\").split(\" \") for line in txt]\n",
    "    for i in range(len(txt)):\n",
    "        test_image_path_list.append(txt[i][0])\n",
    "        test_annotation_path_list.append(txt[i][1].strip())       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 367/367 [00:08<00:00, 40.85it/s]\n",
      "100%|██████████| 101/101 [00:01<00:00, 52.13it/s]\n"
     ]
    }
   ],
   "source": [
    "uniq_label = get_unique_label(train_annotation_path_list)\n",
    "uniq_label_valid = get_unique_label(valid_annotation_path_list)\n",
    "\n",
    "for l in uniq_label_valid:\n",
    "    if not l in uniq_label:\n",
    "        uniq_label.append(l)\n",
    "uniq_label.sort()\n",
    "\n",
    "class_map = np.arange(len(uniq_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'ignore_bias'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-88d220d49a4e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mrenom_img\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msegmentation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfcn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFCN8s\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFCN8s\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/Artificial Intelligence/AI-Lab/deep-learning/visual-learning/fcn-implementation/fcn-renom/ReNomIMG/renom_img/api/segmentation/fcn.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, class_map, imsize, load_pretrained_weight, train_whole_network)\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ascii\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mclass_map\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCNN_FCN8s\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_whole_network\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_whole_network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecay_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Artificial Intelligence/AI-Lab/deep-learning/visual-learning/fcn-implementation/fcn-renom/ReNomIMG/renom_img/api/segmentation/fcn.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, num_class)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         self.upscore2 = rm.Deconv2d(\n\u001b[0;32m--> 298\u001b[0;31m             num_class, filter=4, stride=2, padding=0, ignore_bias=True)\n\u001b[0m\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m         self.upscore8 = rm.Deconv2d(\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'ignore_bias'"
     ]
    }
   ],
   "source": [
    "from renom_img.api.segmentation.fcn import FCN8s\n",
    "model_class = FCN8s(class_map=class_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lbl = model_class.get_label(valid_annotation_path_list, 0)\n",
    "# check_class = 0\n",
    "# lbl = (lbl==check_class) * 1.0\n",
    "# visualize_img = draw_segment(valid_image_path_list[0], lbl.astype(np.int))\n",
    "# plt.imshow(visualize_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lbl = model_class.get_label(valid_annotation_path_list, 1)\n",
    "# check_class = 2\n",
    "# lbl = (lbl==check_class) * 1.0\n",
    "# visualize_img = draw_segment(valid_image_path_list[1], lbl.astype(np.int))\n",
    "# plt.imshow(visualize_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class_map = [\"sky\", \"building\", \"pole\", \"road\", \"pavement\",\n",
    "                          \"tree\", \"sign_symbol\", \"fence\", \"car\", \"pedestrian\", \"bicyclist\", \"unlabeled\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "augmentation = Augmentation([\n",
    "    Flip(),\n",
    "    WhiteNoise(),\n",
    "    ContrastNorm([0.5, 1.0])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to specify the path to the VGG weight\n",
    "model_file = \"./weight/vgg16_from_caffe.h5\"\n",
    "\n",
    "# Model\n",
    "# model_class.load(model_file)\n",
    "# model_class.set_models(inference=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def callback_end_epoch(epoch, model, avg_train_loss_list, avg_valid_loss_list):\n",
    "    if epoch % 10 == 0:\n",
    "        # need to specify the path to the save directory which you want to save weights\n",
    "        model.save(\"./weight/model_{}_{}.h5\".format(epoch, str(avg_valid_loss_list[-1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model_class.fit(train_img_path_list=train_image_path_list,\n",
    "#                 train_annotation_list=train_annotation_path_list,\n",
    "#                 valid_img_path_list=valid_image_path_list,\n",
    "#                 valid_annotation_list=valid_annotation_path_list,\n",
    "#                 batch_size=8,\n",
    "#                 callback_end_epoch=callback_end_epoch,\n",
    "#                 augmentation=augmentation,\n",
    "#                 epoch=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# need  to specify the path to the directory that you want to save the segmented images\n",
    "if not osp.exists(\"./data/segmentation\"):\n",
    "    os.makedirs(\"./data/segmentation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lbl_preds, lbl_trues = [], []\n",
    "# for i in tqdm.trange(len(valid_image_path_list)):\n",
    "#     datum = model_class.get_preprocessed_data(valid_image_path_list, i)\n",
    "#     x_data = np.expand_dims(datum, axis=0)\n",
    "#     lbl_true = model_class.get_label(valid_annotation_path_list, i)\n",
    "\n",
    "#     x = rm.Variable(x_data)\n",
    "#     score = model_class(x)\n",
    "#     lbl_pred = np.argmax(score.as_ndarray(), axis=1)\n",
    "\n",
    "#     lbl_preds.append(lbl_pred)\n",
    "#     lbl_trues.append(lbl_true)\n",
    "#     visualize_img = draw_segment(valid_image_path_list[i], lbl_pred)\n",
    "#     visualize_img.save(\"./data/segmentation/{}.png\".format(str(i)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scratch Training Algorithm"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "epochs = 300\n",
    "opt = rm.Sgd(lr=0.03,momentum=0.8) # rm.Adam(lr=0.00001)\n",
    "lr = [0.03] * 10 + [0.02] * 10 + [0.01] * 10 + [0.008] * 30 + [0.006] * 240\n",
    "N = len(train_keys)\n",
    "val_N = len(val_keys)\n",
    "best_loss = np.inf\n",
    "\n",
    "all_accuracies = [0]*epochs\n",
    "all_train_losses = [0]*epochs\n",
    "all_validation_losses = [0]*epochs\n",
    "all_ious = [0]*epochs\n",
    "\n",
    "CLASS_WEIGHT = 8 # hyperparameter to give more weight to the imbalanced class\n",
    "AUG_PROB = 1.0\n",
    "PLOT_WHILE_TRAINING = False\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    counts = 0\n",
    "    total = 0\n",
    "    total_area = 0\n",
    "    bar = tqdm(range(N//batch_size))\n",
    "    loss = 0\n",
    "    val_loss = 0\n",
    "    \n",
    "    opt._lr = lr[epoch]\n",
    "    for j in range(N//batch_size):\n",
    "        with model_FCN.train():\n",
    "            x, y = genr.generate(True, AUG_PROB).__next__()\n",
    "            t = model_FCN(x)\n",
    "            \n",
    "            reg = 0\n",
    "            for layer in model_FCN.iter_models():\n",
    "                if hasattr(layer, \"params\") and hasattr(layer.params, \"w\"):\n",
    "                    reg += rm.sum(layer.params.w*layer.params.w)\n",
    "                    reg_loss = 0.0001*reg/(img_size[0] * img_size[1] * x.shape[0] * 2)\n",
    "            \n",
    "            n, c, _, _ = y.shape\n",
    "            weight_loss = -rm.log(rm.softmax(t) + 1e-8) * y\n",
    "            weight_loss += weight_loss * np.broadcast_to(np.expand_dims(np.argmax(y, axis=1) * CLASS_WEIGHT, axis=1), weight_loss.shape)\n",
    "            l = rm.sum(weight_loss) / (img_size[0] * img_size[1] * x.shape[0]) + reg_loss\n",
    "            \n",
    "        l.grad().update(opt)\n",
    "        bar.update(1)\n",
    "        loss += l.as_ndarray()\n",
    "        \n",
    "    for k in range(val_N//batch_size):\n",
    "        x, y = gen.generate(False).__next__()\n",
    "        t = model_FCN(x)\n",
    "        l = -rm.log(rm.softmax(t) + 1e-8) * y\n",
    "        l += l * np.broadcast_to(np.expand_dims(np.argmax(y, axis=1) * CLASS_WEIGHT, axis=1), l.shape)\n",
    "        l = rm.sum(l) / (img_size[0] * img_size[1] * batch_size)\n",
    "        results = rm.softmax(t)\n",
    "        results = results.as_ndarray()\n",
    "        tmp = 0\n",
    "        \n",
    "        target_index = np.where(np.argmax(y, axis=1).flatten()==1)[0]\n",
    "        \n",
    "        for index in np.where(np.argmax(results, axis=1).flatten()==1)[0]:\n",
    "            if index in target_index:\n",
    "                counts += 1\n",
    "                tmp += 1\n",
    "                \n",
    "        total += np.sum(np.argmax(y, axis=1)==1)\n",
    "        total_area += (np.sum(np.argmax(y, axis=1)==1) + np.sum(np.argmax(results, axis=1)==1) - tmp)\n",
    "        \n",
    "        val_loss += l.as_ndarray()\n",
    "        \n",
    "    # PLOT\n",
    "    if PLOT_WHILE_TRAINING:\n",
    "        fig=plt.figure(figsize=(8, 8))\n",
    "        pred_img = visualize(np.argmax(results[0], axis=0), False)\n",
    "        annot = visualize(np.argmax(y[0], axis=0), False)\n",
    "        fig.add_subplot(1, 4, 1)\n",
    "        plt.imshow(pred_img/255.)\n",
    "        fig.add_subplot(1, 4, 2)\n",
    "        plt.imshow(annot/255.)\n",
    "        fig.add_subplot(1, 4, 3)\n",
    "        plt.imshow(x[0][0], cmap='gray')\n",
    "        fig.add_subplot(1, 4, 4)\n",
    "        plt.imshow(x[0][10], cmap='gray')\n",
    "        plt.show\n",
    "        \n",
    "    iou = counts / total_area\n",
    "    accuracy = counts / total\n",
    "    all_accuracies[epoch] = accuracy\n",
    "    \n",
    "    train_loss = loss/(j+1) # tran loss\n",
    "    all_train_losses[epoch] = train_loss\n",
    "    all_validation_losses[epoch] = val_loss/(k+1)\n",
    "    all_ious[epoch] = iou\n",
    "    \n",
    "    if (val_loss/(k+1)) < best_loss or epoch%25 == 0:\n",
    "        print('Epoch %d, saving the model...' %epoch)\n",
    "        model_FCN.save('../03_results/weights/weight.Augmented_%.1f_best_Angga_FCN_CLASS_WEIGHT_rotate_zoom_%d_epoch%d.hdf5'%(AUG_PROB, CLASS_WEIGHT, epoch))\n",
    "        \n",
    "        best_loss = (val_loss/ (k+1))\n",
    "        bar.set_description(\"epoch {:03d} avg loss:{:6.4f}  val loss:{:6.4f} accuracy:{:6.4f} iou:{:6.4f} val loss is improved\".format(epoch, float((loss/(j+1))), float((val_loss/(k+1))), accuracy, iou))\n",
    "    else:\n",
    "        bar.set_description(\"epoch {:03d} avg loss:{:6.4f}  val loss:{:6.4f} accuracy:{:6.4f} iou:{:6.4f}\".format(epoch, float((loss/(j+1))), float((val_loss/(k+1))), accuracy, iou))\n",
    "        \n",
    "    bar.update(0)\n",
    "    bar.refresh()\n",
    "    bar.close()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
