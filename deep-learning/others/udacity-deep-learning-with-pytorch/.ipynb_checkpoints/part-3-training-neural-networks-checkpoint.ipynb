{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def view_classify(img, ps, version=\"MNIST\"):\n",
    "    ''' Function for viewing an image and it's predicted classes.\n",
    "    '''\n",
    "    ps = ps.data.numpy().squeeze()\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(figsize=(6,9), ncols=2)\n",
    "    ax1.imshow(img.resize_(1, 28, 28).numpy().squeeze())\n",
    "    ax1.axis('off')\n",
    "    ax2.barh(np.arange(10), ps)\n",
    "    ax2.set_aspect(0.1)\n",
    "    ax2.set_yticks(np.arange(10))\n",
    "    if version == \"MNIST\":\n",
    "        ax2.set_yticklabels(np.arange(10))\n",
    "    elif version == \"Fashion\":\n",
    "        ax2.set_yticklabels(['T-shirt/top',\n",
    "                            'Trouser',\n",
    "                            'Pullover',\n",
    "                            'Dress',\n",
    "                            'Coat',\n",
    "                            'Sandal',\n",
    "                            'Shirt',\n",
    "                            'Sneaker',\n",
    "                            'Bag',\n",
    "                            'Ankle Boot'], size='small');\n",
    "    ax2.set_title('Class Probability')\n",
    "    ax2.set_xlim(0, 1.1)\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,), (0.5,)),\n",
    "                              ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the training data\n",
    "trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits: tensor([[-0.0710,  0.1834,  0.1518,  0.0645, -0.0750, -0.1821, -0.0530,  0.1575,\n",
      "         -0.0042,  0.1451],\n",
      "        [-0.0305,  0.1579,  0.0876, -0.0199, -0.0421, -0.1722, -0.0114,  0.0822,\n",
      "          0.0751,  0.1669],\n",
      "        [-0.0758,  0.1489,  0.1123,  0.0468, -0.0717, -0.1885,  0.0225,  0.0801,\n",
      "          0.0337,  0.1446],\n",
      "        [-0.0649,  0.1709,  0.1037,  0.0106, -0.0528, -0.1549, -0.1213,  0.1528,\n",
      "          0.0279,  0.2254],\n",
      "        [-0.0651,  0.1937,  0.1400,  0.0231, -0.0351, -0.1844, -0.0431,  0.1952,\n",
      "          0.0481,  0.1829],\n",
      "        [-0.1323,  0.2052,  0.1438, -0.0345, -0.0794, -0.1133, -0.0298,  0.1067,\n",
      "         -0.0218,  0.1694],\n",
      "        [-0.0190,  0.1998,  0.0667,  0.0571, -0.0529, -0.1499, -0.1840,  0.1874,\n",
      "         -0.0904,  0.1673],\n",
      "        [-0.1026,  0.2116,  0.0937,  0.0052, -0.0442, -0.1658, -0.1520,  0.2108,\n",
      "         -0.0278,  0.2216],\n",
      "        [-0.1452,  0.2520,  0.0656, -0.0203, -0.0596, -0.2093, -0.0581,  0.1217,\n",
      "          0.0806,  0.1421],\n",
      "        [-0.0974,  0.1959,  0.0463, -0.0199, -0.1404, -0.1237, -0.1079,  0.0731,\n",
      "          0.0003,  0.1201],\n",
      "        [-0.1215,  0.2503,  0.1349, -0.0315, -0.0770, -0.1959, -0.0297,  0.1554,\n",
      "          0.0888,  0.1434],\n",
      "        [-0.1702,  0.2519,  0.0875,  0.0193, -0.0781, -0.1968, -0.0027,  0.1732,\n",
      "          0.0487,  0.1742],\n",
      "        [-0.0648,  0.1481,  0.1368, -0.0645, -0.0626, -0.1825, -0.0278,  0.1027,\n",
      "          0.0819,  0.1710],\n",
      "        [-0.0471,  0.2035,  0.0991,  0.0366, -0.0588, -0.1590, -0.1118,  0.1517,\n",
      "          0.0144,  0.1566],\n",
      "        [-0.1564,  0.2343,  0.1434, -0.0337, -0.0807, -0.1863, -0.0325,  0.1764,\n",
      "          0.0386,  0.1498],\n",
      "        [-0.1185,  0.2064,  0.1616, -0.0350, -0.0276, -0.2157, -0.0762,  0.1523,\n",
      "          0.1200,  0.2257],\n",
      "        [-0.1378,  0.1287,  0.0810,  0.0090, -0.0615, -0.1786, -0.1224,  0.1212,\n",
      "          0.0007,  0.2339],\n",
      "        [-0.0522,  0.1664,  0.1704, -0.0544, -0.1085, -0.1499, -0.0840,  0.1586,\n",
      "         -0.0125,  0.1912],\n",
      "        [-0.0348,  0.2531,  0.1171, -0.0100, -0.0640, -0.1945, -0.1029,  0.1229,\n",
      "          0.0764,  0.1295],\n",
      "        [-0.0861,  0.1431,  0.0395,  0.0014, -0.0861, -0.1193, -0.1532,  0.1313,\n",
      "         -0.0366,  0.1797],\n",
      "        [-0.0271,  0.2238,  0.1079,  0.0424, -0.0380, -0.1623, -0.0712,  0.1232,\n",
      "          0.0605,  0.2776],\n",
      "        [-0.0886,  0.1690,  0.0962,  0.0358, -0.0672, -0.1286, -0.0821,  0.1555,\n",
      "         -0.0435,  0.1753],\n",
      "        [-0.0728,  0.2338,  0.0613,  0.0203, -0.1026, -0.1200, -0.0732,  0.1681,\n",
      "         -0.0609,  0.1732],\n",
      "        [-0.1314,  0.2190,  0.1229, -0.0023, -0.1072, -0.1525, -0.1178,  0.1596,\n",
      "          0.0368,  0.2027],\n",
      "        [-0.0703,  0.2437,  0.2350,  0.0778, -0.0654, -0.0957, -0.0421,  0.2551,\n",
      "         -0.0354,  0.1654],\n",
      "        [-0.0733,  0.2352,  0.0484, -0.0892, -0.0659, -0.1750, -0.0294,  0.0705,\n",
      "          0.1029,  0.1812],\n",
      "        [-0.0279,  0.2065,  0.1956,  0.0550, -0.0094, -0.1239, -0.1119,  0.2373,\n",
      "          0.0211,  0.2198],\n",
      "        [-0.0729,  0.1670,  0.1120, -0.0078, -0.0802, -0.1530, -0.0417,  0.0540,\n",
      "          0.0147,  0.1745],\n",
      "        [-0.1757,  0.1455,  0.0730, -0.0205, -0.1074, -0.1388, -0.0549,  0.0688,\n",
      "          0.0891,  0.2098],\n",
      "        [-0.1582,  0.2025,  0.1311, -0.0094, -0.0836, -0.1801, -0.0173,  0.1454,\n",
      "          0.1326,  0.2215],\n",
      "        [-0.1369,  0.1809,  0.1136,  0.0560, -0.0546, -0.1421, -0.0782,  0.1341,\n",
      "          0.0643,  0.1892],\n",
      "        [-0.1702,  0.2503,  0.1830, -0.0175, -0.0734, -0.2223, -0.0371,  0.2638,\n",
      "          0.0651,  0.1972],\n",
      "        [-0.1134,  0.2325,  0.1463, -0.0075, -0.0231, -0.1735, -0.0257,  0.1447,\n",
      "          0.0465,  0.1881],\n",
      "        [-0.1457,  0.2096,  0.0914, -0.0699, -0.0737, -0.1893, -0.0684,  0.1197,\n",
      "          0.0862,  0.1939],\n",
      "        [-0.1913,  0.2130,  0.0594, -0.0197, -0.0585, -0.1965, -0.1119,  0.1849,\n",
      "          0.0842,  0.2417],\n",
      "        [-0.0344,  0.2225,  0.1442, -0.0120, -0.0761, -0.0808, -0.0811,  0.1578,\n",
      "         -0.0547,  0.1826],\n",
      "        [-0.0646,  0.1667,  0.1393, -0.0352, -0.0569, -0.1193, -0.1190,  0.1292,\n",
      "          0.0541,  0.2286],\n",
      "        [-0.0977,  0.2627,  0.0557, -0.0202, -0.0434, -0.1652, -0.0296,  0.0340,\n",
      "          0.1457,  0.1200],\n",
      "        [-0.1926,  0.2036,  0.1062, -0.0361, -0.0935, -0.1845, -0.0280,  0.1649,\n",
      "          0.0394,  0.1594],\n",
      "        [-0.1621,  0.1992,  0.1212, -0.0580, -0.1139, -0.1709, -0.0375,  0.0917,\n",
      "          0.0199,  0.1257],\n",
      "        [-0.1588,  0.2645,  0.1117, -0.0072, -0.0905, -0.1865,  0.0003,  0.1147,\n",
      "          0.1070,  0.1958],\n",
      "        [-0.1463,  0.2413,  0.1525, -0.0253, -0.0556, -0.1752, -0.1259,  0.1795,\n",
      "          0.0351,  0.1614],\n",
      "        [-0.0853,  0.2415,  0.1252,  0.0073, -0.0342, -0.2129, -0.0615,  0.1531,\n",
      "          0.0607,  0.2162],\n",
      "        [-0.1404,  0.2000,  0.0962,  0.0104, -0.0423, -0.1318, -0.0910,  0.1460,\n",
      "          0.0741,  0.2330],\n",
      "        [-0.1412,  0.2295,  0.1575,  0.0168, -0.0513, -0.1524, -0.0495,  0.1667,\n",
      "          0.0870,  0.2469],\n",
      "        [-0.1001,  0.2057,  0.1753, -0.0392, -0.0624, -0.2197, -0.0610,  0.1054,\n",
      "          0.1046,  0.1811],\n",
      "        [-0.1531,  0.2403,  0.0672,  0.0047, -0.0681, -0.2160, -0.0361,  0.1618,\n",
      "          0.0618,  0.1643],\n",
      "        [-0.1039,  0.2574,  0.0510, -0.0037, -0.0741, -0.2004, -0.1004,  0.1525,\n",
      "          0.0367,  0.1780],\n",
      "        [-0.1105,  0.1807,  0.0547,  0.0534, -0.0390, -0.1079, -0.1105,  0.1444,\n",
      "          0.0154,  0.2078],\n",
      "        [-0.0286,  0.1465,  0.0402,  0.0206, -0.0274, -0.1492, -0.0305,  0.0484,\n",
      "          0.0153,  0.1414],\n",
      "        [-0.0912,  0.2377,  0.0647, -0.0456, -0.0938, -0.1348, -0.0292,  0.0785,\n",
      "          0.0628,  0.1350],\n",
      "        [-0.0436,  0.1956,  0.1937,  0.0347, -0.0436, -0.1525, -0.1176,  0.2112,\n",
      "          0.0120,  0.2277],\n",
      "        [-0.0535,  0.2621,  0.0440,  0.0275, -0.0553, -0.1633, -0.1258,  0.1345,\n",
      "         -0.0063,  0.1520],\n",
      "        [-0.0334,  0.2661,  0.0330,  0.0686, -0.0383, -0.1280, -0.1276,  0.1374,\n",
      "         -0.0102,  0.1531],\n",
      "        [-0.0358,  0.2209,  0.2021, -0.0006, -0.0156, -0.2321, -0.0329,  0.2616,\n",
      "          0.0077,  0.1826],\n",
      "        [-0.1446,  0.1859,  0.0638,  0.0112, -0.0840, -0.1247, -0.0318,  0.0416,\n",
      "          0.0913,  0.1410],\n",
      "        [-0.1664,  0.2010,  0.1297,  0.0719, -0.0648, -0.1206, -0.0851,  0.1216,\n",
      "          0.0038,  0.2032],\n",
      "        [-0.0657,  0.1835,  0.1011, -0.0150, -0.0419, -0.2152, -0.0294,  0.1635,\n",
      "          0.0195,  0.1609],\n",
      "        [-0.1173,  0.2353,  0.1737, -0.0751, -0.0823, -0.2265, -0.0555,  0.1664,\n",
      "          0.0768,  0.0878],\n",
      "        [-0.0580,  0.1312,  0.0683,  0.0671, -0.0003, -0.1512, -0.1341,  0.0806,\n",
      "         -0.0082,  0.2091],\n",
      "        [-0.1031,  0.2256,  0.1190,  0.0739, -0.0655, -0.1025, -0.1078,  0.1431,\n",
      "         -0.0240,  0.1245],\n",
      "        [-0.1327,  0.2196,  0.1091, -0.0825, -0.0731, -0.2049, -0.0591,  0.1155,\n",
      "          0.1230,  0.1526],\n",
      "        [-0.1111,  0.1978,  0.1545,  0.0086, -0.0672, -0.1910, -0.0091,  0.1846,\n",
      "          0.0948,  0.2048],\n",
      "        [-0.0287,  0.2857,  0.0773,  0.0076, -0.0739, -0.1190, -0.0243,  0.1346,\n",
      "          0.0027,  0.1409]], grad_fn=<ThAddmmBackward>)\n",
      "tensor(2.2917, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "# build a feed-forward network\n",
    "model = nn.Sequential(nn.Linear(784, 128),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(128, 64),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(64, 10))\n",
    "\n",
    "# define the loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# get our data\n",
    "images, labels = next(iter(trainloader))\n",
    "\n",
    "# flatten images\n",
    "images = images.view(images.shape[0], -1)\n",
    "\n",
    "# forward pass, get our logits\n",
    "logits = model(images)\n",
    "print('logits:',logits)\n",
    "\n",
    "# calculate the loss with the logits and the labels\n",
    "loss = criterion(logits, labels)\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Negative Log Likelihood Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logps: tensor([[-2.3045, -2.3681, -2.1900, -2.3900, -2.2844, -2.4215, -2.1620, -2.3286,\n",
      "         -2.1684, -2.4594],\n",
      "        [-2.2545, -2.3140, -2.2506, -2.3761, -2.2600, -2.4844, -2.2066, -2.3351,\n",
      "         -2.1992, -2.3811],\n",
      "        [-2.2679, -2.3659, -2.1944, -2.3465, -2.3304, -2.4770, -2.1548, -2.3606,\n",
      "         -2.1388, -2.4518],\n",
      "        [-2.3055, -2.3110, -2.2189, -2.2848, -2.2913, -2.4084, -2.1764, -2.3515,\n",
      "         -2.2580, -2.4504],\n",
      "        [-2.2338, -2.3001, -2.2210, -2.3296, -2.3038, -2.4055, -2.1630, -2.4070,\n",
      "         -2.2559, -2.4436],\n",
      "        [-2.3022, -2.3299, -2.1965, -2.3236, -2.3374, -2.4082, -2.1978, -2.3360,\n",
      "         -2.1453, -2.4988],\n",
      "        [-2.3256, -2.2380, -2.2111, -2.3169, -2.3445, -2.4281, -2.1511, -2.3802,\n",
      "         -2.1963, -2.4861],\n",
      "        [-2.2807, -2.3297, -2.3076, -2.3240, -2.2153, -2.3995, -2.2174, -2.4031,\n",
      "         -2.1761, -2.4037],\n",
      "        [-2.2830, -2.3418, -2.2367, -2.3247, -2.3658, -2.4036, -2.1147, -2.3834,\n",
      "         -2.1261, -2.5162],\n",
      "        [-2.3376, -2.3864, -2.2576, -2.2955, -2.3438, -2.3882, -2.1047, -2.4089,\n",
      "         -2.1441, -2.4142],\n",
      "        [-2.3154, -2.2562, -2.1916, -2.3041, -2.3671, -2.4444, -2.1522, -2.3732,\n",
      "         -2.1978, -2.4775],\n",
      "        [-2.2831, -2.3175, -2.2479, -2.3206, -2.3482, -2.3979, -2.1380, -2.3764,\n",
      "         -2.1670, -2.4779],\n",
      "        [-2.3077, -2.3018, -2.1212, -2.3492, -2.3114, -2.5232, -2.1869, -2.3030,\n",
      "         -2.1978, -2.4950],\n",
      "        [-2.3227, -2.3243, -2.2349, -2.3192, -2.2609, -2.4259, -2.1596, -2.3701,\n",
      "         -2.1889, -2.4621],\n",
      "        [-2.2825, -2.3790, -2.2673, -2.3505, -2.2364, -2.3951, -2.2267, -2.3517,\n",
      "         -2.1667, -2.3994],\n",
      "        [-2.3319, -2.3763, -2.2268, -2.3469, -2.2669, -2.4147, -2.1521, -2.3887,\n",
      "         -2.1551, -2.4130],\n",
      "        [-2.2951, -2.2677, -2.1871, -2.2930, -2.2788, -2.4497, -2.2086, -2.3567,\n",
      "         -2.2631, -2.4632],\n",
      "        [-2.2366, -2.3466, -2.2438, -2.3415, -2.3327, -2.3847, -2.1508, -2.3720,\n",
      "         -2.1578, -2.5155],\n",
      "        [-2.2461, -2.3455, -2.1940, -2.3472, -2.3430, -2.3799, -2.1887, -2.3695,\n",
      "         -2.1692, -2.4910],\n",
      "        [-2.3483, -2.2829, -2.2166, -2.2987, -2.2994, -2.5071, -2.2023, -2.2877,\n",
      "         -2.1614, -2.4762],\n",
      "        [-2.2410, -2.3530, -2.2155, -2.3461, -2.3254, -2.3904, -2.1643, -2.3665,\n",
      "         -2.1618, -2.5179],\n",
      "        [-2.2820, -2.3730, -2.2410, -2.3303, -2.3659, -2.3903, -2.1041, -2.3854,\n",
      "         -2.1284, -2.4937],\n",
      "        [-2.3074, -2.3267, -2.1486, -2.3523, -2.2925, -2.4058, -2.2043, -2.3638,\n",
      "         -2.1966, -2.4735],\n",
      "        [-2.2682, -2.2518, -2.1860, -2.2745, -2.2563, -2.4607, -2.2313, -2.3511,\n",
      "         -2.2805, -2.5123],\n",
      "        [-2.2269, -2.3551, -2.2502, -2.4383, -2.2651, -2.4321, -2.1716, -2.3973,\n",
      "         -2.1734, -2.3629],\n",
      "        [-2.2735, -2.2876, -2.2182, -2.2875, -2.3355, -2.3809, -2.1927, -2.4388,\n",
      "         -2.1797, -2.4765],\n",
      "        [-2.2433, -2.2993, -2.2497, -2.2860, -2.3093, -2.4729, -2.1767, -2.3493,\n",
      "         -2.2141, -2.4690],\n",
      "        [-2.3441, -2.3671, -2.2324, -2.3091, -2.3410, -2.4255, -2.1253, -2.3640,\n",
      "         -2.1420, -2.4285],\n",
      "        [-2.3253, -2.3031, -2.2155, -2.3133, -2.3398, -2.4361, -2.1348, -2.3599,\n",
      "         -2.1565, -2.5012],\n",
      "        [-2.3735, -2.3247, -2.2452, -2.2794, -2.3151, -2.4371, -2.1265, -2.4422,\n",
      "         -2.1434, -2.3953],\n",
      "        [-2.3194, -2.3092, -2.2239, -2.3027, -2.3355, -2.4201, -2.1623, -2.3457,\n",
      "         -2.1706, -2.4825],\n",
      "        [-2.2855, -2.3596, -2.2249, -2.4060, -2.2978, -2.4100, -2.1246, -2.3735,\n",
      "         -2.2049, -2.3821],\n",
      "        [-2.3291, -2.2456, -2.2021, -2.2631, -2.2748, -2.4590, -2.1752, -2.3742,\n",
      "         -2.2305, -2.5299],\n",
      "        [-2.3201, -2.2718, -2.2136, -2.3180, -2.2241, -2.4363, -2.2418, -2.3330,\n",
      "         -2.2403, -2.4595],\n",
      "        [-2.2576, -2.4350, -2.2146, -2.3026, -2.3043, -2.3883, -2.1240, -2.4322,\n",
      "         -2.2061, -2.4139],\n",
      "        [-2.3331, -2.3653, -2.2295, -2.2805, -2.3226, -2.4161, -2.1404, -2.3803,\n",
      "         -2.1550, -2.4540],\n",
      "        [-2.2904, -2.3071, -2.1985, -2.2843, -2.3173, -2.4400, -2.1896, -2.3516,\n",
      "         -2.1856, -2.5116],\n",
      "        [-2.3110, -2.3235, -2.2117, -2.2861, -2.2978, -2.4579, -2.2004, -2.3385,\n",
      "         -2.1654, -2.4804],\n",
      "        [-2.3440, -2.3239, -2.2093, -2.3293, -2.3204, -2.3688, -2.1158, -2.4240,\n",
      "         -2.1974, -2.4420],\n",
      "        [-2.3413, -2.2900, -2.2386, -2.3023, -2.2974, -2.4535, -2.1468, -2.4264,\n",
      "         -2.1508, -2.4330],\n",
      "        [-2.3129, -2.2940, -2.2100, -2.3362, -2.2922, -2.4288, -2.1574, -2.4099,\n",
      "         -2.1837, -2.4475],\n",
      "        [-2.2884, -2.2833, -2.2266, -2.2871, -2.3261, -2.3891, -2.1985, -2.3565,\n",
      "         -2.1953, -2.5165],\n",
      "        [-2.3510, -2.2436, -2.2068, -2.2996, -2.3209, -2.4312, -2.1652, -2.3775,\n",
      "         -2.2270, -2.4438],\n",
      "        [-2.2877, -2.3737, -2.2228, -2.3115, -2.3431, -2.3903, -2.1225, -2.4024,\n",
      "         -2.1473, -2.4854],\n",
      "        [-2.2814, -2.2705, -2.2090, -2.3855, -2.2799, -2.4892, -2.1496, -2.4076,\n",
      "         -2.1951, -2.4129],\n",
      "        [-2.3111, -2.3423, -2.1931, -2.3165, -2.3040, -2.4071, -2.1822, -2.3585,\n",
      "         -2.1984, -2.4501],\n",
      "        [-2.2202, -2.3471, -2.2133, -2.4219, -2.2859, -2.4188, -2.1674, -2.4285,\n",
      "         -2.2051, -2.3636],\n",
      "        [-2.3645, -2.2224, -2.2064, -2.2927, -2.2583, -2.4165, -2.1899, -2.4142,\n",
      "         -2.2028, -2.5141],\n",
      "        [-2.3196, -2.2970, -2.1835, -2.2593, -2.3267, -2.4087, -2.2179, -2.3353,\n",
      "         -2.1970, -2.5289],\n",
      "        [-2.2602, -2.3353, -2.2204, -2.3161, -2.2863, -2.4389, -2.1852, -2.3615,\n",
      "         -2.2006, -2.4615],\n",
      "        [-2.3090, -2.3115, -2.2052, -2.2953, -2.2995, -2.4052, -2.1862, -2.3715,\n",
      "         -2.2229, -2.4531],\n",
      "        [-2.3265, -2.2181, -2.2064, -2.2572, -2.3487, -2.4381, -2.1888, -2.3627,\n",
      "         -2.2253, -2.5042],\n",
      "        [-2.2949, -2.3156, -2.1873, -2.2444, -2.3300, -2.4146, -2.2331, -2.3294,\n",
      "         -2.1938, -2.5311],\n",
      "        [-2.2736, -2.3643, -2.2020, -2.2650, -2.3251, -2.4200, -2.2039, -2.3308,\n",
      "         -2.1940, -2.4903],\n",
      "        [-2.2781, -2.3132, -2.2635, -2.3333, -2.2782, -2.4821, -2.1534, -2.3929,\n",
      "         -2.1566, -2.4258],\n",
      "        [-2.2808, -2.3287, -2.2233, -2.3173, -2.3174, -2.4276, -2.1794, -2.3413,\n",
      "         -2.1680, -2.4878],\n",
      "        [-2.3338, -2.2417, -2.1787, -2.2668, -2.2940, -2.4392, -2.1612, -2.4206,\n",
      "         -2.2740, -2.4661],\n",
      "        [-2.2816, -2.4124, -2.1692, -2.3613, -2.3201, -2.4207, -2.1659, -2.3432,\n",
      "         -2.1518, -2.4574],\n",
      "        [-2.2984, -2.3391, -2.1819, -2.3478, -2.2768, -2.5026, -2.1493, -2.3910,\n",
      "         -2.1716, -2.4285],\n",
      "        [-2.2606, -2.3162, -2.1977, -2.3874, -2.2978, -2.3995, -2.1546, -2.3426,\n",
      "         -2.2553, -2.4531],\n",
      "        [-2.2605, -2.3645, -2.2551, -2.3565, -2.3581, -2.3921, -2.0926, -2.3876,\n",
      "         -2.1373, -2.4905],\n",
      "        [-2.2577, -2.3314, -2.2086, -2.3634, -2.2742, -2.3860, -2.1732, -2.4437,\n",
      "         -2.2077, -2.4213],\n",
      "        [-2.2806, -2.4168, -2.2061, -2.3234, -2.3347, -2.4023, -2.0800, -2.4651,\n",
      "         -2.1797, -2.4063],\n",
      "        [-2.3095, -2.3234, -2.2086, -2.2994, -2.3563, -2.4006, -2.1566, -2.3711,\n",
      "         -2.1464, -2.5116]], grad_fn=<LogSoftmaxBackward>)\n",
      "tensor(2.3160, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(nn.Linear(784, 128),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(128, 64),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(64, 10),\n",
    "                      nn.LogSoftmax(dim=1))\n",
    "\n",
    "# define the loss\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "# get our data\n",
    "images, labels = next(iter(trainloader))\n",
    "\n",
    "# flatten images\n",
    "images = images.view(images.shape[0], -1)\n",
    "\n",
    "# forward pass, get our log-probabilities\n",
    "logps = model(images)\n",
    "print('logps:',logps)\n",
    "\n",
    "# calculate the loss with the logps and the labels\n",
    "loss = criterion(logps, labels)\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get gradients using backward to update the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before backward pass: \n",
      " None\n",
      "After backward pass: \n",
      " tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0037,  0.0037,  0.0037,  ...,  0.0037,  0.0037,  0.0037],\n",
      "        ...,\n",
      "        [-0.0004, -0.0004, -0.0004,  ..., -0.0004, -0.0004, -0.0004],\n",
      "        [ 0.0008,  0.0008,  0.0008,  ...,  0.0008,  0.0008,  0.0008],\n",
      "        [-0.0005, -0.0005, -0.0005,  ..., -0.0005, -0.0005, -0.0005]])\n"
     ]
    }
   ],
   "source": [
    "print('Before backward pass: \\n', model[0].weight.grad)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print('After backward pass: \\n', model[0].weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "# optimizers require the parameters to optimize and a learning rate\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial weights -  Parameter containing:\n",
      "tensor([[-0.0017,  0.0107, -0.0190,  ...,  0.0021, -0.0085,  0.0041],\n",
      "        [ 0.0259,  0.0184, -0.0052,  ..., -0.0160, -0.0043, -0.0073],\n",
      "        [-0.0114,  0.0178, -0.0056,  ..., -0.0102, -0.0141,  0.0058],\n",
      "        ...,\n",
      "        [-0.0269, -0.0009,  0.0097,  ...,  0.0319,  0.0169,  0.0066],\n",
      "        [ 0.0091, -0.0123, -0.0162,  ...,  0.0251,  0.0326,  0.0049],\n",
      "        [ 0.0068, -0.0267,  0.0288,  ...,  0.0236,  0.0097,  0.0062]],\n",
      "       requires_grad=True)\n",
      "\n",
      "\n",
      "Gradient - tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0033,  0.0033,  0.0033,  ...,  0.0033,  0.0033,  0.0033],\n",
      "        ...,\n",
      "        [ 0.0021,  0.0021,  0.0021,  ...,  0.0021,  0.0021,  0.0021],\n",
      "        [ 0.0012,  0.0012,  0.0012,  ...,  0.0012,  0.0012,  0.0012],\n",
      "        [-0.0022, -0.0022, -0.0022,  ..., -0.0022, -0.0022, -0.0022]])\n",
      "\n",
      "\n",
      "Updated weights -  Parameter containing:\n",
      "tensor([[-0.0017,  0.0107, -0.0190,  ...,  0.0021, -0.0085,  0.0041],\n",
      "        [ 0.0259,  0.0184, -0.0052,  ..., -0.0160, -0.0043, -0.0073],\n",
      "        [-0.0115,  0.0178, -0.0056,  ..., -0.0103, -0.0141,  0.0058],\n",
      "        ...,\n",
      "        [-0.0270, -0.0009,  0.0097,  ...,  0.0319,  0.0169,  0.0066],\n",
      "        [ 0.0091, -0.0123, -0.0162,  ...,  0.0251,  0.0326,  0.0049],\n",
      "        [ 0.0068, -0.0266,  0.0288,  ...,  0.0236,  0.0097,  0.0062]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print('Initial weights - ', model[0].weight)\n",
    "\n",
    "images, labels = next(iter(trainloader))\n",
    "images.resize_(64, 784)\n",
    "\n",
    "# clear the gradients, do this because gradients are accumulated\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# forward pass, then backward pass, then update weights\n",
    "output = model(images)\n",
    "loss = criterion(output, labels)\n",
    "loss.backward()\n",
    "print('\\n')\n",
    "print('Gradient -', model[0].weight.grad)\n",
    "\n",
    "# take an update step and few the new weights\n",
    "optimizer.step()\n",
    "\n",
    "print('\\n')\n",
    "print('Updated weights - ', model[0].weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 1.9696211362444263\n",
      "Training loss: 0.8954915590504847\n",
      "Training loss: 0.5417580208671626\n",
      "Training loss: 0.4370123802471771\n",
      "Training loss: 0.38928247576774055\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(nn.Linear(784, 128),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(128, 64),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(64, 10),\n",
    "                      nn.LogSoftmax(dim=1))\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.003)\n",
    "\n",
    "epochs = 5\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for images, labels in trainloader:\n",
    "\n",
    "        # flatten MNIST images into a 784 long vector\n",
    "        images = images.view(images.shape[0], -1)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(images)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    else:\n",
    "        print(f\"Training loss: {running_loss/len(trainloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAADhCAYAAACdkiHQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIqAAAFn6f3BwGfzi41qAAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1281b1cc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import helper\n",
    "\n",
    "images, labels = next(iter(trainloader))\n",
    "\n",
    "img = images[0].view(1, 784)\n",
    "# turn off gradients to speed up this part\n",
    "with torch.no_grad():\n",
    "    logps = model(img)\n",
    "    \n",
    "# output of the network are log-probabilities, need to take exponential for probabilities\n",
    "ps = torch.exp(logps)\n",
    "view_classify(img.view(1, 28, 28), ps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
